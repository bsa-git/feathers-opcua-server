{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is OPC UA? As the next-generation of OPC technology, OPC UA (Unified Architecture) is a major leap forward for secure, reliable and platform independent interoperability. OPC UA is designed for the transport of data and information from first-tier factory and process control devices through to the enterprise information system. The OPC UA specification , first released in 2008, integrates all the functionality from the existing OPC Classic specifications into a single service-oriented architecture. It adds essential new features, such as platform independence, diagnostics, discovery, rendering of complex information models, security, and reliability. Additionally, OPC UA was released as an IEC Standard, IEC 62541 in October 2011. OPC UA provides a single set of services for the OPC data models, such as Data Access , Alarms & Conditions , and Historical Access and can be implemented on non-Microsoft systems, including embedded devices. OPC UA provides services for simple / complex information models of vendors and consortia to be \u201cplugged in\u201d to the address space of OPC UA. This information modeling feature allows generic software applications to browse, read, write and subscribe to a whole new world of information. Unified Architecture The OPC Unified Architecture (UA), released in 2008, is a platform independent service-oriented architecture that integrates all the functionality of the individual OPC Classic specifications into one extensible framework. This multi-layered approach accomplishes the original design specification goals of: * Functional equivalence: all COM OPC Classic specifications are mapped to UA * Platform independence: from an embedded micro-controller to cloud-based infrastructure * Secure: encryption, authentication, and auditing * Extensible: ability to add new features without affecting existing applications * Comprehensive information modeling: for defining complex information Functional Equivalence Building on the success of OPC Classic, OPC UA was designed to enhance and surpass the capabilities of the OPC Classic specifications. OPC UA is functionally equivalent to OPC Classic, yet capable of much more: Discovery: find the availability of OPC Servers on local PCs and/or networks Address space: all data is represented hierarchically (e.g. files and folders) allowing for simple and complex structures to be discovered and utilized by OPC Clients On-demand: read and write data/information based on access-permissions Subscriptions: monitor data/information and report-by-exception when values change based on a client\u2019s criteria Events: notify important information based on client\u2019s criteria Methods: clients can execute programs, etc. based on methods defined on the server Integration between OPC UA products and OPC Classic products is easily accomplished with COM/Proxy wrappers that are available in the download section. Platform Independence Given the wide array of available hardware platforms and operating systems, platform independence is essential. OPC UA functions on any of the following and more: Hardware platforms: traditional PC hardware, cloud-based servers, PLCs, micro-controllers (ARM etc.) Operating Systems: Microsoft Windows, Apple OSX, Android, or any distribution of Linux, etc. OPC UA provides the necessary infrastructure for interoperability across the enterprise, from machine-to-machine, machine-to-enterprise and everything in-between. Security One of the most important considerations in choosing a technology is security. OPC UA is firewall-friendly while addressing security concerns by providing a suite of controls: Transport: numerous protocols are defined providing options such as the ultra-fast OPC-binary transport or the more universally compatible JSON over Websockets, for example Session Encryption: messages are transmitted securely at various encryption levels Message Signing: with message signing the recipient can verify the origin and integrity of received messages Sequenced Packets: exposure to message replay attacks is eliminated with sequencing Authentication: each UA client and server is identified through X509 certificates providing control over which applications and systems are permitted to connect with each other User Control: applications can require users to authenticate (login credentials, certificate, web token etc.) and can further restrict and enhance their capabilities with access rights and address-space \u201cviews\u201d Auditing: activities by user and/or system are logged providing an access audit trail Extensible The multi-layered architecture of OPC UA provides a \u201cfuture proof\u201d framework. Innovative technologies and methodologies such as new transport protocols, security algorithms, encoding standards, or application-services can be incorporated into OPC UA while maintaining backwards compatibility for existing products. UA products built today will work with the products of tomorrow. Information Modeling and Access The OPC UA information modeling framework turns data into information. With complete object-oriented capabilities, even the most complex multi-level structures can be modeled and extended. This framework is THE fundamental element of OPC Unified Architecture. It defines the rules and base building blocks necessary to expose an information model with OPC UA. While OPC UA already defines several core models that can be applied in many industries, other organizations build their models upon them, exposing their more specific information with OPC UA. OPC UA also defines the necessary access mechanisms to information models. Look-up mechanism (browsing) to locate instances and their semantic Read and write operations for current data and historical data Method execution Notification for data and events For Client-Server communication the full range of information model access is available via services and in doing so follows the design paradigm of service-oriented architecture (SOA), with which a service provider receives requests, processes them and sends the results back with the response. Publish-Subscribe (PubSub), provides an alternative mechanism for data and event notification. While in Client-Server communication each notification is for a single client with guaranteed delivery, PubSub has been optimized for many-to-many configurations. With PubSub, OPC UA applications do not directly exchange requests and responses. Instead, Publishers send messages to a Message Oriented Middleware, without knowledge of what, if any, Subscribers there may be. Similarly, Subscribers express interest in specific types of data, and process messages that contain this data, without a need to know where it originated from. Learn how other standards organizations are leveraging OPC UA through collaborations with the OPC Foundation.","title":"What is OPC UA?"},{"location":"#what-is-opc-ua","text":"As the next-generation of OPC technology, OPC UA (Unified Architecture) is a major leap forward for secure, reliable and platform independent interoperability. OPC UA is designed for the transport of data and information from first-tier factory and process control devices through to the enterprise information system. The OPC UA specification , first released in 2008, integrates all the functionality from the existing OPC Classic specifications into a single service-oriented architecture. It adds essential new features, such as platform independence, diagnostics, discovery, rendering of complex information models, security, and reliability. Additionally, OPC UA was released as an IEC Standard, IEC 62541 in October 2011. OPC UA provides a single set of services for the OPC data models, such as Data Access , Alarms & Conditions , and Historical Access and can be implemented on non-Microsoft systems, including embedded devices. OPC UA provides services for simple / complex information models of vendors and consortia to be \u201cplugged in\u201d to the address space of OPC UA. This information modeling feature allows generic software applications to browse, read, write and subscribe to a whole new world of information.","title":"What is OPC UA?"},{"location":"#unified-architecture","text":"The OPC Unified Architecture (UA), released in 2008, is a platform independent service-oriented architecture that integrates all the functionality of the individual OPC Classic specifications into one extensible framework. This multi-layered approach accomplishes the original design specification goals of: * Functional equivalence: all COM OPC Classic specifications are mapped to UA * Platform independence: from an embedded micro-controller to cloud-based infrastructure * Secure: encryption, authentication, and auditing * Extensible: ability to add new features without affecting existing applications * Comprehensive information modeling: for defining complex information","title":"Unified Architecture"},{"location":"#functional-equivalence","text":"Building on the success of OPC Classic, OPC UA was designed to enhance and surpass the capabilities of the OPC Classic specifications. OPC UA is functionally equivalent to OPC Classic, yet capable of much more: Discovery: find the availability of OPC Servers on local PCs and/or networks Address space: all data is represented hierarchically (e.g. files and folders) allowing for simple and complex structures to be discovered and utilized by OPC Clients On-demand: read and write data/information based on access-permissions Subscriptions: monitor data/information and report-by-exception when values change based on a client\u2019s criteria Events: notify important information based on client\u2019s criteria Methods: clients can execute programs, etc. based on methods defined on the server Integration between OPC UA products and OPC Classic products is easily accomplished with COM/Proxy wrappers that are available in the download section.","title":"Functional Equivalence"},{"location":"#platform-independence","text":"Given the wide array of available hardware platforms and operating systems, platform independence is essential. OPC UA functions on any of the following and more: Hardware platforms: traditional PC hardware, cloud-based servers, PLCs, micro-controllers (ARM etc.) Operating Systems: Microsoft Windows, Apple OSX, Android, or any distribution of Linux, etc. OPC UA provides the necessary infrastructure for interoperability across the enterprise, from machine-to-machine, machine-to-enterprise and everything in-between.","title":"Platform Independence"},{"location":"#security","text":"One of the most important considerations in choosing a technology is security. OPC UA is firewall-friendly while addressing security concerns by providing a suite of controls: Transport: numerous protocols are defined providing options such as the ultra-fast OPC-binary transport or the more universally compatible JSON over Websockets, for example Session Encryption: messages are transmitted securely at various encryption levels Message Signing: with message signing the recipient can verify the origin and integrity of received messages Sequenced Packets: exposure to message replay attacks is eliminated with sequencing Authentication: each UA client and server is identified through X509 certificates providing control over which applications and systems are permitted to connect with each other User Control: applications can require users to authenticate (login credentials, certificate, web token etc.) and can further restrict and enhance their capabilities with access rights and address-space \u201cviews\u201d Auditing: activities by user and/or system are logged providing an access audit trail","title":"Security"},{"location":"#extensible","text":"The multi-layered architecture of OPC UA provides a \u201cfuture proof\u201d framework. Innovative technologies and methodologies such as new transport protocols, security algorithms, encoding standards, or application-services can be incorporated into OPC UA while maintaining backwards compatibility for existing products. UA products built today will work with the products of tomorrow.","title":"Extensible"},{"location":"#information-modeling-and-access","text":"The OPC UA information modeling framework turns data into information. With complete object-oriented capabilities, even the most complex multi-level structures can be modeled and extended. This framework is THE fundamental element of OPC Unified Architecture. It defines the rules and base building blocks necessary to expose an information model with OPC UA. While OPC UA already defines several core models that can be applied in many industries, other organizations build their models upon them, exposing their more specific information with OPC UA. OPC UA also defines the necessary access mechanisms to information models. Look-up mechanism (browsing) to locate instances and their semantic Read and write operations for current data and historical data Method execution Notification for data and events For Client-Server communication the full range of information model access is available via services and in doing so follows the design paradigm of service-oriented architecture (SOA), with which a service provider receives requests, processes them and sends the results back with the response. Publish-Subscribe (PubSub), provides an alternative mechanism for data and event notification. While in Client-Server communication each notification is for a single client with guaranteed delivery, PubSub has been optimized for many-to-many configurations. With PubSub, OPC UA applications do not directly exchange requests and responses. Instead, Publishers send messages to a Message Oriented Middleware, without knowledge of what, if any, Subscribers there may be. Similarly, Subscribers express interest in specific types of data, and process messages that contain this data, without a need to know where it originated from. Learn how other standards organizations are leveraging OPC UA through collaborations with the OPC Foundation.","title":"Information Modeling and Access"},{"location":"LICENSE/","text":"MIT License Copyright (c) 2020 Sergii Beskorovainyi Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"LICENSE/#mit-license","text":"","title":"MIT License"},{"location":"LICENSE/#copyright-c-2020-sergii-beskorovainyi","text":"Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Copyright (c) 2020 Sergii Beskorovainyi"},{"location":"tutorial/image-building-best-practices/","text":"Security Scanning When you have built an image, it is good practice to scan it for security vulnerabilities using the docker scan command. Docker has partnered with Snyk to provide the vulnerability scanning service. For example, to scan the getting-started image you created earlier in the tutorial, you can just type docker scan getting-started The scan uses a constantly updated database of vulnerabilities, so the output you see will vary as new vulnerabilities are discovered, but it might look something like this: \u2717 Low severity vulnerability found in freetype/freetype Description: CVE-2020-15999 Info: https://snyk.io/vuln/SNYK-ALPINE310-FREETYPE-1019641 Introduced through: freetype/freetype@2.10.0-r0, gd/libgd@2.2.5-r2 From: freetype/freetype@2.10.0-r0 From: gd/libgd@2.2.5-r2 > freetype/freetype@2.10.0-r0 Fixed in: 2.10.0-r1 \u2717 Medium severity vulnerability found in libxml2/libxml2 Description: Out-of-bounds Read Info: https://snyk.io/vuln/SNYK-ALPINE310-LIBXML2-674791 Introduced through: libxml2/libxml2@2.9.9-r3, libxslt/libxslt@1.1.33-r3, nginx-module-xslt/nginx-module-xslt@1.17.9-r1 From: libxml2/libxml2@2.9.9-r3 From: libxslt/libxslt@1.1.33-r3 > libxml2/libxml2@2.9.9-r3 From: nginx-module-xslt/nginx-module-xslt@1.17.9-r1 > libxml2/libxml2@2.9.9-r3 Fixed in: 2.9.9-r4 The output lists the type of vulnerability, a URL to learn more, and importantly which version of the relevant library fixes the vulnerability. There are several other options, which you can read about in the docker scan documentation . As well as scanning your newly built image on the command line, you can also configure Docker Hub to scan all newly pushed images automatically, and you can then see the results in both Docker Hub and Docker Desktop. {: style=width:75% } {: .text-center } Image Layering Did you know that you can look at what makes up an image? Using the docker image history command, you can see the command that was used to create each layer within an image. Use the docker image history command to see the layers in the getting-started image you created earlier in the tutorial. bash docker image history getting-started You should get output that looks something like this (dates/IDs may be different). plaintext IMAGE CREATED CREATED BY SIZE COMMENT a78a40cbf866 18 seconds ago /bin/sh -c #(nop) CMD [\"node\" \"src/index.j\u2026 0B f1d1808565d6 19 seconds ago /bin/sh -c yarn install --production 85.4MB a2c054d14948 36 seconds ago /bin/sh -c #(nop) COPY dir:5dc710ad87c789593\u2026 198kB 9577ae713121 37 seconds ago /bin/sh -c #(nop) WORKDIR /app 0B b95baba1cfdb 13 days ago /bin/sh -c #(nop) CMD [\"node\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ENTRYPOINT [\"docker-entry\u2026 0B <missing> 13 days ago /bin/sh -c #(nop) COPY file:238737301d473041\u2026 116B <missing> 13 days ago /bin/sh -c apk add --no-cache --virtual .bui\u2026 5.35MB <missing> 13 days ago /bin/sh -c #(nop) ENV YARN_VERSION=1.21.1 0B <missing> 13 days ago /bin/sh -c addgroup -g 1000 node && addu\u2026 74.3MB <missing> 13 days ago /bin/sh -c #(nop) ENV NODE_VERSION=12.14.1 0B <missing> 13 days ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ADD file:e69d441d729412d24\u2026 5.59MB Each of the lines represents a layer in the image. The display here shows the base at the bottom with the newest layer at the top. Using this, you can also quickly see the size of each layer, helping diagnose large images. You'll notice that several of the lines are truncated. If you add the --no-trunc flag, you'll get the full output (yes... funny how you use a truncated flag to get untruncated output, huh?) bash docker image history --no-trunc getting-started Layer Caching Now that you've seen the layering in action, there's an important lesson to learn to help decrease build times for your container images. Once a layer changes, all downstream layers have to be recreated as well Let's look at the Dockerfile we were using one more time... FROM node:12-alpine WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] Going back to the image history output, we see that each command in the Dockerfile becomes a new layer in the image. You might remember that when we made a change to the image, the yarn dependencies had to be reinstalled. Is there a way to fix this? It doesn't make much sense to ship around the same dependencies every time we build, right? To fix this, we need to restructure our Dockerfile to help support the caching of the dependencies. For Node-based applications, those dependencies are defined in the package.json file. So, what if we copied only that file in first, install the dependencies, and then copy in everything else? Then, we only recreate the yarn dependencies if there was a change to the package.json . Make sense? Update the Dockerfile to copy in the package.json first, install dependencies, and then copy everything else in. dockerfile hl_lines=\"3 4 5\" FROM node:12-alpine WORKDIR /app COPY package.json yarn.lock ./ RUN yarn install --production COPY . . CMD [\"node\", \"src/index.js\"] Create a file named .dockerignore in the same folder as the Dockerfile with the following contents. ignore node_modules .dockerignore files are an easy way to selectively copy only image relevant files. You can read more about this here . In this case, the node_modules folder should be omitted in the second COPY step because otherwise, it would possibly overwrite files which were created by the command in the RUN step. For further details on why this is recommended for Node.js applications and other best practices, have a look at their guide on Dockerizing a Node.js web app . Build a new image using docker build . bash docker build -t getting-started . You should see output like this... plaintext Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Running in d53a06c9e4c2 yarn install v1.17.3 [1/4] Resolving packages... [2/4] Fetching packages... info fsevents@1.2.9: The platform \"linux\" is incompatible with this module. info \"fsevents@1.2.9\" is an optional dependency and failed compatibility check. Excluding it from installation. [3/4] Linking dependencies... [4/4] Building fresh packages... Done in 10.89s. Removing intermediate container d53a06c9e4c2 ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> a239a11f68d8 Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 49999f68df8f Removing intermediate container 49999f68df8f ---> e709c03bc597 Successfully built e709c03bc597 Successfully tagged getting-started:latest You'll see that all layers were rebuilt. Perfectly fine since we changed the Dockerfile quite a bit. Now, make a change to the src/static/index.html file (like change the <title> to say \"The Awesome Todo App\"). Build the Docker image now using docker build -t getting-started . again. This time, your output should look a little different. plaintext hl_lines=\"5 8 11\" Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> Using cache ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Using cache ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> cccde25a3d9a Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 2be75662c150 Removing intermediate container 2be75662c150 ---> 458e5c6f080c Successfully built 458e5c6f080c Successfully tagged getting-started:latest First off, you should notice that the build was MUCH faster! And, you'll see that steps 1-4 all have Using cache . So, hooray! We're using the build cache. Pushing and pulling this image and updates to it will be much faster as well. Hooray! Multi-Stage Builds While we're not going to dive into it too much in this tutorial, multi-stage builds are an incredibly powerful tool to help use multiple stages to create an image. There are several advantages for them: Separate build-time dependencies from runtime dependencies Reduce overall image size by shipping only what your app needs to run Maven/Tomcat Example When building Java-based applications, a JDK is needed to compile the source code to Java bytecode. However, that JDK isn't needed in production. Also, you might be using tools like Maven or Gradle to help build the app. Those also aren't needed in our final image. Multi-stage builds help. FROM maven AS build WORKDIR /app COPY . . RUN mvn package FROM tomcat COPY --from=build /app/target/file.war /usr/local/tomcat/webapps In this example, we use one stage (called build ) to perform the actual Java build using Maven. In the second stage (starting at FROM tomcat ), we copy in files from the build stage. The final image is only the last stage being created (which can be overridden using the --target flag). React Example When building React applications, we need a Node environment to compile the JS code (typically JSX), SASS stylesheets, and more into static HTML, JS, and CSS. If we aren't doing server-side rendering, we don't even need a Node environment for our production build. Why not ship the static resources in a static nginx container? FROM node:12 AS build WORKDIR /app COPY package* yarn.lock ./ RUN yarn install COPY public ./public COPY src ./src RUN yarn run build FROM nginx:alpine COPY --from=build /app/build /usr/share/nginx/html Here, we are using a node:12 image to perform the build (maximizing layer caching) and then copying the output into an nginx container. Cool, huh? Recap By understanding a little bit about how images are structured, we can build images faster and ship fewer changes. Scanning images gives us confidence that the containers we are running and distributing are secure. Multi-stage builds also help us reduce overall image size and increase final container security by separating build-time dependencies from runtime dependencies.","title":"Index"},{"location":"tutorial/image-building-best-practices/#security-scanning","text":"When you have built an image, it is good practice to scan it for security vulnerabilities using the docker scan command. Docker has partnered with Snyk to provide the vulnerability scanning service. For example, to scan the getting-started image you created earlier in the tutorial, you can just type docker scan getting-started The scan uses a constantly updated database of vulnerabilities, so the output you see will vary as new vulnerabilities are discovered, but it might look something like this: \u2717 Low severity vulnerability found in freetype/freetype Description: CVE-2020-15999 Info: https://snyk.io/vuln/SNYK-ALPINE310-FREETYPE-1019641 Introduced through: freetype/freetype@2.10.0-r0, gd/libgd@2.2.5-r2 From: freetype/freetype@2.10.0-r0 From: gd/libgd@2.2.5-r2 > freetype/freetype@2.10.0-r0 Fixed in: 2.10.0-r1 \u2717 Medium severity vulnerability found in libxml2/libxml2 Description: Out-of-bounds Read Info: https://snyk.io/vuln/SNYK-ALPINE310-LIBXML2-674791 Introduced through: libxml2/libxml2@2.9.9-r3, libxslt/libxslt@1.1.33-r3, nginx-module-xslt/nginx-module-xslt@1.17.9-r1 From: libxml2/libxml2@2.9.9-r3 From: libxslt/libxslt@1.1.33-r3 > libxml2/libxml2@2.9.9-r3 From: nginx-module-xslt/nginx-module-xslt@1.17.9-r1 > libxml2/libxml2@2.9.9-r3 Fixed in: 2.9.9-r4 The output lists the type of vulnerability, a URL to learn more, and importantly which version of the relevant library fixes the vulnerability. There are several other options, which you can read about in the docker scan documentation . As well as scanning your newly built image on the command line, you can also configure Docker Hub to scan all newly pushed images automatically, and you can then see the results in both Docker Hub and Docker Desktop. {: style=width:75% } {: .text-center }","title":"Security Scanning"},{"location":"tutorial/image-building-best-practices/#image-layering","text":"Did you know that you can look at what makes up an image? Using the docker image history command, you can see the command that was used to create each layer within an image. Use the docker image history command to see the layers in the getting-started image you created earlier in the tutorial. bash docker image history getting-started You should get output that looks something like this (dates/IDs may be different). plaintext IMAGE CREATED CREATED BY SIZE COMMENT a78a40cbf866 18 seconds ago /bin/sh -c #(nop) CMD [\"node\" \"src/index.j\u2026 0B f1d1808565d6 19 seconds ago /bin/sh -c yarn install --production 85.4MB a2c054d14948 36 seconds ago /bin/sh -c #(nop) COPY dir:5dc710ad87c789593\u2026 198kB 9577ae713121 37 seconds ago /bin/sh -c #(nop) WORKDIR /app 0B b95baba1cfdb 13 days ago /bin/sh -c #(nop) CMD [\"node\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ENTRYPOINT [\"docker-entry\u2026 0B <missing> 13 days ago /bin/sh -c #(nop) COPY file:238737301d473041\u2026 116B <missing> 13 days ago /bin/sh -c apk add --no-cache --virtual .bui\u2026 5.35MB <missing> 13 days ago /bin/sh -c #(nop) ENV YARN_VERSION=1.21.1 0B <missing> 13 days ago /bin/sh -c addgroup -g 1000 node && addu\u2026 74.3MB <missing> 13 days ago /bin/sh -c #(nop) ENV NODE_VERSION=12.14.1 0B <missing> 13 days ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ADD file:e69d441d729412d24\u2026 5.59MB Each of the lines represents a layer in the image. The display here shows the base at the bottom with the newest layer at the top. Using this, you can also quickly see the size of each layer, helping diagnose large images. You'll notice that several of the lines are truncated. If you add the --no-trunc flag, you'll get the full output (yes... funny how you use a truncated flag to get untruncated output, huh?) bash docker image history --no-trunc getting-started","title":"Image Layering"},{"location":"tutorial/image-building-best-practices/#layer-caching","text":"Now that you've seen the layering in action, there's an important lesson to learn to help decrease build times for your container images. Once a layer changes, all downstream layers have to be recreated as well Let's look at the Dockerfile we were using one more time... FROM node:12-alpine WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] Going back to the image history output, we see that each command in the Dockerfile becomes a new layer in the image. You might remember that when we made a change to the image, the yarn dependencies had to be reinstalled. Is there a way to fix this? It doesn't make much sense to ship around the same dependencies every time we build, right? To fix this, we need to restructure our Dockerfile to help support the caching of the dependencies. For Node-based applications, those dependencies are defined in the package.json file. So, what if we copied only that file in first, install the dependencies, and then copy in everything else? Then, we only recreate the yarn dependencies if there was a change to the package.json . Make sense? Update the Dockerfile to copy in the package.json first, install dependencies, and then copy everything else in. dockerfile hl_lines=\"3 4 5\" FROM node:12-alpine WORKDIR /app COPY package.json yarn.lock ./ RUN yarn install --production COPY . . CMD [\"node\", \"src/index.js\"] Create a file named .dockerignore in the same folder as the Dockerfile with the following contents. ignore node_modules .dockerignore files are an easy way to selectively copy only image relevant files. You can read more about this here . In this case, the node_modules folder should be omitted in the second COPY step because otherwise, it would possibly overwrite files which were created by the command in the RUN step. For further details on why this is recommended for Node.js applications and other best practices, have a look at their guide on Dockerizing a Node.js web app . Build a new image using docker build . bash docker build -t getting-started . You should see output like this... plaintext Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Running in d53a06c9e4c2 yarn install v1.17.3 [1/4] Resolving packages... [2/4] Fetching packages... info fsevents@1.2.9: The platform \"linux\" is incompatible with this module. info \"fsevents@1.2.9\" is an optional dependency and failed compatibility check. Excluding it from installation. [3/4] Linking dependencies... [4/4] Building fresh packages... Done in 10.89s. Removing intermediate container d53a06c9e4c2 ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> a239a11f68d8 Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 49999f68df8f Removing intermediate container 49999f68df8f ---> e709c03bc597 Successfully built e709c03bc597 Successfully tagged getting-started:latest You'll see that all layers were rebuilt. Perfectly fine since we changed the Dockerfile quite a bit. Now, make a change to the src/static/index.html file (like change the <title> to say \"The Awesome Todo App\"). Build the Docker image now using docker build -t getting-started . again. This time, your output should look a little different. plaintext hl_lines=\"5 8 11\" Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> Using cache ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Using cache ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> cccde25a3d9a Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 2be75662c150 Removing intermediate container 2be75662c150 ---> 458e5c6f080c Successfully built 458e5c6f080c Successfully tagged getting-started:latest First off, you should notice that the build was MUCH faster! And, you'll see that steps 1-4 all have Using cache . So, hooray! We're using the build cache. Pushing and pulling this image and updates to it will be much faster as well. Hooray!","title":"Layer Caching"},{"location":"tutorial/image-building-best-practices/#multi-stage-builds","text":"While we're not going to dive into it too much in this tutorial, multi-stage builds are an incredibly powerful tool to help use multiple stages to create an image. There are several advantages for them: Separate build-time dependencies from runtime dependencies Reduce overall image size by shipping only what your app needs to run","title":"Multi-Stage Builds"},{"location":"tutorial/image-building-best-practices/#maventomcat-example","text":"When building Java-based applications, a JDK is needed to compile the source code to Java bytecode. However, that JDK isn't needed in production. Also, you might be using tools like Maven or Gradle to help build the app. Those also aren't needed in our final image. Multi-stage builds help. FROM maven AS build WORKDIR /app COPY . . RUN mvn package FROM tomcat COPY --from=build /app/target/file.war /usr/local/tomcat/webapps In this example, we use one stage (called build ) to perform the actual Java build using Maven. In the second stage (starting at FROM tomcat ), we copy in files from the build stage. The final image is only the last stage being created (which can be overridden using the --target flag).","title":"Maven/Tomcat Example"},{"location":"tutorial/image-building-best-practices/#react-example","text":"When building React applications, we need a Node environment to compile the JS code (typically JSX), SASS stylesheets, and more into static HTML, JS, and CSS. If we aren't doing server-side rendering, we don't even need a Node environment for our production build. Why not ship the static resources in a static nginx container? FROM node:12 AS build WORKDIR /app COPY package* yarn.lock ./ RUN yarn install COPY public ./public COPY src ./src RUN yarn run build FROM nginx:alpine COPY --from=build /app/build /usr/share/nginx/html Here, we are using a node:12 image to perform the build (maximizing layer caching) and then copying the output into an nginx container. Cool, huh?","title":"React Example"},{"location":"tutorial/image-building-best-practices/#recap","text":"By understanding a little bit about how images are structured, we can build images faster and ship fewer changes. Scanning images gives us confidence that the containers we are running and distributing are secure. Multi-stage builds also help us reduce overall image size and increase final container security by separating build-time dependencies from runtime dependencies.","title":"Recap"},{"location":"tutorial/part1_getting-started/","text":"Introduction. The application FEATHERS-OPCUA-SERVER is designed to implement the use of the library NodeOPCUA . NodeOPCUA is a OPC UA stack fully written in TypeScript for NodeJS . Basic properties: NodeOPCUA takes advantage of the asynchronous nature of node.js, creating highly responsive applications. NodeOPCUA has been developed using TDD and benefits from more than 2500 unit tests and 90% code coverage. NodeOPCUA can be use in Javascript as well as in Typescript. NodeOPCUA is free for commercial use. Check out the license . NodeOPCUA is available on GitHub . Check out the source code . NodeOPCUA runs on all the platforms that nodeJS supports. NodeOPCUA will benefit from a comprehensive SDK API documentation , numerous end-to-end functional tests, and a set of practical examples to help you learn how to use it. Distributed data collection system An application is configured for its host on which it is installed using a configuration file src\\api\\opcua\\config\\OPCUA_Config.json . In doing so, you can define separate configurations for this host. Each configuration has its own unique id , for example ua-cherkassy-azot-asutp_dev1 , you can also define the server and client and what tags they will work with. Also in the configuration, you can determine which database to work with and in which mode, which data transfer protection mode to use, and how to authenticate the user. Details can be found here . Why do you need a framework. There was an idea to embed the implementation of the library NodeOPCUA in the framework in order to use the features of this framework, such as: Working with databases. Access to data through protocols REST API and Websockets . Users authentication mechanisms. Create a client application and display OPC UA data in the client application. Why the framework FeathersJS was chosen. FeathersJS is a set of tools and an architecture template that makes it easy to build scalable real-time REST APIs. Comparison with other frameworks can be found here . The main idea of the FeathersJS framework is to move away from the MVC concept and use the concept of working with services and hooks . This made it possible to create a structure that can grow with you as your product grows. It's flexible enough to quickly adapt to changing business needs, powerful enough to build modern applications, and simple enough to build and run quickly. Here is an overview of the API documentation fit together:","title":"Part1. Getting started"},{"location":"tutorial/part1_getting-started/#introduction","text":"The application FEATHERS-OPCUA-SERVER is designed to implement the use of the library NodeOPCUA . NodeOPCUA is a OPC UA stack fully written in TypeScript for NodeJS . Basic properties: NodeOPCUA takes advantage of the asynchronous nature of node.js, creating highly responsive applications. NodeOPCUA has been developed using TDD and benefits from more than 2500 unit tests and 90% code coverage. NodeOPCUA can be use in Javascript as well as in Typescript. NodeOPCUA is free for commercial use. Check out the license . NodeOPCUA is available on GitHub . Check out the source code . NodeOPCUA runs on all the platforms that nodeJS supports. NodeOPCUA will benefit from a comprehensive SDK API documentation , numerous end-to-end functional tests, and a set of practical examples to help you learn how to use it.","title":"Introduction."},{"location":"tutorial/part1_getting-started/#distributed-data-collection-system","text":"An application is configured for its host on which it is installed using a configuration file src\\api\\opcua\\config\\OPCUA_Config.json . In doing so, you can define separate configurations for this host. Each configuration has its own unique id , for example ua-cherkassy-azot-asutp_dev1 , you can also define the server and client and what tags they will work with. Also in the configuration, you can determine which database to work with and in which mode, which data transfer protection mode to use, and how to authenticate the user. Details can be found here .","title":"Distributed data collection system"},{"location":"tutorial/part1_getting-started/#why-do-you-need-a-framework","text":"There was an idea to embed the implementation of the library NodeOPCUA in the framework in order to use the features of this framework, such as: Working with databases. Access to data through protocols REST API and Websockets . Users authentication mechanisms. Create a client application and display OPC UA data in the client application.","title":"Why do you need a framework."},{"location":"tutorial/part1_getting-started/#why-the-framework-feathersjs-was-chosen","text":"FeathersJS is a set of tools and an architecture template that makes it easy to build scalable real-time REST APIs. Comparison with other frameworks can be found here . The main idea of the FeathersJS framework is to move away from the MVC concept and use the concept of working with services and hooks . This made it possible to create a structure that can grow with you as your product grows. It's flexible enough to quickly adapt to changing business needs, powerful enough to build modern applications, and simple enough to build and run quickly. Here is an overview of the API documentation fit together:","title":"Why the framework FeathersJS was chosen."},{"location":"tutorial/part2_framework-overview/","text":"Introducing FeathersJS FeathersJS a flexible, real-time JavaScript framework built on top of Express for the server and as a standalone client for the browser and React Native. FeathersJS isn\u2019t just another Rails clone. Instead of the typical MVC pattern it encourages a Service Oriented Architecture paired with Cross-cutting Concerns allowing you to build complex real-time apps and scalable REST APIs very quickly and with very little code. Sounds too good to be true? Once you try it you\u2019ll see that you can build prototypes in minutes and flexible, scalable, production-ready apps in days. Not weeks or months. Modern, solid, and 100% JavaScript Feathers is built using promises and ES6 so you can build your apps with the latest JavaScript features and write terse, elegant code. Feathers itself is only a few hundred lines of code and is a fully compatible wrapper over top of Express , Socket.io and Primus , all of which have been used in production by thousands of companies. Universal Feathers can be used in the browser, React Native and server side and provides everything you need to structure your application and communicate with a Feathers server while still letting you pick your favourite view engine. Using the Feathers client you can quickly add authentication, share validation and business logic code between your server and client, and easily make your apps real-time. Framework Friendly Feathers is completely client agnostic and easily integrates with any client side framework. It plays especially well with React, Angular and React Native. They\u2019re practically BFFs. We have guides for some of the most popular JS frameworks and are adding new ones every week. Service Oriented Services are the core of Feathers. They provide instant CRUD functionality for a resource through a series of familiar methods; find, get, create, update, patch, and remove. Almost any resource can be mapped to these actions; external APIs, database resources, file uploads, you name it. This consistent interface makes it easy to \u201chook\u201d into these CRUD actions to provide custom functionality. For example, if you have a socket transport like Socket.io enabled, Feathers will automatically emit created, updated, patched, and removed events for you. Feathers gives you the structure to build service oriented apps from day one by keeping services discrete. If you eventually need to split up your app into microservices it\u2019s an easy transition and your Feathers apps can scale painlessly. Instant Real-time REST APIs Since Feathers provides instant CRUD functionality via Services , it also exposes both a RESTful and real-time API automatically through HTTP/HTTPS and over websockets. Feathers allows you to send and receive data over sockets similar to Meteor\u2019s DDP so you can use Primus or Socket.io for your sole app communication\u2026 or not. Feathers gives you the flexibility to choose how you want to expose your REST API ; over HTTP(S) , websockets or both \u2014 and it does this with just a few lines of code. Datastore Agnostic Feathers has adapters for 15+ data sources and 4 different ORMs out of the box. More than any other real-time framework! This gives you the ability to access data in MongoDB , Postgres , MySQL , Sequel Server , S3 and more! You can have multiple datastores in a single app and swap them out painlessly due to our consistent query interface . Incredibly Pluggable We like to consider Feathers as a \u201cbatteries included but easily swappable framework\u201d. We have entirely optional plugins that provide authentication , SMS , or email messaging out of the box. You can include exactly what you need, typically in just a couple lines of code. No more, no less.","title":"Part2. FeathersJS framework overview"},{"location":"tutorial/part2_framework-overview/#introducing-feathersjs","text":"FeathersJS a flexible, real-time JavaScript framework built on top of Express for the server and as a standalone client for the browser and React Native. FeathersJS isn\u2019t just another Rails clone. Instead of the typical MVC pattern it encourages a Service Oriented Architecture paired with Cross-cutting Concerns allowing you to build complex real-time apps and scalable REST APIs very quickly and with very little code. Sounds too good to be true? Once you try it you\u2019ll see that you can build prototypes in minutes and flexible, scalable, production-ready apps in days. Not weeks or months.","title":"Introducing FeathersJS"},{"location":"tutorial/part2_framework-overview/#modern-solid-and-100-javascript","text":"Feathers is built using promises and ES6 so you can build your apps with the latest JavaScript features and write terse, elegant code. Feathers itself is only a few hundred lines of code and is a fully compatible wrapper over top of Express , Socket.io and Primus , all of which have been used in production by thousands of companies.","title":"Modern, solid, and 100% JavaScript"},{"location":"tutorial/part2_framework-overview/#universal","text":"Feathers can be used in the browser, React Native and server side and provides everything you need to structure your application and communicate with a Feathers server while still letting you pick your favourite view engine. Using the Feathers client you can quickly add authentication, share validation and business logic code between your server and client, and easily make your apps real-time.","title":"Universal"},{"location":"tutorial/part2_framework-overview/#framework-friendly","text":"Feathers is completely client agnostic and easily integrates with any client side framework. It plays especially well with React, Angular and React Native. They\u2019re practically BFFs. We have guides for some of the most popular JS frameworks and are adding new ones every week.","title":"Framework Friendly"},{"location":"tutorial/part2_framework-overview/#service-oriented","text":"Services are the core of Feathers. They provide instant CRUD functionality for a resource through a series of familiar methods; find, get, create, update, patch, and remove. Almost any resource can be mapped to these actions; external APIs, database resources, file uploads, you name it. This consistent interface makes it easy to \u201chook\u201d into these CRUD actions to provide custom functionality. For example, if you have a socket transport like Socket.io enabled, Feathers will automatically emit created, updated, patched, and removed events for you. Feathers gives you the structure to build service oriented apps from day one by keeping services discrete. If you eventually need to split up your app into microservices it\u2019s an easy transition and your Feathers apps can scale painlessly.","title":"Service Oriented"},{"location":"tutorial/part2_framework-overview/#instant-real-time-rest-apis","text":"Since Feathers provides instant CRUD functionality via Services , it also exposes both a RESTful and real-time API automatically through HTTP/HTTPS and over websockets. Feathers allows you to send and receive data over sockets similar to Meteor\u2019s DDP so you can use Primus or Socket.io for your sole app communication\u2026 or not. Feathers gives you the flexibility to choose how you want to expose your REST API ; over HTTP(S) , websockets or both \u2014 and it does this with just a few lines of code.","title":"Instant Real-time REST APIs"},{"location":"tutorial/part2_framework-overview/#datastore-agnostic","text":"Feathers has adapters for 15+ data sources and 4 different ORMs out of the box. More than any other real-time framework! This gives you the ability to access data in MongoDB , Postgres , MySQL , Sequel Server , S3 and more! You can have multiple datastores in a single app and swap them out painlessly due to our consistent query interface .","title":"Datastore Agnostic"},{"location":"tutorial/part2_framework-overview/#incredibly-pluggable","text":"We like to consider Feathers as a \u201cbatteries included but easily swappable framework\u201d. We have entirely optional plugins that provide authentication , SMS , or email messaging out of the box. You can include exactly what you need, typically in just a couple lines of code. No more, no less.","title":"Incredibly Pluggable"},{"location":"tutorial/part3_app-features/","text":"Application Features Working with services on the server side and on the client side . Using Hooks when working with services. Creation of Real-time APIs . NodeOPCUA library functions are implemented as two classes OpcuaServer and OpcuaClient . An instances of the class OpcuaServer is placed in the service opcua-servers as a list item. An instances of the class OpcuaClient is placed in the service opcua-clients as a list item. OPC UA tags are stored in the database MongoDB or neDB . Operations with tags occur through the service opcua-tags . OPC UA values are stored in the database MongoDB or neDB . Operations with values occur through the service opcua-values . User registration / logging procedure is provided. Authentication process is based on Express Password strategies. JWT Authentication uses JSON Web Token . Local Authentication is used by Email and Password . OAuth 2.0 Authentication via Google , GitHub . The authorization process is based on feathers-castle strategies. Provides Email validation when registering a user. Provides validation when Email is changed by the user. Provides a procedure for recovering a password forgotten by the user. Provides user password change procedure. The administrator can manage users: assign roles, divide by groups, change user activity, delete a user. Working with the database: MongoDB or neDB through services. Site event logging procedure provided. Created tests for the server side.","title":"Part3. Application Features"},{"location":"tutorial/part3_app-features/#application-features","text":"Working with services on the server side and on the client side . Using Hooks when working with services. Creation of Real-time APIs . NodeOPCUA library functions are implemented as two classes OpcuaServer and OpcuaClient . An instances of the class OpcuaServer is placed in the service opcua-servers as a list item. An instances of the class OpcuaClient is placed in the service opcua-clients as a list item. OPC UA tags are stored in the database MongoDB or neDB . Operations with tags occur through the service opcua-tags . OPC UA values are stored in the database MongoDB or neDB . Operations with values occur through the service opcua-values . User registration / logging procedure is provided. Authentication process is based on Express Password strategies. JWT Authentication uses JSON Web Token . Local Authentication is used by Email and Password . OAuth 2.0 Authentication via Google , GitHub . The authorization process is based on feathers-castle strategies. Provides Email validation when registering a user. Provides validation when Email is changed by the user. Provides a procedure for recovering a password forgotten by the user. Provides user password change procedure. The administrator can manage users: assign roles, divide by groups, change user activity, delete a user. Working with the database: MongoDB or neDB through services. Site event logging procedure provided. Created tests for the server side.","title":"Application Features"},{"location":"tutorial/part4_resources-info/","text":"Resources info Node.js (Chrome's V8 JavaScript engine) Documentation v12.22.12. Guides . npm is package manager. yarn v1.* (Classic) is package manager. NodeOPCUA is a OPC UA stack fully written in TypeScript for NodeJS What is OPC UA . About NodeOPCUA library. GitHub . API documentation . Express (Node.js framework) Documentation . API . Passport.js Documentation . FeathersJS Documentation . API . Check our blog posts . Resources, plugins, examples . MongoDB (noSQL DataBase) Documentation Atlas (cloud hosting). \u0421ompass (data visualization tool). Mongoosejs (NPM component for working with database models). NeDB (NoSQL datastore that mimics MongoDB) GitHub . feathers-nedb .","title":"Part4. Resources info"},{"location":"tutorial/part4_resources-info/#resources-info","text":"Node.js (Chrome's V8 JavaScript engine) Documentation v12.22.12. Guides . npm is package manager. yarn v1.* (Classic) is package manager. NodeOPCUA is a OPC UA stack fully written in TypeScript for NodeJS What is OPC UA . About NodeOPCUA library. GitHub . API documentation . Express (Node.js framework) Documentation . API . Passport.js Documentation . FeathersJS Documentation . API . Check our blog posts . Resources, plugins, examples . MongoDB (noSQL DataBase) Documentation Atlas (cloud hosting). \u0421ompass (data visualization tool). Mongoosejs (NPM component for working with database models). NeDB (NoSQL datastore that mimics MongoDB) GitHub . feathers-nedb .","title":"Resources info"},{"location":"tutorial/part5_installation/","text":"Installation Make sure you have installed NodeJS v12.* MongoDB x64 v6.* (if used) MongoDB x32 v3.2.22 (if used) Yarn v1.* Clone or download the project from GitHub . Install your dependencies: cd to/<feathers-opcua-server> yarn install If you are using a database MongoDB , then you need to start it before testing or working, using the command: npm run start-mongod or npm run start-mongod32 Environment variables The .env file must be located at the root of the project, if it is missing, then it must be created based on the example file .env.example , otherwise the application will throw an error . The .env file sets environment variables. Environment variables usually contain user secrets, such as user_id , user_secret , etc. e.g. ... # MONGODB MONGODB_DEV_URL=\"mongodb://localhost:27017/feathers_opcua_devserver\" MONGODB_TEST_URL=\"mongodb://localhost:27017/feathers_opcua_testserver\" MONGODB_PROD_URL=\"mongodb://localhost:27017/feathers_opcua_prodserver\" # NEDB NEDB_DEV_PATH=\"nedb://../data/nedb/dev\" NEDB_TEST_PATH=\"nedb://../data/nedb/test\" NEDB_PROD_PATH=\"nedb://../data/nedb/prod\" Testing Before working with the project, you need to start testing using the command. npm run test:all All tests in the /test directory will be executed. If you need to run a separate test, you need to move it to the /test/debug directory and run the command. npm run test:debug The test in the /test/debug directory will be executed. This sets the environment variable NODE_ENV to test mode. cross-env NODE_ENV=test In this case, all test data is written to the test database. e.g. mongodb://localhost:27017/feathers_opcua_testserver or nedb://../data/nedb/test Script testing When testing scripts, the command is used: npm run test:script The script test number must be set in the command execution arguments in the package.json file, see here: \"scripts\": { ... \"mocha:script\": \"... --timeout 60000 --script=#4.2\", } This will run the script in test mode /scripts/test/#4-scriptRunSessionCommand.js . Writing Test Data to the Database To write the test data from the /seeds/fake-data.json file to the test database, you need to run the command: npm run start:seed In this case, the test data will be written to the test database e.g. mongodb://localhost:27017/feathers_opcua_testserver or nedb://../data/nedb/test Launch of the CLI Scripts The links to the cli scripts are in the package.json file, see here: ... \"bin\": { \"callOpcuaMethod\": \"./scripts/cli/callOpcuaMethod.js\", \"runOpcuaCommand\": \"./scripts/cli/runOpcuaCommand.js\", \"createAcmYearTemplate\": \"./scripts/cli/createAcmYearTemplate.js\", \"getAcmDayReportsData\": \"./scripts/cli/getAcmDayReportsData.js\" }, ... To create links to cli scripts , you need to run the command: npm run clone:cli-scripts Launch of the project In development mode Run the application in development mode with the command: npm run dev his sets the environment variable NODE_ENV to development mode. cross-env NODE_ENV=development In this case, all development data is written to the development database. e.g. mongodb://localhost:27017/feathers_opcua_devserver or nedb://../data/nedb/dev In production mode Run the application in production mode with the command: npm start his sets the environment variable NODE_ENV to production mode. cross-env NODE_ENV=production In this case, all production data is written to the production database. e.g. mongodb://localhost:27017/feathers_opcua_prodserver or nedb://../data/nedb/prod","title":"Part5. Installation"},{"location":"tutorial/part5_installation/#installation","text":"Make sure you have installed NodeJS v12.* MongoDB x64 v6.* (if used) MongoDB x32 v3.2.22 (if used) Yarn v1.* Clone or download the project from GitHub . Install your dependencies: cd to/<feathers-opcua-server> yarn install If you are using a database MongoDB , then you need to start it before testing or working, using the command: npm run start-mongod or npm run start-mongod32","title":"Installation"},{"location":"tutorial/part5_installation/#environment-variables","text":"The .env file must be located at the root of the project, if it is missing, then it must be created based on the example file .env.example , otherwise the application will throw an error . The .env file sets environment variables. Environment variables usually contain user secrets, such as user_id , user_secret , etc. e.g. ... # MONGODB MONGODB_DEV_URL=\"mongodb://localhost:27017/feathers_opcua_devserver\" MONGODB_TEST_URL=\"mongodb://localhost:27017/feathers_opcua_testserver\" MONGODB_PROD_URL=\"mongodb://localhost:27017/feathers_opcua_prodserver\" # NEDB NEDB_DEV_PATH=\"nedb://../data/nedb/dev\" NEDB_TEST_PATH=\"nedb://../data/nedb/test\" NEDB_PROD_PATH=\"nedb://../data/nedb/prod\"","title":"Environment variables"},{"location":"tutorial/part5_installation/#testing","text":"Before working with the project, you need to start testing using the command. npm run test:all All tests in the /test directory will be executed. If you need to run a separate test, you need to move it to the /test/debug directory and run the command. npm run test:debug The test in the /test/debug directory will be executed. This sets the environment variable NODE_ENV to test mode. cross-env NODE_ENV=test In this case, all test data is written to the test database. e.g. mongodb://localhost:27017/feathers_opcua_testserver or nedb://../data/nedb/test","title":"Testing"},{"location":"tutorial/part5_installation/#script-testing","text":"When testing scripts, the command is used: npm run test:script The script test number must be set in the command execution arguments in the package.json file, see here: \"scripts\": { ... \"mocha:script\": \"... --timeout 60000 --script=#4.2\", } This will run the script in test mode /scripts/test/#4-scriptRunSessionCommand.js .","title":"Script testing"},{"location":"tutorial/part5_installation/#writing-test-data-to-the-database","text":"To write the test data from the /seeds/fake-data.json file to the test database, you need to run the command: npm run start:seed In this case, the test data will be written to the test database e.g. mongodb://localhost:27017/feathers_opcua_testserver or nedb://../data/nedb/test","title":"Writing Test Data to the Database"},{"location":"tutorial/part5_installation/#launch-of-the-cli-scripts","text":"The links to the cli scripts are in the package.json file, see here: ... \"bin\": { \"callOpcuaMethod\": \"./scripts/cli/callOpcuaMethod.js\", \"runOpcuaCommand\": \"./scripts/cli/runOpcuaCommand.js\", \"createAcmYearTemplate\": \"./scripts/cli/createAcmYearTemplate.js\", \"getAcmDayReportsData\": \"./scripts/cli/getAcmDayReportsData.js\" }, ... To create links to cli scripts , you need to run the command: npm run clone:cli-scripts","title":"Launch of the CLI Scripts"},{"location":"tutorial/part5_installation/#launch-of-the-project","text":"","title":"Launch of the project"},{"location":"tutorial/part5_installation/#in-development-mode","text":"Run the application in development mode with the command: npm run dev his sets the environment variable NODE_ENV to development mode. cross-env NODE_ENV=development In this case, all development data is written to the development database. e.g. mongodb://localhost:27017/feathers_opcua_devserver or nedb://../data/nedb/dev","title":"In development mode"},{"location":"tutorial/part5_installation/#in-production-mode","text":"Run the application in production mode with the command: npm start his sets the environment variable NODE_ENV to production mode. cross-env NODE_ENV=production In this case, all production data is written to the production database. e.g. mongodb://localhost:27017/feathers_opcua_prodserver or nedb://../data/nedb/prod","title":"In production mode"},{"location":"tutorial/part6_configuration/","text":"Configuration @feathersjs/configuration It is a wrapper for node-config which allows to configure a server side Feathers application. By default this implementation will look in config/* for default.json which retains convention. It will be merged with other configuration files in the config/ folder using the NODE_ENV environment variable. So setting NODE_ENV=production will merge config/default.json with config/production.json . As per the config docs you can organize \"hierarchical configurations for your app deployments\" . .env The .env file must be located at the root of the project, if it is missing, then it must be created based on the example file .env.example , otherwise the application will throw an error . The .env file sets environment variables. Environment variables usually contain user secrets, such as user_id , user_secret , etc. Example for .env.example . ### SERVER ### #-------------# HOST=\"localhost\" PORT=3030 BASE_URL=\"http://localhost:3030\" ... ### AUTH ### #----------# # External accounts to login EXTERNAL_ACCOUNTS=\"google;github\" # yes | no; true | false; 1 | 0 MY_LOCALHOST_TO_IP=0 # yes | no; true | false; 1 | 0 IS_AUTH_MANAGER=0 # yes | no; true | false; 1 | 0 SET_USER_ACTIVE=1 ... ### OPC-UA ### #------------# # localUpdate|localAdd|remoteUpdate|remoteAdd|no DEFAULT_OPCUA_SAVEMODE_TODB=\"no\" # client|server|asyncServer DEFAULT_EXECUTE_METHODS_FROM=\"client\" # http://localhost:3131 DEFAULT_OPCUA_REMOTE_DB_URL=\"http://localhost:3131\" # yes | no; true | false; 1 | 0 OPCUA_BOOTSTRAP_ENABLE=1 # -1(no limits) | 1..n(n limit) OPCUA_VALUES_MAXROWS=10 # AuthenticatedUser, Supervisor, Administrator OPCUA_USER_LOGIN=\"xxxxxxxxxxxxxx\" OPCUA_USER_PASS=\"xxxxxxxxxxxxxx\" OPCUA_ADMIN_LOGIN=\"xxxxxxxxxxxxxx\" OPCUA_ADMIN_PASS=\"xxxxxxxxxxxxxx\" OPCUA_KEP_LOGIN=\"xxxxxxxxxxxxxx\" OPCUA_KEP_PASS=\"xxxxxxxxxxxxxx\" ### LOG ### #----------# # yes | no; true | false; 1 | 0 LOGMSG_ENABLE=1 # -1(no limits) | 1..n(n limit) LOGMSG_MAXROWS=100 ### SECRET DATA ### #-----------------# GITHUB_ID=\"xxxxxxxxxxxxxxxxxxxxxxxx\" GITHUB_SECRET=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" GOOGLE_ID=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.apps.googleusercontent.com\" GOOGLE_SECRET=\"xxxxxxxxxxxxxxxxxxxxxxxxxxx\" ### DATA-BASE ### #-----------------# # MSSQL MSSQL_ASODU_TEST_ID=\"OGMT-MZTP.FIRST.dbBSA\" MSSQL_ASODU_TEST_USER=\"xxxxxxxxxxxxxxx\" MSSQL_ASODU_TEST_PASS=\"xxxxxxxxxxxxxxx\" MSSQL_BSAHOME_TEST_ID=\"BSA-HOME.SQLEXPRESS.Feathers_test\" MSSQL_BSAHOME_TEST_USER=\"xxxx\" MSSQL_BSAHOME_TEST_PASS=\"xxxxxxxxxxxxx\" # DEFAULT TYPE DB (mongodb | nedb) DEFAULT_TYPE_DB=\"nedb\" # MONGODB MONGODB_DEV_URL=\"mongodb://localhost:27017/feathers_opcua_devserver\" MONGODB_TEST_URL=\"mongodb://localhost:27017/feathers_opcua_testserver\" MONGODB_PROD_URL=\"mongodb://localhost:27017/feathers_opcua_prodserver\" # NEDB NEDB_DEV_PATH=\"nedb://../data/nedb/dev\" NEDB_TEST_PATH=\"nedb://../data/nedb/test\" NEDB_PROD_PATH=\"nedb://../data/nedb/prod\" Note: To have real keys to access the API of various services, you need to register your application in these services. Examples of registering an application and obtaining API keys in different services can be found here . Main sections of the environment variable file .env : SERVER - used in configuration files \\config\\default.json , \\config\\development.json , \\config\\production.json , \\config\\test.json . AUTH - used to authenticate and authorize users to access application resources. OPC-UA - used to set the parameters of the library NodeOPCUA. LOG - used for logging events. SECRET DATA - secret data for services (GOOGLE, GITHUB). DATA-BASE - database information (MSSQL, MONGODB, NEDB). feathers-specs This configuration file is located in the folder \\config\\feathers-specs.json . Main sections of feathers-specs : envTestModeName - the value is set to a variable NODE_ENV for the test mode. { \"app\": { \"envTestModeName\": \"test\", ... }, ... } envAllowingSeedData - modes for which it is allowed to change data in the database with \\seeds\\fake-data.json . { \"app\": { ... \"envAllowingSeedData\": [\"test\"], ... }, ... } providers - transport types used to transfer data. { \"app\": { ... \"providers\": [ \"rest\", \"socketio\" ] ... }, ... } services - service properties such as name, path, etc. { \"services\": { \"users\": { \"name\": \"users\", \"nameSingular\": \"user\", \"subFolder\": \"\", \"fileName\": \"users\", \"adapter\": \"nedb\", \"path\": \"/users\", \"isAuthEntity\": true, \"requiresAuth\": true, \"overriddenAuth\": { \"create\": \"noauth\", \"get\": \"auth\", \"find\": \"auth\", \"update\": \"auth\", \"patch\": \"auth\", \"remove\": \"auth\" } }, }, ... } hooks - hook properties such as fileName, ifMulti, etc. { \"hooks\": { \"normalize\": { \"fileName\": \"normalize\", \"camelName\": \"normalize\", \"ifMulti\": \"y\", \"multiServices\": [ \"*app\" ], \"singleService\": \"\" }, ... }, ... } OPCUA Config An application is configured for its host on which it is installed using a configuration file src\\api\\opcua\\config\\OPCUA_Config.json . In doing so, you can define separate configurations for this host. Each configuration has its own unique number, for example ua-cherkassy-azot-asutp_dev1 , you can also define the server and client and what tags they will work with. Also in the configuration, you can determine which database to work with and in which mode, which data transfer protection mode to use, and how to authenticate the user. Main sections of the config file OPCUA_Config : id - unique configuration code. name - unique configuration name. endpointUrl - opcua server endpoint url. endpointPort - opcua server endpoint port. srvServiceUrl - features framework host url for opcua-servers service. clientServiceUrl - features framework host url for opcua-clients service. clientScript - the name of the function to initialize the subscription on the client at startup src\\plugins\\opcua\\opcua-bootstrap.js . executeMethodsFrom - the place where the method should be executed, possible values: client|server|asyncServer . hostTypeDB - the type of database the application will work with, possible values: (mongodb|nedb) . opcuaSaveModeToDB - database operation mode, working or not, local or remote database, updating or adding current opcua-values , possible values: localUpdate|localAdd|remoteUpdate|remoteAdd|no . opcuaRemoteDbUrl - the url of the database where we will store the opcua-tags or opcua-values data. opcuaBootstrapParams - parameters at startup of src\\plugins\\opcua\\opcua-bootstrap.js . security - opcua data transfer security settings and user authentication. NodeId - parameters for the client to receive specific values NodeId from the server. e.g. nodeName . include tests - what tests will we run on this host e.g. [\"mssql-tedious.test.js\",\"mssql-datasets.test.js\", \"http-operations.test.js\"] . paths['base-options'] - base tag paths. paths['options'] - path to tags for a specific configuration, tags that override tag values from base paths. [ ... { \"isEnable\": true, \"id\": \"ua-cherkassy-azot-asutp_dev1\", \"name\": \"380-472-00203826-asutp_dev1\", \"description\": \"Opcua server and client for `azot-m5`, `azot-ogmtmztp` tags from `opc.tcp://M5-0095488.OSTCHEM.COM.UA:26570`\", \"endpointUrl\": \"opc.tcp://M5-0095488.OSTCHEM.COM.UA:26570\", \"endpointPort\": 26570, \"srvServiceUrl\": \"http://10.60.0.220:3030\", \"clientServiceUrl\": \"http://10.60.0.220:3030\", \"clientScript\": \"startSubscriptionMonitor\", \"executeMethodsFrom\": \"client\", \"hostTypeDB\": \"mongodb\", \"opcuaSaveModeToDB\": \"localUpdate\", \"opcuaRemoteDbUrl\": \"http://M5-0095488.OSTCHEM.COM.UA:3131\", \"opcuaBootstrapParams\": { \"clearHistoryAtStartup\": false, \"syncHistoryAtStartup\": { \"active\": false, \"methodName\": \"methodAcmDayReportsDataGet\" }, \"syncReportAtStartup\": { \"active\": false, \"methodName\": \"methodAcmYearReportUpdate\" } }, \"security\": { \"mode\": \"SignAndEncrypt\", \"policy\": \"Basic256Sha256\", \"userName\": \"OPCUA_ADMIN_LOGIN\", \"password\": \"OPCUA_ADMIN_PASS\" }, \"NodeId\": { \"namespaceIndex\": 2, \"identifierType\": \"displayName\", \"identifierPrefix\": \"Channel1.Device1\", \"addObjectItem\": true }, \"include\": { \"tests\": [ \"mssql-tedious.test.js\", \"mssql-datasets.test.js\", \"http-operations.test.js\" ] }, \"paths\": { \"base-options\": [ \"/src/api/opcua/tags/ua-cherkassy-azot-m5_prod1\", \"/src/api/opcua/tags/ua-cherkassy-azot-ogmttsd_prod1\" ], \"options\": \"/src/api/opcua/tags/ua-cherkassy-azot-asutp_dev1/AddressSpaceOptions.json\", \"getters\": \"\", \"methods\": \"\", \"subscriptions\": \"\", \"client-scripts\": \"\" } } ... ] OPCUA Tags An application is configured for its host on which it is installed using a configuration file src\\api\\opcua\\config\\OPCUA_Config.json . In doing so, you can define separate configurations for this host. Each configuration has its own unique number, for example ua-cherkassy-azot-asutp_dev1 , you can also define the server and client and what tags they will work with. The configuration specifies the base paths paths['base-options'] to the tag files and the config path paths['options'] to the tag file, which overrides the values of the base tags. Example of base path tags: { \"objects\": [ { \"isEnable\": true, \"browseName\": \"CH_M5\", \"displayName\": \"Workshop M5 Cherkasy `AZOT`\", \"type\": \"object\" }, { \"isEnable\": true, \"browseName\": \"CH_M51\", \"displayName\": \"Workshop M5 dep.1 Cherkassy `AZOT`\", \"type\": \"object\" } ], \"variables\": [ { \"browseName\": \"CH_M51::ValueFromFile\", \"displayName\": \"Values from file for CH_M51\", \"ownerName\": \"CH_M51\", \"type\": \"variable.simple\", \"dataType\": \"String\", \"hist\": 1, \"group\": true, \"subscription\": \"onChangedGroupHandlerForDB\", \"variableGetType\": \"valueFromSource\", \"getter\": \"getterHistValueFromPath\", \"getterParams\": { \"path\": \"//192.168.3.5/www_m5/m5_data2\" } } ], \"groups\": [ { \"browseName\": \"CH_M51::01AMIAK:01T4\", \"aliasName\": \"01NH3_T4\", \"displayName\": \"01NH3_T4\", \"ownerName\": \"CH_M51\", \"ownerGroup\": \"CH_M51::ValueFromFile\", \"type\": \"variable.analog\", \"dataType\": \"Double\", \"hist\": 1, \"variableGetType\": \"valueFromSource\", \"getter\": \"getterPlugForVariable\", \"valueParams\": { \"engineeringUnitsRange\": { \"low\": 0, \"high\": 200 }, \"engineeringUnits\": \"degree_celsius\" }, \"description\": \"Ammonia gas temperature NH3 (T4)\" }, ], \"methods\": [ { \"browseName\": \"CH_M5::YearTemplateCreate\", \"displayName\": \"Create an annual report template\", \"ownerName\": \"CH_M5\", \"type\": \"method\", \"bindMethod\": \"methodAcmYearTemplateCreate\", \"inputArguments\": [ { \"name\": \"methodParameters\", \"description\": { \"text\": \"Method parameters\" }, \"dataType\": \"String\" } ], \"outputArguments\": [ { \"name\": \"methodExecutionResult\", \"description\": { \"text\": \"Method execution result\" }, \"dataType\": \"String\", \"valueRank\": 1 } ], \"userAccessLevel\": { \"inputArguments\": \"CurrentRead\", \"outputArguments\": \"CurrentRead\" } } ] } Example of config path tags: { \"objects\": [ { \"isEnable\": false, \"browseName\": \"CH_M5\", \"displayName\": \"Workshop M5 Cherkasy `AZOT`\", \"type\": \"object\" }, { \"isEnable\": true, \"browseName\": \"CH_M51\", \"displayName\": \"Workshop M5 dep.1 Cherkassy `AZOT`\", \"type\": \"object\" } ], \"variables\": [ { \"browseName\": \"CH_M51::ValueFromFile\", \"displayName\": \"Values from file for CH_M51\", \"ownerName\": \"CH_M51\", \"type\": \"variable.simple\", \"dataType\": \"String\", \"hist\": 1, \"group\": true, \"subscription\": \"onChangedGroupHandlerForDB\", \"variableGetType\": \"valueFromSource\", \"getter\": \"getterHistValueFromHttpPath\", \"getterParams\": { \"path\": \"http://192.168.3.5/www_m5/m5_data2/\", \"interval\": 20000 } } ] } Main sections of the tag file: objects - set of object tags. All tags (\"variables\", \"groups\", \"methods\") belong to some object tag through the ownerName field (e.g. \"ownerName\": \"CH_M51\" ). variables - set of variable tags. If the \"group\" field is set to true, then this tag is the owner of the tag group (e.g. \"group\": true ). groups - set of group tags. These are tags that belong to a tag group. The name of the owner of this group is indicated in the ownerGroup field (e.g. \"ownerGroup\": \"CH_M51::ValueFromFile\" ). methods - set of method tags. browseName - browse name (e.g. \"browseName\": \"CH_M51::ValueFromFile\" ). displayName - display name (e.g. \"displayName\": \"Values from file for CH_M51\" ). ownerName - name of owner (e.g. \"ownerName\": \"CH_M51\" ). type - variable type (e.g. \"type\": \"variable.simple\" ). dataType - data type (e.g. \"dataType\": \"Double\" ). hist - story size (e.g. \"hist\": 1 ). group - group tag (e.g. \"group\": true ). ownerGroup - owner tag name (e.g. \"ownerGroup\": \"CH_M51::ValueFromFile\" ). variableGetType - data source for a variable (e.g. \"variableGetType\": \"valueFromSource\" ). subscription - variable change event handler function name (e.g. \"subscription\": \"onChangedGroupHandlerForDB\" ). getter - the name of the get data function for the variable (e.g. \"getter\": \"getterHistValueFromHttpPath\" ). getterParams - parameters for the get data function for a variable (e.g. \"getterParams\": { \"path\": \"http://192.168.3.5/www_m5/m5_data2/\", \"interval\": 20000 } ). valueParams.engineeringUnitsRange - engineering units range (e.g. valueParams.engineeringUnitsRange: { \"low\": 0, \"high\": 200 } ). valueParams.engineeringUnits - engineering units (e.g. valueParams.engineeringUnits: \"degree_celsius\" ). description - tag description (e.g. \"description\": \"Ammonia gas temperature NH3 (T4)\" ). bindMethod - bind method (e.g. \"bindMethod\": \"methodAcmYearTemplateCreate\" ). inputArguments - input arguments for the method. outputArguments - output arguments for the method.","title":"Part6. Configuration"},{"location":"tutorial/part6_configuration/#configuration","text":"","title":"Configuration"},{"location":"tutorial/part6_configuration/#feathersjsconfiguration","text":"It is a wrapper for node-config which allows to configure a server side Feathers application. By default this implementation will look in config/* for default.json which retains convention. It will be merged with other configuration files in the config/ folder using the NODE_ENV environment variable. So setting NODE_ENV=production will merge config/default.json with config/production.json . As per the config docs you can organize \"hierarchical configurations for your app deployments\" .","title":"@feathersjs/configuration"},{"location":"tutorial/part6_configuration/#env","text":"The .env file must be located at the root of the project, if it is missing, then it must be created based on the example file .env.example , otherwise the application will throw an error . The .env file sets environment variables. Environment variables usually contain user secrets, such as user_id , user_secret , etc. Example for .env.example . ### SERVER ### #-------------# HOST=\"localhost\" PORT=3030 BASE_URL=\"http://localhost:3030\" ... ### AUTH ### #----------# # External accounts to login EXTERNAL_ACCOUNTS=\"google;github\" # yes | no; true | false; 1 | 0 MY_LOCALHOST_TO_IP=0 # yes | no; true | false; 1 | 0 IS_AUTH_MANAGER=0 # yes | no; true | false; 1 | 0 SET_USER_ACTIVE=1 ... ### OPC-UA ### #------------# # localUpdate|localAdd|remoteUpdate|remoteAdd|no DEFAULT_OPCUA_SAVEMODE_TODB=\"no\" # client|server|asyncServer DEFAULT_EXECUTE_METHODS_FROM=\"client\" # http://localhost:3131 DEFAULT_OPCUA_REMOTE_DB_URL=\"http://localhost:3131\" # yes | no; true | false; 1 | 0 OPCUA_BOOTSTRAP_ENABLE=1 # -1(no limits) | 1..n(n limit) OPCUA_VALUES_MAXROWS=10 # AuthenticatedUser, Supervisor, Administrator OPCUA_USER_LOGIN=\"xxxxxxxxxxxxxx\" OPCUA_USER_PASS=\"xxxxxxxxxxxxxx\" OPCUA_ADMIN_LOGIN=\"xxxxxxxxxxxxxx\" OPCUA_ADMIN_PASS=\"xxxxxxxxxxxxxx\" OPCUA_KEP_LOGIN=\"xxxxxxxxxxxxxx\" OPCUA_KEP_PASS=\"xxxxxxxxxxxxxx\" ### LOG ### #----------# # yes | no; true | false; 1 | 0 LOGMSG_ENABLE=1 # -1(no limits) | 1..n(n limit) LOGMSG_MAXROWS=100 ### SECRET DATA ### #-----------------# GITHUB_ID=\"xxxxxxxxxxxxxxxxxxxxxxxx\" GITHUB_SECRET=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" GOOGLE_ID=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.apps.googleusercontent.com\" GOOGLE_SECRET=\"xxxxxxxxxxxxxxxxxxxxxxxxxxx\" ### DATA-BASE ### #-----------------# # MSSQL MSSQL_ASODU_TEST_ID=\"OGMT-MZTP.FIRST.dbBSA\" MSSQL_ASODU_TEST_USER=\"xxxxxxxxxxxxxxx\" MSSQL_ASODU_TEST_PASS=\"xxxxxxxxxxxxxxx\" MSSQL_BSAHOME_TEST_ID=\"BSA-HOME.SQLEXPRESS.Feathers_test\" MSSQL_BSAHOME_TEST_USER=\"xxxx\" MSSQL_BSAHOME_TEST_PASS=\"xxxxxxxxxxxxx\" # DEFAULT TYPE DB (mongodb | nedb) DEFAULT_TYPE_DB=\"nedb\" # MONGODB MONGODB_DEV_URL=\"mongodb://localhost:27017/feathers_opcua_devserver\" MONGODB_TEST_URL=\"mongodb://localhost:27017/feathers_opcua_testserver\" MONGODB_PROD_URL=\"mongodb://localhost:27017/feathers_opcua_prodserver\" # NEDB NEDB_DEV_PATH=\"nedb://../data/nedb/dev\" NEDB_TEST_PATH=\"nedb://../data/nedb/test\" NEDB_PROD_PATH=\"nedb://../data/nedb/prod\" Note: To have real keys to access the API of various services, you need to register your application in these services. Examples of registering an application and obtaining API keys in different services can be found here . Main sections of the environment variable file .env : SERVER - used in configuration files \\config\\default.json , \\config\\development.json , \\config\\production.json , \\config\\test.json . AUTH - used to authenticate and authorize users to access application resources. OPC-UA - used to set the parameters of the library NodeOPCUA. LOG - used for logging events. SECRET DATA - secret data for services (GOOGLE, GITHUB). DATA-BASE - database information (MSSQL, MONGODB, NEDB).","title":".env"},{"location":"tutorial/part6_configuration/#feathers-specs","text":"This configuration file is located in the folder \\config\\feathers-specs.json . Main sections of feathers-specs : envTestModeName - the value is set to a variable NODE_ENV for the test mode. { \"app\": { \"envTestModeName\": \"test\", ... }, ... } envAllowingSeedData - modes for which it is allowed to change data in the database with \\seeds\\fake-data.json . { \"app\": { ... \"envAllowingSeedData\": [\"test\"], ... }, ... } providers - transport types used to transfer data. { \"app\": { ... \"providers\": [ \"rest\", \"socketio\" ] ... }, ... } services - service properties such as name, path, etc. { \"services\": { \"users\": { \"name\": \"users\", \"nameSingular\": \"user\", \"subFolder\": \"\", \"fileName\": \"users\", \"adapter\": \"nedb\", \"path\": \"/users\", \"isAuthEntity\": true, \"requiresAuth\": true, \"overriddenAuth\": { \"create\": \"noauth\", \"get\": \"auth\", \"find\": \"auth\", \"update\": \"auth\", \"patch\": \"auth\", \"remove\": \"auth\" } }, }, ... } hooks - hook properties such as fileName, ifMulti, etc. { \"hooks\": { \"normalize\": { \"fileName\": \"normalize\", \"camelName\": \"normalize\", \"ifMulti\": \"y\", \"multiServices\": [ \"*app\" ], \"singleService\": \"\" }, ... }, ... }","title":"feathers-specs"},{"location":"tutorial/part6_configuration/#opcua-config","text":"An application is configured for its host on which it is installed using a configuration file src\\api\\opcua\\config\\OPCUA_Config.json . In doing so, you can define separate configurations for this host. Each configuration has its own unique number, for example ua-cherkassy-azot-asutp_dev1 , you can also define the server and client and what tags they will work with. Also in the configuration, you can determine which database to work with and in which mode, which data transfer protection mode to use, and how to authenticate the user. Main sections of the config file OPCUA_Config : id - unique configuration code. name - unique configuration name. endpointUrl - opcua server endpoint url. endpointPort - opcua server endpoint port. srvServiceUrl - features framework host url for opcua-servers service. clientServiceUrl - features framework host url for opcua-clients service. clientScript - the name of the function to initialize the subscription on the client at startup src\\plugins\\opcua\\opcua-bootstrap.js . executeMethodsFrom - the place where the method should be executed, possible values: client|server|asyncServer . hostTypeDB - the type of database the application will work with, possible values: (mongodb|nedb) . opcuaSaveModeToDB - database operation mode, working or not, local or remote database, updating or adding current opcua-values , possible values: localUpdate|localAdd|remoteUpdate|remoteAdd|no . opcuaRemoteDbUrl - the url of the database where we will store the opcua-tags or opcua-values data. opcuaBootstrapParams - parameters at startup of src\\plugins\\opcua\\opcua-bootstrap.js . security - opcua data transfer security settings and user authentication. NodeId - parameters for the client to receive specific values NodeId from the server. e.g. nodeName . include tests - what tests will we run on this host e.g. [\"mssql-tedious.test.js\",\"mssql-datasets.test.js\", \"http-operations.test.js\"] . paths['base-options'] - base tag paths. paths['options'] - path to tags for a specific configuration, tags that override tag values from base paths. [ ... { \"isEnable\": true, \"id\": \"ua-cherkassy-azot-asutp_dev1\", \"name\": \"380-472-00203826-asutp_dev1\", \"description\": \"Opcua server and client for `azot-m5`, `azot-ogmtmztp` tags from `opc.tcp://M5-0095488.OSTCHEM.COM.UA:26570`\", \"endpointUrl\": \"opc.tcp://M5-0095488.OSTCHEM.COM.UA:26570\", \"endpointPort\": 26570, \"srvServiceUrl\": \"http://10.60.0.220:3030\", \"clientServiceUrl\": \"http://10.60.0.220:3030\", \"clientScript\": \"startSubscriptionMonitor\", \"executeMethodsFrom\": \"client\", \"hostTypeDB\": \"mongodb\", \"opcuaSaveModeToDB\": \"localUpdate\", \"opcuaRemoteDbUrl\": \"http://M5-0095488.OSTCHEM.COM.UA:3131\", \"opcuaBootstrapParams\": { \"clearHistoryAtStartup\": false, \"syncHistoryAtStartup\": { \"active\": false, \"methodName\": \"methodAcmDayReportsDataGet\" }, \"syncReportAtStartup\": { \"active\": false, \"methodName\": \"methodAcmYearReportUpdate\" } }, \"security\": { \"mode\": \"SignAndEncrypt\", \"policy\": \"Basic256Sha256\", \"userName\": \"OPCUA_ADMIN_LOGIN\", \"password\": \"OPCUA_ADMIN_PASS\" }, \"NodeId\": { \"namespaceIndex\": 2, \"identifierType\": \"displayName\", \"identifierPrefix\": \"Channel1.Device1\", \"addObjectItem\": true }, \"include\": { \"tests\": [ \"mssql-tedious.test.js\", \"mssql-datasets.test.js\", \"http-operations.test.js\" ] }, \"paths\": { \"base-options\": [ \"/src/api/opcua/tags/ua-cherkassy-azot-m5_prod1\", \"/src/api/opcua/tags/ua-cherkassy-azot-ogmttsd_prod1\" ], \"options\": \"/src/api/opcua/tags/ua-cherkassy-azot-asutp_dev1/AddressSpaceOptions.json\", \"getters\": \"\", \"methods\": \"\", \"subscriptions\": \"\", \"client-scripts\": \"\" } } ... ]","title":"OPCUA Config"},{"location":"tutorial/part6_configuration/#opcua-tags","text":"An application is configured for its host on which it is installed using a configuration file src\\api\\opcua\\config\\OPCUA_Config.json . In doing so, you can define separate configurations for this host. Each configuration has its own unique number, for example ua-cherkassy-azot-asutp_dev1 , you can also define the server and client and what tags they will work with. The configuration specifies the base paths paths['base-options'] to the tag files and the config path paths['options'] to the tag file, which overrides the values of the base tags. Example of base path tags: { \"objects\": [ { \"isEnable\": true, \"browseName\": \"CH_M5\", \"displayName\": \"Workshop M5 Cherkasy `AZOT`\", \"type\": \"object\" }, { \"isEnable\": true, \"browseName\": \"CH_M51\", \"displayName\": \"Workshop M5 dep.1 Cherkassy `AZOT`\", \"type\": \"object\" } ], \"variables\": [ { \"browseName\": \"CH_M51::ValueFromFile\", \"displayName\": \"Values from file for CH_M51\", \"ownerName\": \"CH_M51\", \"type\": \"variable.simple\", \"dataType\": \"String\", \"hist\": 1, \"group\": true, \"subscription\": \"onChangedGroupHandlerForDB\", \"variableGetType\": \"valueFromSource\", \"getter\": \"getterHistValueFromPath\", \"getterParams\": { \"path\": \"//192.168.3.5/www_m5/m5_data2\" } } ], \"groups\": [ { \"browseName\": \"CH_M51::01AMIAK:01T4\", \"aliasName\": \"01NH3_T4\", \"displayName\": \"01NH3_T4\", \"ownerName\": \"CH_M51\", \"ownerGroup\": \"CH_M51::ValueFromFile\", \"type\": \"variable.analog\", \"dataType\": \"Double\", \"hist\": 1, \"variableGetType\": \"valueFromSource\", \"getter\": \"getterPlugForVariable\", \"valueParams\": { \"engineeringUnitsRange\": { \"low\": 0, \"high\": 200 }, \"engineeringUnits\": \"degree_celsius\" }, \"description\": \"Ammonia gas temperature NH3 (T4)\" }, ], \"methods\": [ { \"browseName\": \"CH_M5::YearTemplateCreate\", \"displayName\": \"Create an annual report template\", \"ownerName\": \"CH_M5\", \"type\": \"method\", \"bindMethod\": \"methodAcmYearTemplateCreate\", \"inputArguments\": [ { \"name\": \"methodParameters\", \"description\": { \"text\": \"Method parameters\" }, \"dataType\": \"String\" } ], \"outputArguments\": [ { \"name\": \"methodExecutionResult\", \"description\": { \"text\": \"Method execution result\" }, \"dataType\": \"String\", \"valueRank\": 1 } ], \"userAccessLevel\": { \"inputArguments\": \"CurrentRead\", \"outputArguments\": \"CurrentRead\" } } ] } Example of config path tags: { \"objects\": [ { \"isEnable\": false, \"browseName\": \"CH_M5\", \"displayName\": \"Workshop M5 Cherkasy `AZOT`\", \"type\": \"object\" }, { \"isEnable\": true, \"browseName\": \"CH_M51\", \"displayName\": \"Workshop M5 dep.1 Cherkassy `AZOT`\", \"type\": \"object\" } ], \"variables\": [ { \"browseName\": \"CH_M51::ValueFromFile\", \"displayName\": \"Values from file for CH_M51\", \"ownerName\": \"CH_M51\", \"type\": \"variable.simple\", \"dataType\": \"String\", \"hist\": 1, \"group\": true, \"subscription\": \"onChangedGroupHandlerForDB\", \"variableGetType\": \"valueFromSource\", \"getter\": \"getterHistValueFromHttpPath\", \"getterParams\": { \"path\": \"http://192.168.3.5/www_m5/m5_data2/\", \"interval\": 20000 } } ] } Main sections of the tag file: objects - set of object tags. All tags (\"variables\", \"groups\", \"methods\") belong to some object tag through the ownerName field (e.g. \"ownerName\": \"CH_M51\" ). variables - set of variable tags. If the \"group\" field is set to true, then this tag is the owner of the tag group (e.g. \"group\": true ). groups - set of group tags. These are tags that belong to a tag group. The name of the owner of this group is indicated in the ownerGroup field (e.g. \"ownerGroup\": \"CH_M51::ValueFromFile\" ). methods - set of method tags. browseName - browse name (e.g. \"browseName\": \"CH_M51::ValueFromFile\" ). displayName - display name (e.g. \"displayName\": \"Values from file for CH_M51\" ). ownerName - name of owner (e.g. \"ownerName\": \"CH_M51\" ). type - variable type (e.g. \"type\": \"variable.simple\" ). dataType - data type (e.g. \"dataType\": \"Double\" ). hist - story size (e.g. \"hist\": 1 ). group - group tag (e.g. \"group\": true ). ownerGroup - owner tag name (e.g. \"ownerGroup\": \"CH_M51::ValueFromFile\" ). variableGetType - data source for a variable (e.g. \"variableGetType\": \"valueFromSource\" ). subscription - variable change event handler function name (e.g. \"subscription\": \"onChangedGroupHandlerForDB\" ). getter - the name of the get data function for the variable (e.g. \"getter\": \"getterHistValueFromHttpPath\" ). getterParams - parameters for the get data function for a variable (e.g. \"getterParams\": { \"path\": \"http://192.168.3.5/www_m5/m5_data2/\", \"interval\": 20000 } ). valueParams.engineeringUnitsRange - engineering units range (e.g. valueParams.engineeringUnitsRange: { \"low\": 0, \"high\": 200 } ). valueParams.engineeringUnits - engineering units (e.g. valueParams.engineeringUnits: \"degree_celsius\" ). description - tag description (e.g. \"description\": \"Ammonia gas temperature NH3 (T4)\" ). bindMethod - bind method (e.g. \"bindMethod\": \"methodAcmYearTemplateCreate\" ). inputArguments - input arguments for the method. outputArguments - output arguments for the method.","title":"OPCUA Tags"},{"location":"tutorial/part7_install-as-win-service/","text":"Install a node application as a Windows service There is a NPM package node-windows which can install a node application as a Windows service. This service can be automatically started when the server restarts. The node-windows can do this for us. Run the following commands: npm install -g node-windows Then, in your project root, run: npm link node-windows Once the package is installed it can be used to install the application as a service with the following node script: scripts/cli/install-windows.service.js const { cwd } = require('process'); const appRoot = cwd(); const { join } = require('path'); const filePath = join(appRoot, 'src\\\\index.js'); const Service = require('node-windows').Service; // Create a new service object const svc = new Service({ name: 'Feathers opcua server', description: 'Feathers opcua server application as Windows Service', script: filePath, nodeOptions: [ '--harmony', '--max_old_space_size=4096' ], env: [{ name: 'NODE_ENV', value: 'production' // env NODE_ENV = 'production' }, { name: 'IS_SHOW_LOG', value: false // env IS_SHOW_LOG = true|false; Is show log for production }, { name: 'START_APP', value: 'win_service' // env START_APP = 'win_service'; Start application as windows service }, ] }); // Listen for the \"install\" event, which indicates the // process is available as a service. svc.on('install', function () { svc.start(); console.log('Install complete.'); }); // Error - Fired in some instances when an error occurs. svc.on('error', function (err) { console.log('feathers-opcua-server.Error:', err); }); svc.install(); Just run the script as any other node script: node install-windows-service.js If User Account Control (UAC) is enabled on Windows you will have to give permission a few times to complete the installation. Once this script has finished the service is installed and the application is running. You can find the service in the Services dialog. It will have the name that you have passed to the Service class in the node script. If the service ever needs to be uninstalled, the Service class also has an uninstall method: scripts/cli/uninstall-windows.service.js const { cwd } = require('process'); const appRoot = cwd(); const { join } = require('path'); const filePath = join(appRoot, 'src\\\\index.js'); const Service = require('node-windows').Service; // Create a new service object const svc = new Service({ name: 'Feathers opcua server', description: 'Feathers opcua server application as Windows Service', script: filePath }); // Listen for the \"uninstall\" event so we know when it's done. svc.on('uninstall', function () { console.log('Uninstall complete.'); console.log('The service exists: ', svc.exists); }); // Uninstall the service. svc.uninstall(); This can also be run as any other node script: node uninstall-windows.service.js","title":"Part7. Install as a Windows service"},{"location":"tutorial/part7_install-as-win-service/#install-a-node-application-as-a-windows-service","text":"There is a NPM package node-windows which can install a node application as a Windows service. This service can be automatically started when the server restarts. The node-windows can do this for us. Run the following commands: npm install -g node-windows Then, in your project root, run: npm link node-windows Once the package is installed it can be used to install the application as a service with the following node script: scripts/cli/install-windows.service.js const { cwd } = require('process'); const appRoot = cwd(); const { join } = require('path'); const filePath = join(appRoot, 'src\\\\index.js'); const Service = require('node-windows').Service; // Create a new service object const svc = new Service({ name: 'Feathers opcua server', description: 'Feathers opcua server application as Windows Service', script: filePath, nodeOptions: [ '--harmony', '--max_old_space_size=4096' ], env: [{ name: 'NODE_ENV', value: 'production' // env NODE_ENV = 'production' }, { name: 'IS_SHOW_LOG', value: false // env IS_SHOW_LOG = true|false; Is show log for production }, { name: 'START_APP', value: 'win_service' // env START_APP = 'win_service'; Start application as windows service }, ] }); // Listen for the \"install\" event, which indicates the // process is available as a service. svc.on('install', function () { svc.start(); console.log('Install complete.'); }); // Error - Fired in some instances when an error occurs. svc.on('error', function (err) { console.log('feathers-opcua-server.Error:', err); }); svc.install(); Just run the script as any other node script: node install-windows-service.js If User Account Control (UAC) is enabled on Windows you will have to give permission a few times to complete the installation. Once this script has finished the service is installed and the application is running. You can find the service in the Services dialog. It will have the name that you have passed to the Service class in the node script. If the service ever needs to be uninstalled, the Service class also has an uninstall method: scripts/cli/uninstall-windows.service.js const { cwd } = require('process'); const appRoot = cwd(); const { join } = require('path'); const filePath = join(appRoot, 'src\\\\index.js'); const Service = require('node-windows').Service; // Create a new service object const svc = new Service({ name: 'Feathers opcua server', description: 'Feathers opcua server application as Windows Service', script: filePath }); // Listen for the \"uninstall\" event so we know when it's done. svc.on('uninstall', function () { console.log('Uninstall complete.'); console.log('The service exists: ', svc.exists); }); // Uninstall the service. svc.uninstall(); This can also be run as any other node script: node uninstall-windows.service.js","title":"Install a node application as a Windows service"},{"location":"tutorial/using-docker-compose/","text":"Docker Compose is a tool that was developed to help define and share multi-container applications. With Compose, we can create a YAML file to define the services and with a single command, can spin everything up or tear it all down. The big advantage of using Compose is you can define your application stack in a file, keep it at the root of your project repo (it's now version controlled), and easily enable someone else to contribute to your project. Someone would only need to clone your repo and start the compose app. In fact, you might see quite a few projects on GitHub/GitLab doing exactly this now. So, how do we get started? Installing Docker Compose If you installed Docker Desktop/Toolbox for either Windows or Mac, you already have Docker Compose! Play-with-Docker instances already have Docker Compose installed as well. If you are on a Linux machine, you will need to install Docker Compose using the instructions here . After installation, you should be able to run the following and see version information. docker-compose version Creating our Compose File At the root of the app project, create a file named docker-compose.yml . In the compose file, we'll start off by defining the schema version. In most cases, it's best to use the latest supported version. You can look at the Compose file reference for the current schema versions and the compatibility matrix. yaml version: \"3.8\" Next, we'll define the list of services (or containers) we want to run as part of our application. ```yaml hl_lines=\"3\" version: \"3.8\" services: ``` And now, we'll start migrating a service at a time into the compose file. Defining the App Service To remember, this was the command we were using to define our app container. docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you are using PowerShell then use this command. docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" First, let's define the service entry and the image for the container. We can pick any name for the service. The name will automatically become a network alias, which will be useful when defining our MySQL service. ```yaml hl_lines=\"4 5\" version: \"3.8\" services: app: image: node:12-alpine ``` Typically, you will see the command close to the image definition, although there is no requirement on ordering. So, let's go ahead and move that into our file. ```yaml hl_lines=\"6\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ``` Let's migrate the -p 3000:3000 part of the command by defining the ports for the service. We will use the short syntax here, but there is also a more verbose long syntax available as well. ```yaml hl_lines=\"7 8\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 ``` Next, we'll migrate both the working directory ( -w /app ) and the volume mapping ( -v \"$(pwd):/app\" ) by using the working_dir and volumes definitions. Volumes also has a short and long syntax. One advantage of Docker Compose volume definitions is we can use relative paths from the current directory. ```yaml hl_lines=\"9 10 11\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app ``` Finally, we need to migrate the environment variable definitions using the environment key. ```yaml hl_lines=\"12 13 14 15 16\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos ``` Defining the MySQL Service Now, it's time to define the MySQL service. The command that we used for that container was the following: docker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command. docker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 We will first define the new service and name it mysql so it automatically gets the network alias. We'll go ahead and specify the image to use as well. ```yaml hl_lines=\"6 7\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 ``` Next, we'll define the volume mapping. When we ran the container with docker run , the named volume was created automatically. However, that doesn't happen when running with Compose. We need to define the volume in the top-level volumes: section and then specify the mountpoint in the service config. By simply providing only the volume name, the default options are used. There are many more options available though. ```yaml hl_lines=\"8 9 10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql volumes: todo-mysql-data: ``` Finally, we only need to specify the environment variables. ```yaml hl_lines=\"10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: ``` At this point, our complete docker-compose.yml should look like this: version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: Running our Application Stack Now that we have our docker-compose.yml file, we can start it up! Make sure no other copies of the app/db are running first ( docker ps and docker rm -f <ids> ). Start up the application stack using the docker-compose up command. We'll add the -d flag to run everything in the background. bash docker-compose up -d When we run this, we should see output like this: plaintext Creating network \"app_default\" with the default driver Creating volume \"app_todo-mysql-data\" with default driver Creating app_app_1 ... done Creating app_mysql_1 ... done You'll notice that the volume was created as well as a network! By default, Docker Compose automatically creates a network specifically for the application stack (which is why we didn't define one in the compose file). Let's look at the logs using the docker-compose logs -f command. You'll see the logs from each of the services interleaved into a single stream. This is incredibly useful when you want to watch for timing-related issues. The -f flag \"follows\" the log, so will give you live output as it's generated. If you don't already, you'll see output that looks like this... plaintext mysql_1 | 2019-10-03T03:07:16.083639Z 0 [Note] mysqld: ready for connections. mysql_1 | Version: '5.7.27' socket: '/var/run/mysqld/mysqld.sock' port: 3306 MySQL Community Server (GPL) app_1 | Connected to mysql db at host mysql app_1 | Listening on port 3000 The service name is displayed at the beginning of the line (often colored) to help distinguish messages. If you want to view the logs for a specific service, you can add the service name to the end of the logs command (for example, docker-compose logs -f app ). !!! info \"Pro tip - Waiting for the DB before starting the app\" When the app is starting up, it actually sits and waits for MySQL to be up and ready before trying to connect to it. Docker doesn't have any built-in support to wait for another container to be fully up, running, and ready before starting another container. For Node-based projects, you can use the wait-port dependency. Similar projects exist for other languages/frameworks. At this point, you should be able to open your app and see it running. And hey! We're down to a single command! Seeing our App Stack in Docker Dashboard If we look at the Docker Dashboard, we'll see that there is a group named app . This is the \"project name\" from Docker Compose and used to group the containers together. By default, the project name is simply the name of the directory that the docker-compose.yml was located in. If you twirl down the app, you will see the two containers we defined in the compose file. The names are also a little more descriptive, as they follow the pattern of <project-name>_<service-name>_<replica-number> . So, it's very easy to quickly see what container is our app and which container is the mysql database. Tearing it All Down When you're ready to tear it all down, simply run docker-compose down or hit the trash can on the Docker Dashboard for the entire app. The containers will stop and the network will be removed. !!! warning \"Removing Volumes\" By default, named volumes in your compose file are NOT removed when running docker-compose down . If you want to remove the volumes, you will need to add the --volumes flag. The Docker Dashboard does _not_ remove volumes when you delete the app stack. Once torn down, you can switch to another project, run docker-compose up and be ready to contribute to that project! It really doesn't get much simpler than that! Recap In this section, we learned about Docker Compose and how it helps us dramatically simplify the defining and sharing of multi-service applications. We created a Compose file by translating the commands we were using into the appropriate compose format. At this point, we're starting to wrap up the tutorial. However, there are a few best practices about image building we want to cover, as there is a big issue with the Dockerfile we've been using. So, let's take a look!","title":"Index"},{"location":"tutorial/using-docker-compose/#installing-docker-compose","text":"If you installed Docker Desktop/Toolbox for either Windows or Mac, you already have Docker Compose! Play-with-Docker instances already have Docker Compose installed as well. If you are on a Linux machine, you will need to install Docker Compose using the instructions here . After installation, you should be able to run the following and see version information. docker-compose version","title":"Installing Docker Compose"},{"location":"tutorial/using-docker-compose/#creating-our-compose-file","text":"At the root of the app project, create a file named docker-compose.yml . In the compose file, we'll start off by defining the schema version. In most cases, it's best to use the latest supported version. You can look at the Compose file reference for the current schema versions and the compatibility matrix. yaml version: \"3.8\" Next, we'll define the list of services (or containers) we want to run as part of our application. ```yaml hl_lines=\"3\" version: \"3.8\" services: ``` And now, we'll start migrating a service at a time into the compose file.","title":"Creating our Compose File"},{"location":"tutorial/using-docker-compose/#defining-the-app-service","text":"To remember, this was the command we were using to define our app container. docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you are using PowerShell then use this command. docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" First, let's define the service entry and the image for the container. We can pick any name for the service. The name will automatically become a network alias, which will be useful when defining our MySQL service. ```yaml hl_lines=\"4 5\" version: \"3.8\" services: app: image: node:12-alpine ``` Typically, you will see the command close to the image definition, although there is no requirement on ordering. So, let's go ahead and move that into our file. ```yaml hl_lines=\"6\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ``` Let's migrate the -p 3000:3000 part of the command by defining the ports for the service. We will use the short syntax here, but there is also a more verbose long syntax available as well. ```yaml hl_lines=\"7 8\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 ``` Next, we'll migrate both the working directory ( -w /app ) and the volume mapping ( -v \"$(pwd):/app\" ) by using the working_dir and volumes definitions. Volumes also has a short and long syntax. One advantage of Docker Compose volume definitions is we can use relative paths from the current directory. ```yaml hl_lines=\"9 10 11\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app ``` Finally, we need to migrate the environment variable definitions using the environment key. ```yaml hl_lines=\"12 13 14 15 16\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos ```","title":"Defining the App Service"},{"location":"tutorial/using-docker-compose/#defining-the-mysql-service","text":"Now, it's time to define the MySQL service. The command that we used for that container was the following: docker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command. docker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 We will first define the new service and name it mysql so it automatically gets the network alias. We'll go ahead and specify the image to use as well. ```yaml hl_lines=\"6 7\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 ``` Next, we'll define the volume mapping. When we ran the container with docker run , the named volume was created automatically. However, that doesn't happen when running with Compose. We need to define the volume in the top-level volumes: section and then specify the mountpoint in the service config. By simply providing only the volume name, the default options are used. There are many more options available though. ```yaml hl_lines=\"8 9 10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql volumes: todo-mysql-data: ``` Finally, we only need to specify the environment variables. ```yaml hl_lines=\"10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: ``` At this point, our complete docker-compose.yml should look like this: version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data:","title":"Defining the MySQL Service"},{"location":"tutorial/using-docker-compose/#running-our-application-stack","text":"Now that we have our docker-compose.yml file, we can start it up! Make sure no other copies of the app/db are running first ( docker ps and docker rm -f <ids> ). Start up the application stack using the docker-compose up command. We'll add the -d flag to run everything in the background. bash docker-compose up -d When we run this, we should see output like this: plaintext Creating network \"app_default\" with the default driver Creating volume \"app_todo-mysql-data\" with default driver Creating app_app_1 ... done Creating app_mysql_1 ... done You'll notice that the volume was created as well as a network! By default, Docker Compose automatically creates a network specifically for the application stack (which is why we didn't define one in the compose file). Let's look at the logs using the docker-compose logs -f command. You'll see the logs from each of the services interleaved into a single stream. This is incredibly useful when you want to watch for timing-related issues. The -f flag \"follows\" the log, so will give you live output as it's generated. If you don't already, you'll see output that looks like this... plaintext mysql_1 | 2019-10-03T03:07:16.083639Z 0 [Note] mysqld: ready for connections. mysql_1 | Version: '5.7.27' socket: '/var/run/mysqld/mysqld.sock' port: 3306 MySQL Community Server (GPL) app_1 | Connected to mysql db at host mysql app_1 | Listening on port 3000 The service name is displayed at the beginning of the line (often colored) to help distinguish messages. If you want to view the logs for a specific service, you can add the service name to the end of the logs command (for example, docker-compose logs -f app ). !!! info \"Pro tip - Waiting for the DB before starting the app\" When the app is starting up, it actually sits and waits for MySQL to be up and ready before trying to connect to it. Docker doesn't have any built-in support to wait for another container to be fully up, running, and ready before starting another container. For Node-based projects, you can use the wait-port dependency. Similar projects exist for other languages/frameworks. At this point, you should be able to open your app and see it running. And hey! We're down to a single command!","title":"Running our Application Stack"},{"location":"tutorial/using-docker-compose/#seeing-our-app-stack-in-docker-dashboard","text":"If we look at the Docker Dashboard, we'll see that there is a group named app . This is the \"project name\" from Docker Compose and used to group the containers together. By default, the project name is simply the name of the directory that the docker-compose.yml was located in. If you twirl down the app, you will see the two containers we defined in the compose file. The names are also a little more descriptive, as they follow the pattern of <project-name>_<service-name>_<replica-number> . So, it's very easy to quickly see what container is our app and which container is the mysql database.","title":"Seeing our App Stack in Docker Dashboard"},{"location":"tutorial/using-docker-compose/#tearing-it-all-down","text":"When you're ready to tear it all down, simply run docker-compose down or hit the trash can on the Docker Dashboard for the entire app. The containers will stop and the network will be removed. !!! warning \"Removing Volumes\" By default, named volumes in your compose file are NOT removed when running docker-compose down . If you want to remove the volumes, you will need to add the --volumes flag. The Docker Dashboard does _not_ remove volumes when you delete the app stack. Once torn down, you can switch to another project, run docker-compose up and be ready to contribute to that project! It really doesn't get much simpler than that!","title":"Tearing it All Down"},{"location":"tutorial/using-docker-compose/#recap","text":"In this section, we learned about Docker Compose and how it helps us dramatically simplify the defining and sharing of multi-service applications. We created a Compose file by translating the commands we were using into the appropriate compose format. At this point, we're starting to wrap up the tutorial. However, there are a few best practices about image building we want to cover, as there is a big issue with the Dockerfile we've been using. So, let's take a look!","title":"Recap"}]}