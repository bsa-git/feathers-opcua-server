{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Docker overview Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker\u2019s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production. The Docker platform Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allows you to run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you do not need to rely on what is currently installed on the host. You can easily share containers while you work, and be sure that everyone you share with gets the same container that works in the same way. Docker provides tooling and a platform to manage the lifecycle of your containers: Develop your application and its supporting components using containers. The container becomes the unit for distributing and testing your application. When you\u2019re ready, deploy your application into your production environment, as a container or an orchestrated service. This works the same whether your production environment is a local data center, a cloud provider, or a hybrid of the two. What can I use Docker for? Fast, consistent delivery of your applications Docker streamlines the development lifecycle by allowing developers to work in standardized environments using local containers which provide your applications and services. Containers are great for continuous integration and continuous delivery (CI/CD) workflows. Consider the following example scenario: Your developers write code locally and share their work with their colleagues using Docker containers. They use Docker to push their applications into a test environment and execute automated and manual tests. When developers find bugs, they can fix them in the development environment and redeploy them to the test environment for testing and validation. When testing is complete, getting the fix to the customer is as simple as pushing the updated image to the production environment. Responsive deployment and scaling Docker\u2019s container-based platform allows for highly portable workloads. Docker containers can run on a developer\u2019s local laptop, on physical or virtual machines in a data center, on cloud providers, or in a mixture of environments. Docker\u2019s portability and lightweight nature also make it easy to dynamically manage workloads, scaling up or tearing down applications and services as business needs dictate, in near real time. Running more workloads on the same hardware Docker is lightweight and fast. It provides a viable, cost-effective alternative to hypervisor-based virtual machines, so you can use more of your compute capacity to achieve your business goals. Docker is perfect for high density environments and for small and medium deployments where you need to do more with fewer resources. Docker architecture Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers. The Docker daemon The Docker daemon ( dockerd ) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services. The Docker client The Docker client ( docker ) is the primary way that many Docker users interact with Docker. When you use commands such as docker run , the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon. Docker Desktop Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and share containerized applications and microservices. Docker Desktop includes the Docker daemon ( dockerd ), the Docker client ( docker ), Docker Compose, Docker Content Trust, Kubernetes, and Credential Helper. For more information, see Docker Desktop . Docker registries A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry. When you use the docker pull or docker run commands, the required images are pulled from your configured registry. When you use the docker push command, your image is pushed to your configured registry. Docker objects When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects. Images An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run. You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies. Containers A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state. By default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container\u2019s network, storage, or other underlying subsystems are from other containers or from the host machine. A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear. Example docker run command The following command runs an ubuntu container, attaches interactively to your local command-line session, and runs /bin/bash . docker run -i -t ubuntu /bin/bash When you run this command, the following happens (assuming you are using the default registry configuration): If you do not have the ubuntu image locally, Docker pulls it from your configured registry, as though you had run docker pull ubuntu manually. Docker creates a new container, as though you had run a docker container create command manually . Docker allocates a read-write filesystem to the container, as its final layer. This allows a running container to create or modify files and directories in its local filesystem. Docker creates a network interface to connect the container to the default network, since you did not specify any networking options. This includes assigning an IP address to the container. By default, containers can connect to external networks using the host machine\u2019s network connection. Docker starts the container and executes /bin/bash . Because the container is running interactively and attached to your terminal (due to the -i and -t flags), you can provide input using your keyboard while the output is logged to your terminal. When you type exit to terminate the /bin/bash command, the container stops but is not removed. You can start it again or remove it. The underlying technology Docker is written in the Go programming language and takes advantage of several features of the Linux kernel to deliver its functionality. Docker uses a technology called namespaces to provide the isolated workspace called the container. When you run a container, Docker creates a set of namespaces for that container. These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace.","title":"Docker overview"},{"location":"#docker-overview","text":"Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker\u2019s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.","title":"Docker overview"},{"location":"#the-docker-platform","text":"Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allows you to run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you do not need to rely on what is currently installed on the host. You can easily share containers while you work, and be sure that everyone you share with gets the same container that works in the same way. Docker provides tooling and a platform to manage the lifecycle of your containers: Develop your application and its supporting components using containers. The container becomes the unit for distributing and testing your application. When you\u2019re ready, deploy your application into your production environment, as a container or an orchestrated service. This works the same whether your production environment is a local data center, a cloud provider, or a hybrid of the two.","title":"The Docker platform"},{"location":"#what-can-i-use-docker-for","text":"Fast, consistent delivery of your applications Docker streamlines the development lifecycle by allowing developers to work in standardized environments using local containers which provide your applications and services. Containers are great for continuous integration and continuous delivery (CI/CD) workflows. Consider the following example scenario: Your developers write code locally and share their work with their colleagues using Docker containers. They use Docker to push their applications into a test environment and execute automated and manual tests. When developers find bugs, they can fix them in the development environment and redeploy them to the test environment for testing and validation. When testing is complete, getting the fix to the customer is as simple as pushing the updated image to the production environment.","title":"What can I use Docker for?"},{"location":"#responsive-deployment-and-scaling","text":"Docker\u2019s container-based platform allows for highly portable workloads. Docker containers can run on a developer\u2019s local laptop, on physical or virtual machines in a data center, on cloud providers, or in a mixture of environments. Docker\u2019s portability and lightweight nature also make it easy to dynamically manage workloads, scaling up or tearing down applications and services as business needs dictate, in near real time.","title":"Responsive deployment and scaling"},{"location":"#running-more-workloads-on-the-same-hardware","text":"Docker is lightweight and fast. It provides a viable, cost-effective alternative to hypervisor-based virtual machines, so you can use more of your compute capacity to achieve your business goals. Docker is perfect for high density environments and for small and medium deployments where you need to do more with fewer resources.","title":"Running more workloads on the same hardware"},{"location":"#docker-architecture","text":"Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers.","title":"Docker architecture"},{"location":"#the-docker-daemon","text":"The Docker daemon ( dockerd ) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.","title":"The Docker daemon"},{"location":"#the-docker-client","text":"The Docker client ( docker ) is the primary way that many Docker users interact with Docker. When you use commands such as docker run , the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.","title":"The Docker client"},{"location":"#docker-desktop","text":"Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and share containerized applications and microservices. Docker Desktop includes the Docker daemon ( dockerd ), the Docker client ( docker ), Docker Compose, Docker Content Trust, Kubernetes, and Credential Helper. For more information, see Docker Desktop .","title":"Docker Desktop"},{"location":"#docker-registries","text":"A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry. When you use the docker pull or docker run commands, the required images are pulled from your configured registry. When you use the docker push command, your image is pushed to your configured registry.","title":"Docker registries"},{"location":"#docker-objects","text":"When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects.","title":"Docker objects"},{"location":"#images","text":"An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run. You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.","title":"Images"},{"location":"#containers","text":"A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state. By default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container\u2019s network, storage, or other underlying subsystems are from other containers or from the host machine. A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear. Example docker run command The following command runs an ubuntu container, attaches interactively to your local command-line session, and runs /bin/bash . docker run -i -t ubuntu /bin/bash When you run this command, the following happens (assuming you are using the default registry configuration): If you do not have the ubuntu image locally, Docker pulls it from your configured registry, as though you had run docker pull ubuntu manually. Docker creates a new container, as though you had run a docker container create command manually . Docker allocates a read-write filesystem to the container, as its final layer. This allows a running container to create or modify files and directories in its local filesystem. Docker creates a network interface to connect the container to the default network, since you did not specify any networking options. This includes assigning an IP address to the container. By default, containers can connect to external networks using the host machine\u2019s network connection. Docker starts the container and executes /bin/bash . Because the container is running interactively and attached to your terminal (due to the -i and -t flags), you can provide input using your keyboard while the output is logged to your terminal. When you type exit to terminate the /bin/bash command, the container stops but is not removed. You can start it again or remove it.","title":"Containers"},{"location":"#the-underlying-technology","text":"Docker is written in the Go programming language and takes advantage of several features of the Linux kernel to deliver its functionality. Docker uses a technology called namespaces to provide the isolated workspace called the container. When you run a container, Docker creates a set of namespaces for that container. These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace.","title":"The underlying technology"},{"location":"LICENSE/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"{}\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright {yyyy} {name of copyright owner} Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"LICENSE/#apache-license","text":"Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"{}\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright {yyyy} {name of copyright owner} Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Apache License"},{"location":"tutorial/getting-started/","text":"Orientation and setup Welcome! We are excited that you want to learn Docker. This page contains step-by-step instructions on how to get started with Docker. In this tutorial, you\u2019ll learn how to: Build and run an image as a container Share images using Docker Hub Deploy Docker applications using multiple containers with a database Run applications using Docker Compose Download and install Docker This tutorial assumes you have a current version of Docker installed on your machine. If you do not have Docker installed, choose your preferred operating system below to download Docker: Mac with Intel chip Mac with Apple chip Windows For Docker Desktop installation instructions, see: Install Docker Desktop on Mac Install Docker Desktop on Windows Install Docker Desktop on Linux Start the tutorial If you\u2019ve already run the command to get started with the tutorial, congratulations! If not, open a command prompt or bash window, and run the command: docker run -d -p 80:80 docker/getting-started You'll notice a few flags being used. Here's some more info on them: -d - run the container in detached mode (in the background) -p 80:80 - map port 80 of the host to port 80 in the container docker/getting-started - the image to use You can combine single character flags to shorten the full command. As an example, the command above could be written as: docker run -dp 80:80 docker/getting-started The Docker Dashboard Before going too far, we want to highlight the Docker Dashboard, which gives you a quick view of the containers running on your machine. It gives you quick access to container logs, lets you get a shell inside the container, and lets you easily manage container lifecycle (stop, remove, etc.). To access the dashboard, follow the instructions in the Docker Desktop manual . If you open the dashboard now, you will see this tutorial running! The container name ( jolly_bouman below) is a randomly created name. So, you'll most likely have a different name. What is a container? Now that you've run a container, what is a container? Simply put, a container is simply another process on your machine that has been isolated from all other processes on the host machine. That isolation leverages kernel namespaces and cgroups , features that have been in Linux for a long time. Docker has worked to make these capabilities approachable and easy to use. What is a container image? When running a container, it uses an isolated filesystem. This custom filesystem is provided by a container image . Since the image contains the container's filesystem, it must contain everything needed to run an application - all dependencies, configuration, scripts, binaries, etc. The image also contains other configuration for the container, such as environment variables, a default command to run, and other metadata.","title":"Part1. Getting started"},{"location":"tutorial/getting-started/#orientation-and-setup","text":"Welcome! We are excited that you want to learn Docker. This page contains step-by-step instructions on how to get started with Docker. In this tutorial, you\u2019ll learn how to: Build and run an image as a container Share images using Docker Hub Deploy Docker applications using multiple containers with a database Run applications using Docker Compose","title":"Orientation and setup"},{"location":"tutorial/getting-started/#download-and-install-docker","text":"This tutorial assumes you have a current version of Docker installed on your machine. If you do not have Docker installed, choose your preferred operating system below to download Docker: Mac with Intel chip Mac with Apple chip Windows For Docker Desktop installation instructions, see: Install Docker Desktop on Mac Install Docker Desktop on Windows Install Docker Desktop on Linux","title":"Download and install Docker"},{"location":"tutorial/getting-started/#start-the-tutorial","text":"If you\u2019ve already run the command to get started with the tutorial, congratulations! If not, open a command prompt or bash window, and run the command: docker run -d -p 80:80 docker/getting-started You'll notice a few flags being used. Here's some more info on them: -d - run the container in detached mode (in the background) -p 80:80 - map port 80 of the host to port 80 in the container docker/getting-started - the image to use You can combine single character flags to shorten the full command. As an example, the command above could be written as: docker run -dp 80:80 docker/getting-started","title":"Start the tutorial"},{"location":"tutorial/getting-started/#the-docker-dashboard","text":"Before going too far, we want to highlight the Docker Dashboard, which gives you a quick view of the containers running on your machine. It gives you quick access to container logs, lets you get a shell inside the container, and lets you easily manage container lifecycle (stop, remove, etc.). To access the dashboard, follow the instructions in the Docker Desktop manual . If you open the dashboard now, you will see this tutorial running! The container name ( jolly_bouman below) is a randomly created name. So, you'll most likely have a different name.","title":"The Docker Dashboard"},{"location":"tutorial/getting-started/#what-is-a-container","text":"Now that you've run a container, what is a container? Simply put, a container is simply another process on your machine that has been isolated from all other processes on the host machine. That isolation leverages kernel namespaces and cgroups , features that have been in Linux for a long time. Docker has worked to make these capabilities approachable and easy to use.","title":"What is a container?"},{"location":"tutorial/getting-started/#what-is-a-container-image","text":"When running a container, it uses an isolated filesystem. This custom filesystem is provided by a container image . Since the image contains the container's filesystem, it must contain everything needed to run an application - all dependencies, configuration, scripts, binaries, etc. The image also contains other configuration for the container, such as environment variables, a default command to run, and other metadata.","title":"What is a container image?"},{"location":"tutorial/image-building-best-practices/","text":"Security Scanning When you have built an image, it is good practice to scan it for security vulnerabilities using the docker scan command. Docker has partnered with Snyk to provide the vulnerability scanning service. For example, to scan the getting-started image you created earlier in the tutorial, you can just type docker scan getting-started The scan uses a constantly updated database of vulnerabilities, so the output you see will vary as new vulnerabilities are discovered, but it might look something like this: \u2717 Low severity vulnerability found in freetype/freetype Description: CVE-2020-15999 Info: https://snyk.io/vuln/SNYK-ALPINE310-FREETYPE-1019641 Introduced through: freetype/freetype@2.10.0-r0, gd/libgd@2.2.5-r2 From: freetype/freetype@2.10.0-r0 From: gd/libgd@2.2.5-r2 > freetype/freetype@2.10.0-r0 Fixed in: 2.10.0-r1 \u2717 Medium severity vulnerability found in libxml2/libxml2 Description: Out-of-bounds Read Info: https://snyk.io/vuln/SNYK-ALPINE310-LIBXML2-674791 Introduced through: libxml2/libxml2@2.9.9-r3, libxslt/libxslt@1.1.33-r3, nginx-module-xslt/nginx-module-xslt@1.17.9-r1 From: libxml2/libxml2@2.9.9-r3 From: libxslt/libxslt@1.1.33-r3 > libxml2/libxml2@2.9.9-r3 From: nginx-module-xslt/nginx-module-xslt@1.17.9-r1 > libxml2/libxml2@2.9.9-r3 Fixed in: 2.9.9-r4 The output lists the type of vulnerability, a URL to learn more, and importantly which version of the relevant library fixes the vulnerability. There are several other options, which you can read about in the docker scan documentation . As well as scanning your newly built image on the command line, you can also configure Docker Hub to scan all newly pushed images automatically, and you can then see the results in both Docker Hub and Docker Desktop. {: style=width:75% } {: .text-center } Image Layering Did you know that you can look at what makes up an image? Using the docker image history command, you can see the command that was used to create each layer within an image. Use the docker image history command to see the layers in the getting-started image you created earlier in the tutorial. bash docker image history getting-started You should get output that looks something like this (dates/IDs may be different). plaintext IMAGE CREATED CREATED BY SIZE COMMENT a78a40cbf866 18 seconds ago /bin/sh -c #(nop) CMD [\"node\" \"src/index.j\u2026 0B f1d1808565d6 19 seconds ago /bin/sh -c yarn install --production 85.4MB a2c054d14948 36 seconds ago /bin/sh -c #(nop) COPY dir:5dc710ad87c789593\u2026 198kB 9577ae713121 37 seconds ago /bin/sh -c #(nop) WORKDIR /app 0B b95baba1cfdb 13 days ago /bin/sh -c #(nop) CMD [\"node\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ENTRYPOINT [\"docker-entry\u2026 0B <missing> 13 days ago /bin/sh -c #(nop) COPY file:238737301d473041\u2026 116B <missing> 13 days ago /bin/sh -c apk add --no-cache --virtual .bui\u2026 5.35MB <missing> 13 days ago /bin/sh -c #(nop) ENV YARN_VERSION=1.21.1 0B <missing> 13 days ago /bin/sh -c addgroup -g 1000 node && addu\u2026 74.3MB <missing> 13 days ago /bin/sh -c #(nop) ENV NODE_VERSION=12.14.1 0B <missing> 13 days ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ADD file:e69d441d729412d24\u2026 5.59MB Each of the lines represents a layer in the image. The display here shows the base at the bottom with the newest layer at the top. Using this, you can also quickly see the size of each layer, helping diagnose large images. You'll notice that several of the lines are truncated. If you add the --no-trunc flag, you'll get the full output (yes... funny how you use a truncated flag to get untruncated output, huh?) bash docker image history --no-trunc getting-started Layer Caching Now that you've seen the layering in action, there's an important lesson to learn to help decrease build times for your container images. Once a layer changes, all downstream layers have to be recreated as well Let's look at the Dockerfile we were using one more time... FROM node:12-alpine WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] Going back to the image history output, we see that each command in the Dockerfile becomes a new layer in the image. You might remember that when we made a change to the image, the yarn dependencies had to be reinstalled. Is there a way to fix this? It doesn't make much sense to ship around the same dependencies every time we build, right? To fix this, we need to restructure our Dockerfile to help support the caching of the dependencies. For Node-based applications, those dependencies are defined in the package.json file. So, what if we copied only that file in first, install the dependencies, and then copy in everything else? Then, we only recreate the yarn dependencies if there was a change to the package.json . Make sense? Update the Dockerfile to copy in the package.json first, install dependencies, and then copy everything else in. dockerfile hl_lines=\"3 4 5\" FROM node:12-alpine WORKDIR /app COPY package.json yarn.lock ./ RUN yarn install --production COPY . . CMD [\"node\", \"src/index.js\"] Create a file named .dockerignore in the same folder as the Dockerfile with the following contents. ignore node_modules .dockerignore files are an easy way to selectively copy only image relevant files. You can read more about this here . In this case, the node_modules folder should be omitted in the second COPY step because otherwise, it would possibly overwrite files which were created by the command in the RUN step. For further details on why this is recommended for Node.js applications and other best practices, have a look at their guide on Dockerizing a Node.js web app . Build a new image using docker build . bash docker build -t getting-started . You should see output like this... plaintext Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Running in d53a06c9e4c2 yarn install v1.17.3 [1/4] Resolving packages... [2/4] Fetching packages... info fsevents@1.2.9: The platform \"linux\" is incompatible with this module. info \"fsevents@1.2.9\" is an optional dependency and failed compatibility check. Excluding it from installation. [3/4] Linking dependencies... [4/4] Building fresh packages... Done in 10.89s. Removing intermediate container d53a06c9e4c2 ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> a239a11f68d8 Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 49999f68df8f Removing intermediate container 49999f68df8f ---> e709c03bc597 Successfully built e709c03bc597 Successfully tagged getting-started:latest You'll see that all layers were rebuilt. Perfectly fine since we changed the Dockerfile quite a bit. Now, make a change to the src/static/index.html file (like change the <title> to say \"The Awesome Todo App\"). Build the Docker image now using docker build -t getting-started . again. This time, your output should look a little different. plaintext hl_lines=\"5 8 11\" Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> Using cache ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Using cache ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> cccde25a3d9a Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 2be75662c150 Removing intermediate container 2be75662c150 ---> 458e5c6f080c Successfully built 458e5c6f080c Successfully tagged getting-started:latest First off, you should notice that the build was MUCH faster! And, you'll see that steps 1-4 all have Using cache . So, hooray! We're using the build cache. Pushing and pulling this image and updates to it will be much faster as well. Hooray! Multi-Stage Builds While we're not going to dive into it too much in this tutorial, multi-stage builds are an incredibly powerful tool to help use multiple stages to create an image. There are several advantages for them: Separate build-time dependencies from runtime dependencies Reduce overall image size by shipping only what your app needs to run Maven/Tomcat Example When building Java-based applications, a JDK is needed to compile the source code to Java bytecode. However, that JDK isn't needed in production. Also, you might be using tools like Maven or Gradle to help build the app. Those also aren't needed in our final image. Multi-stage builds help. FROM maven AS build WORKDIR /app COPY . . RUN mvn package FROM tomcat COPY --from=build /app/target/file.war /usr/local/tomcat/webapps In this example, we use one stage (called build ) to perform the actual Java build using Maven. In the second stage (starting at FROM tomcat ), we copy in files from the build stage. The final image is only the last stage being created (which can be overridden using the --target flag). React Example When building React applications, we need a Node environment to compile the JS code (typically JSX), SASS stylesheets, and more into static HTML, JS, and CSS. If we aren't doing server-side rendering, we don't even need a Node environment for our production build. Why not ship the static resources in a static nginx container? FROM node:12 AS build WORKDIR /app COPY package* yarn.lock ./ RUN yarn install COPY public ./public COPY src ./src RUN yarn run build FROM nginx:alpine COPY --from=build /app/build /usr/share/nginx/html Here, we are using a node:12 image to perform the build (maximizing layer caching) and then copying the output into an nginx container. Cool, huh? Recap By understanding a little bit about how images are structured, we can build images faster and ship fewer changes. Scanning images gives us confidence that the containers we are running and distributing are secure. Multi-stage builds also help us reduce overall image size and increase final container security by separating build-time dependencies from runtime dependencies.","title":"Part9. Image-building best practices"},{"location":"tutorial/image-building-best-practices/#security-scanning","text":"When you have built an image, it is good practice to scan it for security vulnerabilities using the docker scan command. Docker has partnered with Snyk to provide the vulnerability scanning service. For example, to scan the getting-started image you created earlier in the tutorial, you can just type docker scan getting-started The scan uses a constantly updated database of vulnerabilities, so the output you see will vary as new vulnerabilities are discovered, but it might look something like this: \u2717 Low severity vulnerability found in freetype/freetype Description: CVE-2020-15999 Info: https://snyk.io/vuln/SNYK-ALPINE310-FREETYPE-1019641 Introduced through: freetype/freetype@2.10.0-r0, gd/libgd@2.2.5-r2 From: freetype/freetype@2.10.0-r0 From: gd/libgd@2.2.5-r2 > freetype/freetype@2.10.0-r0 Fixed in: 2.10.0-r1 \u2717 Medium severity vulnerability found in libxml2/libxml2 Description: Out-of-bounds Read Info: https://snyk.io/vuln/SNYK-ALPINE310-LIBXML2-674791 Introduced through: libxml2/libxml2@2.9.9-r3, libxslt/libxslt@1.1.33-r3, nginx-module-xslt/nginx-module-xslt@1.17.9-r1 From: libxml2/libxml2@2.9.9-r3 From: libxslt/libxslt@1.1.33-r3 > libxml2/libxml2@2.9.9-r3 From: nginx-module-xslt/nginx-module-xslt@1.17.9-r1 > libxml2/libxml2@2.9.9-r3 Fixed in: 2.9.9-r4 The output lists the type of vulnerability, a URL to learn more, and importantly which version of the relevant library fixes the vulnerability. There are several other options, which you can read about in the docker scan documentation . As well as scanning your newly built image on the command line, you can also configure Docker Hub to scan all newly pushed images automatically, and you can then see the results in both Docker Hub and Docker Desktop. {: style=width:75% } {: .text-center }","title":"Security Scanning"},{"location":"tutorial/image-building-best-practices/#image-layering","text":"Did you know that you can look at what makes up an image? Using the docker image history command, you can see the command that was used to create each layer within an image. Use the docker image history command to see the layers in the getting-started image you created earlier in the tutorial. bash docker image history getting-started You should get output that looks something like this (dates/IDs may be different). plaintext IMAGE CREATED CREATED BY SIZE COMMENT a78a40cbf866 18 seconds ago /bin/sh -c #(nop) CMD [\"node\" \"src/index.j\u2026 0B f1d1808565d6 19 seconds ago /bin/sh -c yarn install --production 85.4MB a2c054d14948 36 seconds ago /bin/sh -c #(nop) COPY dir:5dc710ad87c789593\u2026 198kB 9577ae713121 37 seconds ago /bin/sh -c #(nop) WORKDIR /app 0B b95baba1cfdb 13 days ago /bin/sh -c #(nop) CMD [\"node\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ENTRYPOINT [\"docker-entry\u2026 0B <missing> 13 days ago /bin/sh -c #(nop) COPY file:238737301d473041\u2026 116B <missing> 13 days ago /bin/sh -c apk add --no-cache --virtual .bui\u2026 5.35MB <missing> 13 days ago /bin/sh -c #(nop) ENV YARN_VERSION=1.21.1 0B <missing> 13 days ago /bin/sh -c addgroup -g 1000 node && addu\u2026 74.3MB <missing> 13 days ago /bin/sh -c #(nop) ENV NODE_VERSION=12.14.1 0B <missing> 13 days ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ADD file:e69d441d729412d24\u2026 5.59MB Each of the lines represents a layer in the image. The display here shows the base at the bottom with the newest layer at the top. Using this, you can also quickly see the size of each layer, helping diagnose large images. You'll notice that several of the lines are truncated. If you add the --no-trunc flag, you'll get the full output (yes... funny how you use a truncated flag to get untruncated output, huh?) bash docker image history --no-trunc getting-started","title":"Image Layering"},{"location":"tutorial/image-building-best-practices/#layer-caching","text":"Now that you've seen the layering in action, there's an important lesson to learn to help decrease build times for your container images. Once a layer changes, all downstream layers have to be recreated as well Let's look at the Dockerfile we were using one more time... FROM node:12-alpine WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] Going back to the image history output, we see that each command in the Dockerfile becomes a new layer in the image. You might remember that when we made a change to the image, the yarn dependencies had to be reinstalled. Is there a way to fix this? It doesn't make much sense to ship around the same dependencies every time we build, right? To fix this, we need to restructure our Dockerfile to help support the caching of the dependencies. For Node-based applications, those dependencies are defined in the package.json file. So, what if we copied only that file in first, install the dependencies, and then copy in everything else? Then, we only recreate the yarn dependencies if there was a change to the package.json . Make sense? Update the Dockerfile to copy in the package.json first, install dependencies, and then copy everything else in. dockerfile hl_lines=\"3 4 5\" FROM node:12-alpine WORKDIR /app COPY package.json yarn.lock ./ RUN yarn install --production COPY . . CMD [\"node\", \"src/index.js\"] Create a file named .dockerignore in the same folder as the Dockerfile with the following contents. ignore node_modules .dockerignore files are an easy way to selectively copy only image relevant files. You can read more about this here . In this case, the node_modules folder should be omitted in the second COPY step because otherwise, it would possibly overwrite files which were created by the command in the RUN step. For further details on why this is recommended for Node.js applications and other best practices, have a look at their guide on Dockerizing a Node.js web app . Build a new image using docker build . bash docker build -t getting-started . You should see output like this... plaintext Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Running in d53a06c9e4c2 yarn install v1.17.3 [1/4] Resolving packages... [2/4] Fetching packages... info fsevents@1.2.9: The platform \"linux\" is incompatible with this module. info \"fsevents@1.2.9\" is an optional dependency and failed compatibility check. Excluding it from installation. [3/4] Linking dependencies... [4/4] Building fresh packages... Done in 10.89s. Removing intermediate container d53a06c9e4c2 ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> a239a11f68d8 Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 49999f68df8f Removing intermediate container 49999f68df8f ---> e709c03bc597 Successfully built e709c03bc597 Successfully tagged getting-started:latest You'll see that all layers were rebuilt. Perfectly fine since we changed the Dockerfile quite a bit. Now, make a change to the src/static/index.html file (like change the <title> to say \"The Awesome Todo App\"). Build the Docker image now using docker build -t getting-started . again. This time, your output should look a little different. plaintext hl_lines=\"5 8 11\" Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> Using cache ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Using cache ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> cccde25a3d9a Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 2be75662c150 Removing intermediate container 2be75662c150 ---> 458e5c6f080c Successfully built 458e5c6f080c Successfully tagged getting-started:latest First off, you should notice that the build was MUCH faster! And, you'll see that steps 1-4 all have Using cache . So, hooray! We're using the build cache. Pushing and pulling this image and updates to it will be much faster as well. Hooray!","title":"Layer Caching"},{"location":"tutorial/image-building-best-practices/#multi-stage-builds","text":"While we're not going to dive into it too much in this tutorial, multi-stage builds are an incredibly powerful tool to help use multiple stages to create an image. There are several advantages for them: Separate build-time dependencies from runtime dependencies Reduce overall image size by shipping only what your app needs to run","title":"Multi-Stage Builds"},{"location":"tutorial/image-building-best-practices/#maventomcat-example","text":"When building Java-based applications, a JDK is needed to compile the source code to Java bytecode. However, that JDK isn't needed in production. Also, you might be using tools like Maven or Gradle to help build the app. Those also aren't needed in our final image. Multi-stage builds help. FROM maven AS build WORKDIR /app COPY . . RUN mvn package FROM tomcat COPY --from=build /app/target/file.war /usr/local/tomcat/webapps In this example, we use one stage (called build ) to perform the actual Java build using Maven. In the second stage (starting at FROM tomcat ), we copy in files from the build stage. The final image is only the last stage being created (which can be overridden using the --target flag).","title":"Maven/Tomcat Example"},{"location":"tutorial/image-building-best-practices/#react-example","text":"When building React applications, we need a Node environment to compile the JS code (typically JSX), SASS stylesheets, and more into static HTML, JS, and CSS. If we aren't doing server-side rendering, we don't even need a Node environment for our production build. Why not ship the static resources in a static nginx container? FROM node:12 AS build WORKDIR /app COPY package* yarn.lock ./ RUN yarn install COPY public ./public COPY src ./src RUN yarn run build FROM nginx:alpine COPY --from=build /app/build /usr/share/nginx/html Here, we are using a node:12 image to perform the build (maximizing layer caching) and then copying the output into an nginx container. Cool, huh?","title":"React Example"},{"location":"tutorial/image-building-best-practices/#recap","text":"By understanding a little bit about how images are structured, we can build images faster and ship fewer changes. Scanning images gives us confidence that the containers we are running and distributing are secure. Multi-stage builds also help us reduce overall image size and increase final container security by separating build-time dependencies from runtime dependencies.","title":"Recap"},{"location":"tutorial/multi-container-apps/","text":"Up to this point, we have been working with single container apps. But, we now want to add MySQL to the application stack. The following question often arises - \"Where will MySQL run? Install it in the same container or run it separately?\" In general, each container should do one thing and do it well. A few reasons: There's a good chance you'd have to scale APIs and front-ends differently than databases. Separate containers let you version and update versions in isolation. While you may use a container for the database locally, you may want to use a managed service for the database in production. You don't want to ship your database engine with your app then. Running multiple processes will require a process manager (the container only starts one process), which adds complexity to container startup/shutdown. And there are more reasons. So, we will update our application to work like this: {: .text-center } Container Networking Remember that containers, by default, run in isolation and don't know anything about other processes or containers on the same machine. So, how do we allow one container to talk to another? The answer is networking . Now, you don't have to be a network engineer (hooray!). Simply remember this rule... If two containers are on the same network, they can talk to each other. If they aren't, they can't. Starting MySQL There are two ways to put a container on a network: 1) Assign it at start or 2) connect an existing container. For now, we will create the network first and attach the MySQL container at startup. Create the network. bash docker network create todo-app Start a MySQL container and attach it to the network. We're also going to define a few environment variables that the database will use to initialize the database (see the \"Environment Variables\" section in the MySQL Docker Hub listing ). bash docker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command. powershell docker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 You'll also see we specified the --network-alias flag. We'll come back to that in just a moment. !!! info \"Pro-tip\" You'll notice we're using a volume named todo-mysql-data here and mounting it at /var/lib/mysql , which is where MySQL stores its data. However, we never ran a docker volume create command. Docker recognizes we want to use a named volume and creates one automatically for us. !!! info \"Troubleshooting\" If you see a docker: no matching manifest error, it's because you're trying to run the container in a different architecture than amd64, which is the only supported architecture for the mysql image at the moment. To solve this add the flag --platform linux/amd64 in the previous command. So your new command should look like this: bash docker run -d \\ --network todo-app --network-alias mysql --platform linux/amd64 \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 To confirm we have the database up and running, connect to the database and verify it connects. bash docker exec -it <mysql-container-id> mysql -p When the password prompt comes up, type in secret . In the MySQL shell, list the databases and verify you see the todos database. cli mysql> SHOW DATABASES; You should see output that looks like this: plaintext +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | todos | +--------------------+ 5 rows in set (0.00 sec) Hooray! We have our todos database and it's ready for us to use! To exit the sql terminal type exit in the terminal. Connecting to MySQL Now that we know MySQL is up and running, let's use it! But, the question is... how? If we run another container on the same network, how do we find the container (remember each container has its own IP address)? To figure it out, we're going to make use of the nicolaka/netshoot container, which ships with a lot of tools that are useful for troubleshooting or debugging networking issues. Start a new container using the nicolaka/netshoot image. Make sure to connect it to the same network. bash docker run -it --network todo-app nicolaka/netshoot Inside the container, we're going to use the dig command, which is a useful DNS tool. We're going to look up the IP address for the hostname mysql . bash dig mysql And you'll get an output like this... ```text ; <<>> DiG 9.14.1 <<>> mysql ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 32162 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;mysql. IN A ;; ANSWER SECTION: mysql. 600 IN A 172.23.0.2 ;; Query time: 0 msec ;; SERVER: 127.0.0.11#53(127.0.0.11) ;; WHEN: Tue Oct 01 23:47:24 UTC 2019 ;; MSG SIZE rcvd: 44 ``` In the \"ANSWER SECTION\", you will see an A record for mysql that resolves to 172.23.0.2 (your IP address will most likely have a different value). While mysql isn't normally a valid hostname, Docker was able to resolve it to the IP address of the container that had that network alias (remember the --network-alias flag we used earlier?). What this means is... our app only simply needs to connect to a host named mysql and it'll talk to the database! It doesn't get much simpler than that! Running our App with MySQL The todo app supports the setting of a few environment variables to specify MySQL connection settings. They are: MYSQL_HOST - the hostname for the running MySQL server MYSQL_USER - the username to use for the connection MYSQL_PASSWORD - the password to use for the connection MYSQL_DB - the database to use once connected !!! warning Setting Connection Settings via Env Vars While using env vars to set connection settings is generally ok for development, it is HIGHLY DISCOURAGED when running applications in production. Diogo Monica, the former lead of security at Docker, wrote a fantastic blog post explaining why. A more secure mechanism is to use the secret support provided by your container orchestration framework. In most cases, these secrets are mounted as files in the running container. You'll see many apps (including the MySQL image and the todo app) also support env vars with a `_FILE` suffix to point to a file containing the variable. As an example, setting the `MYSQL_PASSWORD_FILE` var will cause the app to use the contents of the referenced file as the connection password. Docker doesn't do anything to support these env vars. Your app will need to know to look for the variable and get the file contents. With all of that explained, let's start our dev-ready container! We'll specify each of the environment variables above, as well as connect the container to our app network. bash hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you updated your docker file in the Bind Mount section of the tutorial use the updated command: bash hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"apk --no-cache --virtual build-dependencies add python2 make g++ && yarn install && yarn run dev\" If you are using PowerShell then use this command. powershell hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" If we look at the logs for the container ( docker logs <container-id> ), we should see a message indicating it's using the mysql database. ```plaintext hl_lines=\"7\" Previous log messages omitted $ nodemon src/index.js [nodemon] 1.19.2 [nodemon] to restart at any time, enter rs [nodemon] watching dir(s): . [nodemon] starting node src/index.js Connected to mysql db at host mysql Listening on port 3000 ``` Open the app in your browser and add a few items to your todo list. Connect to the mysql database and prove that the items are being written to the database. Remember, the password is secret . bash docker exec -it <mysql-container-id> mysql -p todos And in the mysql shell, run the following: plaintext mysql> select * from todo_items; +--------------------------------------+--------------------+-----------+ | id | name | completed | +--------------------------------------+--------------------+-----------+ | c906ff08-60e6-44e6-8f49-ed56a0853e85 | Do amazing things! | 0 | | 2912a79e-8486-4bc3-a4c5-460793a575ab | Be awesome! | 0 | +--------------------------------------+--------------------+-----------+ Obviously, your table will look different because it has your items. But, you should see them stored there! If you take a quick look at the Docker Dashboard, you'll see that we have two app containers running. But, there's no real indication that they are grouped together in a single app. We'll see how to make that better shortly! Recap At this point, we have an application that now stores its data in an external database running in a separate container. We learned a little bit about container networking and saw how service discovery can be performed using DNS. But, there's a good chance you are starting to feel a little overwhelmed with everything you need to do to start up this application. We have to create a network, start containers, specify all of the environment variables, expose ports, and more! That's a lot to remember and it's certainly making things harder to pass along to someone else. In the next section, we'll talk about Docker Compose. With Docker Compose, we can share our application stacks in a much easier way and let others spin them up with a single (and simple) command!","title":"Part7. Multi-container apps"},{"location":"tutorial/multi-container-apps/#container-networking","text":"Remember that containers, by default, run in isolation and don't know anything about other processes or containers on the same machine. So, how do we allow one container to talk to another? The answer is networking . Now, you don't have to be a network engineer (hooray!). Simply remember this rule... If two containers are on the same network, they can talk to each other. If they aren't, they can't.","title":"Container Networking"},{"location":"tutorial/multi-container-apps/#starting-mysql","text":"There are two ways to put a container on a network: 1) Assign it at start or 2) connect an existing container. For now, we will create the network first and attach the MySQL container at startup. Create the network. bash docker network create todo-app Start a MySQL container and attach it to the network. We're also going to define a few environment variables that the database will use to initialize the database (see the \"Environment Variables\" section in the MySQL Docker Hub listing ). bash docker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command. powershell docker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 You'll also see we specified the --network-alias flag. We'll come back to that in just a moment. !!! info \"Pro-tip\" You'll notice we're using a volume named todo-mysql-data here and mounting it at /var/lib/mysql , which is where MySQL stores its data. However, we never ran a docker volume create command. Docker recognizes we want to use a named volume and creates one automatically for us. !!! info \"Troubleshooting\" If you see a docker: no matching manifest error, it's because you're trying to run the container in a different architecture than amd64, which is the only supported architecture for the mysql image at the moment. To solve this add the flag --platform linux/amd64 in the previous command. So your new command should look like this: bash docker run -d \\ --network todo-app --network-alias mysql --platform linux/amd64 \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 To confirm we have the database up and running, connect to the database and verify it connects. bash docker exec -it <mysql-container-id> mysql -p When the password prompt comes up, type in secret . In the MySQL shell, list the databases and verify you see the todos database. cli mysql> SHOW DATABASES; You should see output that looks like this: plaintext +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | todos | +--------------------+ 5 rows in set (0.00 sec) Hooray! We have our todos database and it's ready for us to use! To exit the sql terminal type exit in the terminal.","title":"Starting MySQL"},{"location":"tutorial/multi-container-apps/#connecting-to-mysql","text":"Now that we know MySQL is up and running, let's use it! But, the question is... how? If we run another container on the same network, how do we find the container (remember each container has its own IP address)? To figure it out, we're going to make use of the nicolaka/netshoot container, which ships with a lot of tools that are useful for troubleshooting or debugging networking issues. Start a new container using the nicolaka/netshoot image. Make sure to connect it to the same network. bash docker run -it --network todo-app nicolaka/netshoot Inside the container, we're going to use the dig command, which is a useful DNS tool. We're going to look up the IP address for the hostname mysql . bash dig mysql And you'll get an output like this... ```text ; <<>> DiG 9.14.1 <<>> mysql ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 32162 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;mysql. IN A ;; ANSWER SECTION: mysql. 600 IN A 172.23.0.2 ;; Query time: 0 msec ;; SERVER: 127.0.0.11#53(127.0.0.11) ;; WHEN: Tue Oct 01 23:47:24 UTC 2019 ;; MSG SIZE rcvd: 44 ``` In the \"ANSWER SECTION\", you will see an A record for mysql that resolves to 172.23.0.2 (your IP address will most likely have a different value). While mysql isn't normally a valid hostname, Docker was able to resolve it to the IP address of the container that had that network alias (remember the --network-alias flag we used earlier?). What this means is... our app only simply needs to connect to a host named mysql and it'll talk to the database! It doesn't get much simpler than that!","title":"Connecting to MySQL"},{"location":"tutorial/multi-container-apps/#running-our-app-with-mysql","text":"The todo app supports the setting of a few environment variables to specify MySQL connection settings. They are: MYSQL_HOST - the hostname for the running MySQL server MYSQL_USER - the username to use for the connection MYSQL_PASSWORD - the password to use for the connection MYSQL_DB - the database to use once connected !!! warning Setting Connection Settings via Env Vars While using env vars to set connection settings is generally ok for development, it is HIGHLY DISCOURAGED when running applications in production. Diogo Monica, the former lead of security at Docker, wrote a fantastic blog post explaining why. A more secure mechanism is to use the secret support provided by your container orchestration framework. In most cases, these secrets are mounted as files in the running container. You'll see many apps (including the MySQL image and the todo app) also support env vars with a `_FILE` suffix to point to a file containing the variable. As an example, setting the `MYSQL_PASSWORD_FILE` var will cause the app to use the contents of the referenced file as the connection password. Docker doesn't do anything to support these env vars. Your app will need to know to look for the variable and get the file contents. With all of that explained, let's start our dev-ready container! We'll specify each of the environment variables above, as well as connect the container to our app network. bash hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you updated your docker file in the Bind Mount section of the tutorial use the updated command: bash hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"apk --no-cache --virtual build-dependencies add python2 make g++ && yarn install && yarn run dev\" If you are using PowerShell then use this command. powershell hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" If we look at the logs for the container ( docker logs <container-id> ), we should see a message indicating it's using the mysql database. ```plaintext hl_lines=\"7\"","title":"Running our App with MySQL"},{"location":"tutorial/multi-container-apps/#previous-log-messages-omitted","text":"$ nodemon src/index.js [nodemon] 1.19.2 [nodemon] to restart at any time, enter rs [nodemon] watching dir(s): . [nodemon] starting node src/index.js Connected to mysql db at host mysql Listening on port 3000 ``` Open the app in your browser and add a few items to your todo list. Connect to the mysql database and prove that the items are being written to the database. Remember, the password is secret . bash docker exec -it <mysql-container-id> mysql -p todos And in the mysql shell, run the following: plaintext mysql> select * from todo_items; +--------------------------------------+--------------------+-----------+ | id | name | completed | +--------------------------------------+--------------------+-----------+ | c906ff08-60e6-44e6-8f49-ed56a0853e85 | Do amazing things! | 0 | | 2912a79e-8486-4bc3-a4c5-460793a575ab | Be awesome! | 0 | +--------------------------------------+--------------------+-----------+ Obviously, your table will look different because it has your items. But, you should see them stored there! If you take a quick look at the Docker Dashboard, you'll see that we have two app containers running. But, there's no real indication that they are grouped together in a single app. We'll see how to make that better shortly!","title":"Previous log messages omitted"},{"location":"tutorial/multi-container-apps/#recap","text":"At this point, we have an application that now stores its data in an external database running in a separate container. We learned a little bit about container networking and saw how service discovery can be performed using DNS. But, there's a good chance you are starting to feel a little overwhelmed with everything you need to do to start up this application. We have to create a network, start containers, specify all of the environment variables, expose ports, and more! That's a lot to remember and it's certainly making things harder to pass along to someone else. In the next section, we'll talk about Docker Compose. With Docker Compose, we can share our application stacks in a much easier way and let others spin them up with a single (and simple) command!","title":"Recap"},{"location":"tutorial/our-application/","text":"Sample application For the rest of this tutorial, we will be working with a simple todo list manager that is running in Node.js. If you're not familiar with Node.js, don't worry! No real JavaScript experience is needed! At this point, your development team is quite small and you're simply building an app to prove out your MVP (minimum viable product). You want to show how it works and what it's capable of doing without needing to think about how it will work for a large team, multiple developers, etc. Getting our App Before we can run the application, we need to get the application source code onto our machine. For real projects, you will typically clone the repo. But, for this tutorial, we have created a ZIP file containing the application. Download the App contents from the getting-started repository . You can either pull the entire project or download it as a zip and extract the app folder out to get started with. Once extracted, use your favorite code editor to open the project. If you're in need of an editor, you can use Visual Studio Code . You should see the package.json and two subdirectories ( src and spec ). Building the App's Container Image In order to build the application, we need to use a Dockerfile . A Dockerfile is simply a text-based script of instructions that is used to create a container image. If you've created Dockerfiles before, you might see a few flaws in the Dockerfile below. But, don't worry! We'll go over them. 1.Create a file named Dockerfile in the same folder as the file package.json with the following contents. # syntax=docker/dockerfile:1 FROM node:12-alpine RUN apk add --no-cache python2 g++ make WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] EXPOSE 3000 Please check that the file Dockerfile has no file extension like .txt . Some editors may append this file extension automatically and this would result in an error in the next step. 2.If you haven't already done so, open a terminal and go to the app directory with the Dockerfile . Now build the container image using the docker build command. docker build -t node-todo . This command used the Dockerfile to build a new container image. You might have noticed that a lot of \"layers\" were downloaded. This is because we instructed the builder that we wanted to start from the node:12-alpine image. But, since we didn't have that on our machine, that image needed to be downloaded. After the image was downloaded, we copied in our application and used yarn to install our application's dependencies. The CMD directive specifies the default command to run when starting a container from this image. Finally, the -t flag tags our image. Think of this simply as a human-readable name for the final image. Since we named the image node-todo , we can refer to that image when we run a container. The . at the end of the docker build command tells that Docker should look for the Dockerfile in the current directory. Starting an App Container Now that we have an image, let's run the application! To do so, we will use the docker run command (remember that from earlier?). 1.Start your container using the docker run command and specify the name of the image we just created: docker run -dp 3000:3000 node-todo Remember the -d and -p flags? We're running the new container in \"detached\" mode (in the background) and creating a mapping between the host's port 3000 to the container's port 3000 . Without the port mapping, we wouldn't be able to access the application. 2.After a few seconds, open your web browser to http://localhost:3000 . You should see our app! 3.Go ahead and add an item or two and see that it works as you expect. You can mark items as complete and remove items. Your frontend is successfully storing items in the backend! Pretty quick and easy, huh? At this point, you should have a running todo list manager with a few items, all built by you! Now, let's make a few changes and learn about managing our containers. If you take a quick look at the Docker Dashboard, you should see your two containers running now (this tutorial and your freshly launched app container)! Recap In this short section, we learned the very basics about building a container image and created a Dockerfile to do so. Once we built an image, we started the container and saw the running app! Next, we're going to make a modification to our app and learn how to update our running application with a new image. Along the way, we'll learn a few other useful commands.","title":"Part2. Sample application"},{"location":"tutorial/our-application/#sample-application","text":"For the rest of this tutorial, we will be working with a simple todo list manager that is running in Node.js. If you're not familiar with Node.js, don't worry! No real JavaScript experience is needed! At this point, your development team is quite small and you're simply building an app to prove out your MVP (minimum viable product). You want to show how it works and what it's capable of doing without needing to think about how it will work for a large team, multiple developers, etc.","title":"Sample application"},{"location":"tutorial/our-application/#getting-our-app","text":"Before we can run the application, we need to get the application source code onto our machine. For real projects, you will typically clone the repo. But, for this tutorial, we have created a ZIP file containing the application. Download the App contents from the getting-started repository . You can either pull the entire project or download it as a zip and extract the app folder out to get started with. Once extracted, use your favorite code editor to open the project. If you're in need of an editor, you can use Visual Studio Code . You should see the package.json and two subdirectories ( src and spec ).","title":"Getting our App"},{"location":"tutorial/our-application/#building-the-apps-container-image","text":"In order to build the application, we need to use a Dockerfile . A Dockerfile is simply a text-based script of instructions that is used to create a container image. If you've created Dockerfiles before, you might see a few flaws in the Dockerfile below. But, don't worry! We'll go over them. 1.Create a file named Dockerfile in the same folder as the file package.json with the following contents. # syntax=docker/dockerfile:1 FROM node:12-alpine RUN apk add --no-cache python2 g++ make WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] EXPOSE 3000 Please check that the file Dockerfile has no file extension like .txt . Some editors may append this file extension automatically and this would result in an error in the next step. 2.If you haven't already done so, open a terminal and go to the app directory with the Dockerfile . Now build the container image using the docker build command. docker build -t node-todo . This command used the Dockerfile to build a new container image. You might have noticed that a lot of \"layers\" were downloaded. This is because we instructed the builder that we wanted to start from the node:12-alpine image. But, since we didn't have that on our machine, that image needed to be downloaded. After the image was downloaded, we copied in our application and used yarn to install our application's dependencies. The CMD directive specifies the default command to run when starting a container from this image. Finally, the -t flag tags our image. Think of this simply as a human-readable name for the final image. Since we named the image node-todo , we can refer to that image when we run a container. The . at the end of the docker build command tells that Docker should look for the Dockerfile in the current directory.","title":"Building the App's Container Image"},{"location":"tutorial/our-application/#starting-an-app-container","text":"Now that we have an image, let's run the application! To do so, we will use the docker run command (remember that from earlier?). 1.Start your container using the docker run command and specify the name of the image we just created: docker run -dp 3000:3000 node-todo Remember the -d and -p flags? We're running the new container in \"detached\" mode (in the background) and creating a mapping between the host's port 3000 to the container's port 3000 . Without the port mapping, we wouldn't be able to access the application. 2.After a few seconds, open your web browser to http://localhost:3000 . You should see our app! 3.Go ahead and add an item or two and see that it works as you expect. You can mark items as complete and remove items. Your frontend is successfully storing items in the backend! Pretty quick and easy, huh? At this point, you should have a running todo list manager with a few items, all built by you! Now, let's make a few changes and learn about managing our containers. If you take a quick look at the Docker Dashboard, you should see your two containers running now (this tutorial and your freshly launched app container)!","title":"Starting an App Container"},{"location":"tutorial/our-application/#recap","text":"In this short section, we learned the very basics about building a container image and created a Dockerfile to do so. Once we built an image, we started the container and saw the running app! Next, we're going to make a modification to our app and learn how to update our running application with a new image. Along the way, we'll learn a few other useful commands.","title":"Recap"},{"location":"tutorial/persisting-our-data/","text":"In case you didn't notice, our todo list is being wiped clean every single time we launch the container. Why is this? Let's dive into how the container is working. The Container's Filesystem When a container runs, it uses the various layers from an image for its filesystem. Each container also gets its own \"scratch space\" to create/update/remove files. Any changes won't be seen in another container, even if they are using the same image. Seeing this in Practice To see this in action, we're going to start two containers and create a file in each. What you'll see is that the files created in one container aren't available in another. Start a ubuntu container that will create a file named /data.txt with a random number between 1 and 10000. bash docker run -d ubuntu bash -c \"shuf -i 1-10000 -n 1 -o /data.txt && tail -f /dev/null\" In case you're curious about the command, we're starting a bash shell and invoking two commands (why we have the && ). The first portion picks a single random number and writes it to /data.txt . The second command is simply watching a file to keep the container running. Validate we can see the output by exec 'ing into the container. To do so, open the Dashboard and click the first action of the container that is running the ubuntu image. {: style=width:75% } {: .text-center } You will see a terminal that is running a shell in the ubuntu container. Run the following command to see the content of the /data.txt file. Close this terminal afterwards again. bash cat /data.txt If you prefer the command line you can use the docker exec command to do the same. You need to get the container's ID (use docker ps to get it) and get the content with the following command. bash docker exec <container-id> cat /data.txt You should see a random number! Now, let's start another ubuntu container (the same image) and we'll see we don't have the same file. bash docker run -it ubuntu ls / And look! There's no data.txt file there! That's because it was written to the scratch space for only the first container. Go ahead and remove the first container using the docker rm -f <container-id> command. bash docker rm -f <container-id> Container Volumes With the previous experiment, we saw that each container starts from the image definition each time it starts. While containers can create, update, and delete files, those changes are lost when the container is removed and all changes are isolated to that container. With volumes, we can change all of this. Volumes provide the ability to connect specific filesystem paths of the container back to the host machine. If a directory in the container is mounted, changes in that directory are also seen on the host machine. If we mount that same directory across container restarts, we'd see the same files. There are two main types of volumes. We will eventually use both, but we will start with named volumes . Persisting our Todo Data By default, the todo app stores its data in a SQLite Database at /etc/todos/todo.db . If you're not familiar with SQLite, no worries! It's simply a relational database in which all of the data is stored in a single file. While this isn't the best for large-scale applications, it works for small demos. We'll talk about switching this to a different database engine later. With the database being a single file, if we can persist that file on the host and make it available to the next container, it should be able to pick up where the last one left off. By creating a volume and attaching (often called \"mounting\") it to the directory the data is stored in, we can persist the data. As our container writes to the todo.db file, it will be persisted to the host in the volume. As mentioned, we are going to use a named volume . Think of a named volume as simply a bucket of data. Docker maintains the physical location on the disk and you only need to remember the name of the volume. Every time you use the volume, Docker will make sure the correct data is provided. Create a volume by using the docker volume create command. bash docker volume create todo-db Stop the todo app container once again in the Dashboard (or with docker rm -f <container-id> ), as it is still running without using the persistent volume. Start the todo app container, but add the -v flag to specify a volume mount. We will use the named volume and mount it to /etc/todos , which will capture all files created at the path. bash docker run -dp 3000:3000 -v todo-db:/etc/todos getting-started Once the container starts up, open the app and add a few items to your todo list. {: style=\"width: 55%; \" } {: .text-center } Remove the container for the todo app. Use the Dashboard or docker ps to get the ID and then docker rm -f <container-id> to remove it. Start a new container using the same command from above. Open the app. You should see your items still in your list! Go ahead and remove the container when you're done checking out your list. Hooray! You've now learned how to persist data! !!! info \"Pro-tip\" While named volumes and bind mounts (which we'll talk about in a minute) are the two main types of volumes supported by a default Docker engine installation, there are many volume driver plugins available to support NFS, SFTP, NetApp, and more! This will be especially important once you start running containers on multiple hosts in a clustered environment with Swarm, Kubernetes, etc. Diving into our Volume A lot of people frequently ask \"Where is Docker actually storing my data when I use a named volume?\" If you want to know, you can use the docker volume inspect command. docker volume inspect todo-db [ { \"CreatedAt\": \"2019-09-26T02:18:36Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/todo-db/_data\", \"Name\": \"todo-db\", \"Options\": {}, \"Scope\": \"local\" } ] The Mountpoint is the actual location on the disk where the data is stored. Note that on most machines, you will need to have root access to access this directory from the host. But, that's where it is! !!! info \"Accessing Volume data directly on Docker Desktop\" While running in Docker Desktop, the Docker commands are actually running inside a small VM on your machine. If you wanted to look at the actual contents of the Mountpoint directory, you would need to first get inside of the VM. Recap At this point, we have a functioning application that can survive restarts! We can show it off to our investors and hope they can catch our vision! However, we saw earlier that rebuilding images for every change takes quite a bit of time. There's got to be a better way to make changes, right? With bind mounts (which we hinted at earlier), there is a better way! Let's take a look at that now!","title":"Part5. Persist the DB"},{"location":"tutorial/persisting-our-data/#the-containers-filesystem","text":"When a container runs, it uses the various layers from an image for its filesystem. Each container also gets its own \"scratch space\" to create/update/remove files. Any changes won't be seen in another container, even if they are using the same image.","title":"The Container's Filesystem"},{"location":"tutorial/persisting-our-data/#seeing-this-in-practice","text":"To see this in action, we're going to start two containers and create a file in each. What you'll see is that the files created in one container aren't available in another. Start a ubuntu container that will create a file named /data.txt with a random number between 1 and 10000. bash docker run -d ubuntu bash -c \"shuf -i 1-10000 -n 1 -o /data.txt && tail -f /dev/null\" In case you're curious about the command, we're starting a bash shell and invoking two commands (why we have the && ). The first portion picks a single random number and writes it to /data.txt . The second command is simply watching a file to keep the container running. Validate we can see the output by exec 'ing into the container. To do so, open the Dashboard and click the first action of the container that is running the ubuntu image. {: style=width:75% } {: .text-center } You will see a terminal that is running a shell in the ubuntu container. Run the following command to see the content of the /data.txt file. Close this terminal afterwards again. bash cat /data.txt If you prefer the command line you can use the docker exec command to do the same. You need to get the container's ID (use docker ps to get it) and get the content with the following command. bash docker exec <container-id> cat /data.txt You should see a random number! Now, let's start another ubuntu container (the same image) and we'll see we don't have the same file. bash docker run -it ubuntu ls / And look! There's no data.txt file there! That's because it was written to the scratch space for only the first container. Go ahead and remove the first container using the docker rm -f <container-id> command. bash docker rm -f <container-id>","title":"Seeing this in Practice"},{"location":"tutorial/persisting-our-data/#container-volumes","text":"With the previous experiment, we saw that each container starts from the image definition each time it starts. While containers can create, update, and delete files, those changes are lost when the container is removed and all changes are isolated to that container. With volumes, we can change all of this. Volumes provide the ability to connect specific filesystem paths of the container back to the host machine. If a directory in the container is mounted, changes in that directory are also seen on the host machine. If we mount that same directory across container restarts, we'd see the same files. There are two main types of volumes. We will eventually use both, but we will start with named volumes .","title":"Container Volumes"},{"location":"tutorial/persisting-our-data/#persisting-our-todo-data","text":"By default, the todo app stores its data in a SQLite Database at /etc/todos/todo.db . If you're not familiar with SQLite, no worries! It's simply a relational database in which all of the data is stored in a single file. While this isn't the best for large-scale applications, it works for small demos. We'll talk about switching this to a different database engine later. With the database being a single file, if we can persist that file on the host and make it available to the next container, it should be able to pick up where the last one left off. By creating a volume and attaching (often called \"mounting\") it to the directory the data is stored in, we can persist the data. As our container writes to the todo.db file, it will be persisted to the host in the volume. As mentioned, we are going to use a named volume . Think of a named volume as simply a bucket of data. Docker maintains the physical location on the disk and you only need to remember the name of the volume. Every time you use the volume, Docker will make sure the correct data is provided. Create a volume by using the docker volume create command. bash docker volume create todo-db Stop the todo app container once again in the Dashboard (or with docker rm -f <container-id> ), as it is still running without using the persistent volume. Start the todo app container, but add the -v flag to specify a volume mount. We will use the named volume and mount it to /etc/todos , which will capture all files created at the path. bash docker run -dp 3000:3000 -v todo-db:/etc/todos getting-started Once the container starts up, open the app and add a few items to your todo list. {: style=\"width: 55%; \" } {: .text-center } Remove the container for the todo app. Use the Dashboard or docker ps to get the ID and then docker rm -f <container-id> to remove it. Start a new container using the same command from above. Open the app. You should see your items still in your list! Go ahead and remove the container when you're done checking out your list. Hooray! You've now learned how to persist data! !!! info \"Pro-tip\" While named volumes and bind mounts (which we'll talk about in a minute) are the two main types of volumes supported by a default Docker engine installation, there are many volume driver plugins available to support NFS, SFTP, NetApp, and more! This will be especially important once you start running containers on multiple hosts in a clustered environment with Swarm, Kubernetes, etc.","title":"Persisting our Todo Data"},{"location":"tutorial/persisting-our-data/#diving-into-our-volume","text":"A lot of people frequently ask \"Where is Docker actually storing my data when I use a named volume?\" If you want to know, you can use the docker volume inspect command. docker volume inspect todo-db [ { \"CreatedAt\": \"2019-09-26T02:18:36Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/todo-db/_data\", \"Name\": \"todo-db\", \"Options\": {}, \"Scope\": \"local\" } ] The Mountpoint is the actual location on the disk where the data is stored. Note that on most machines, you will need to have root access to access this directory from the host. But, that's where it is! !!! info \"Accessing Volume data directly on Docker Desktop\" While running in Docker Desktop, the Docker commands are actually running inside a small VM on your machine. If you wanted to look at the actual contents of the Mountpoint directory, you would need to first get inside of the VM.","title":"Diving into our Volume"},{"location":"tutorial/persisting-our-data/#recap","text":"At this point, we have a functioning application that can survive restarts! We can show it off to our investors and hope they can catch our vision! However, we saw earlier that rebuilding images for every change takes quite a bit of time. There's got to be a better way to make changes, right? With bind mounts (which we hinted at earlier), there is a better way! Let's take a look at that now!","title":"Recap"},{"location":"tutorial/sharing-our-app/","text":"Share the application Now that we've built an image, let's share it! To share Docker images, you have to use a Docker registry. The default registry is Docker Hub and is where all of the images we've used have come from. Docker ID A Docker ID allows you to access Docker Hub which is the world\u2019s largest library and community for container images. Create a Docker ID for free if you don\u2019t have one. Create a Repo To push an image, we first need to create a repo on Docker Hub . Sign up or Sign in to Docker Hub . Click the Create Repository button. For the repo name, use node-todo . Make sure the Visibility is Public . Click the Create button! Private repositories Did you know that Docker offers private repositories which allows you to restrict content to specific users or teams? Check out the details on the Docker pricing page. If you look on the right-side of the page, you'll see a section named Docker commands . This gives an example command that you will need to run to push to this repo. Pushing our Image 1.In the command line, try running the push command you see on Docker Hub. Note that your command will be using your namespace -> YOUR-USER-NAME . $ docker push YOUR-USER-NAME/node-todo The push refers to repository [docker.io/YOUR-USER-NAME/node-todo] An image does not exist locally with the tag: YOUR-USER-NAME/node-todo Why did it fail? The push command was looking for an image named YOUR-USER-NAME/node-todo, but didn't find one. If you run docker image ls , you won't see one either. To fix this, we need to \"tag\" our existing image we've built to give it another name. 2.Login to the Docker Hub using the command docker login -u YOUR-USER-NAME . 3.Use the docker tag command to give the node-todo image a new name. Be sure to swap out YOUR-USER-NAME with your Docker ID. docker tag node-todo YOUR-USER-NAME/node-todo 4.Now try your push command again. If you're copying the value from Docker Hub, you can drop the tagname portion, as we didn't add a tag to the image name. If you don't specify a tag, Docker will use a tag called latest . docker push YOUR-USER-NAME/node-todo Running our Image on a New Instance Now that our image has been built and pushed into a registry, let's try running our app on a brand new instance that has never seen this container image! To do this, we will use Play with Docker. Open your browser to Play with Docker . Log in with your Docker Hub account. Once you're logged in, click on the \"+ ADD NEW INSTANCE\" link in the left side bar. (If you don't see it, make your browser a little wider.) After a few seconds, a terminal window will be opened in your browser. 1.In the terminal, start your freshly pushed app. docker run -dp 3000:3000 YOUR-USER-NAME/node-todo You should see the image get pulled down and eventually start up! 2.Click on the 3000 badge when it comes up and you should see the app with your modifications! Hooray! If the 3000 badge doesn't show up, you can click on the \"Open Port\" button and type in 3000 . Recap In this section, we learned how to share our images by pushing them to a registry. We then went to a brand new instance and were able to run the freshly pushed image. This is quite common in CI pipelines, where the pipeline will create the image and push it to a registry and then the production environment can use the latest version of the image. Now that we have that figured out, let's circle back around to what we noticed at the end of the last section. As a reminder, we noticed that when we restarted the app, we lost all of our todo list items. That's obviously not a great user experience, so let's learn how we can persist the data across restarts!","title":"Part4. Share the application"},{"location":"tutorial/sharing-our-app/#share-the-application","text":"Now that we've built an image, let's share it! To share Docker images, you have to use a Docker registry. The default registry is Docker Hub and is where all of the images we've used have come from. Docker ID A Docker ID allows you to access Docker Hub which is the world\u2019s largest library and community for container images. Create a Docker ID for free if you don\u2019t have one.","title":"Share the application"},{"location":"tutorial/sharing-our-app/#create-a-repo","text":"To push an image, we first need to create a repo on Docker Hub . Sign up or Sign in to Docker Hub . Click the Create Repository button. For the repo name, use node-todo . Make sure the Visibility is Public . Click the Create button! Private repositories Did you know that Docker offers private repositories which allows you to restrict content to specific users or teams? Check out the details on the Docker pricing page. If you look on the right-side of the page, you'll see a section named Docker commands . This gives an example command that you will need to run to push to this repo.","title":"Create a Repo"},{"location":"tutorial/sharing-our-app/#pushing-our-image","text":"1.In the command line, try running the push command you see on Docker Hub. Note that your command will be using your namespace -> YOUR-USER-NAME . $ docker push YOUR-USER-NAME/node-todo The push refers to repository [docker.io/YOUR-USER-NAME/node-todo] An image does not exist locally with the tag: YOUR-USER-NAME/node-todo Why did it fail? The push command was looking for an image named YOUR-USER-NAME/node-todo, but didn't find one. If you run docker image ls , you won't see one either. To fix this, we need to \"tag\" our existing image we've built to give it another name. 2.Login to the Docker Hub using the command docker login -u YOUR-USER-NAME . 3.Use the docker tag command to give the node-todo image a new name. Be sure to swap out YOUR-USER-NAME with your Docker ID. docker tag node-todo YOUR-USER-NAME/node-todo 4.Now try your push command again. If you're copying the value from Docker Hub, you can drop the tagname portion, as we didn't add a tag to the image name. If you don't specify a tag, Docker will use a tag called latest . docker push YOUR-USER-NAME/node-todo","title":"Pushing our Image"},{"location":"tutorial/sharing-our-app/#running-our-image-on-a-new-instance","text":"Now that our image has been built and pushed into a registry, let's try running our app on a brand new instance that has never seen this container image! To do this, we will use Play with Docker. Open your browser to Play with Docker . Log in with your Docker Hub account. Once you're logged in, click on the \"+ ADD NEW INSTANCE\" link in the left side bar. (If you don't see it, make your browser a little wider.) After a few seconds, a terminal window will be opened in your browser. 1.In the terminal, start your freshly pushed app. docker run -dp 3000:3000 YOUR-USER-NAME/node-todo You should see the image get pulled down and eventually start up! 2.Click on the 3000 badge when it comes up and you should see the app with your modifications! Hooray! If the 3000 badge doesn't show up, you can click on the \"Open Port\" button and type in 3000 .","title":"Running our Image on a New Instance"},{"location":"tutorial/sharing-our-app/#recap","text":"In this section, we learned how to share our images by pushing them to a registry. We then went to a brand new instance and were able to run the freshly pushed image. This is quite common in CI pipelines, where the pipeline will create the image and push it to a registry and then the production environment can use the latest version of the image. Now that we have that figured out, let's circle back around to what we noticed at the end of the last section. As a reminder, we noticed that when we restarted the app, we lost all of our todo list items. That's obviously not a great user experience, so let's learn how we can persist the data across restarts!","title":"Recap"},{"location":"tutorial/updating-our-app/","text":"Update the application As a small feature request, we've been asked by the product team to change the \"empty text\" when we don't have any todo list items. They would like to transition it to the following: You have no todo items yet! Add one above! Pretty simple, right? Let's make the change. Update the source code 1.In the src/static/js/app.js file, update line 56 to use the new empty text. - <p className=\"text-center\">No items yet! Add one above!</p> + <p className=\"text-center\">You have no todo items yet! Add one above!</p> 2.Let's build our updated version of the image, using the same command we used before. docker build -t node-todo . 3.Let's start a new container using the updated code. docker run -dp 3000:3000 node-todo Uh oh! You probably saw an error like this (the IDs will be different): docker: Error response from daemon: driver failed programming external connectivity on endpoint laughing_burnell (bb242b2ca4d67eba76e79474fb36bb5125708ebdabd7f45c8eaf16caaabde9dd): Bind for 0.0.0.0:3000 failed: port is already allocated. So, what happened? We aren't able to start the new container because our old container is still running. The reason this is a problem is because that container is using the host's port 3000 and only one process on the machine (containers included) can listen to a specific port. To fix this, we need to remove the old container. Replacing our Old Container To remove a container, it first needs to be stopped. Once it has stopped, it can be removed. We have two ways that we can remove the old container. Feel free to choose the path that you're most comfortable with. Removing a container using the CLI 1.Get the ID of the container by using the docker ps command. docker ps 2.Use the docker stop command to stop the container. # Swap out <the-container-id> with the ID from docker ps docker stop <the-container-id> 3.Once the container has stopped, you can remove it by using the docker rm command. docker rm <the-container-id> You can stop and remove a container in a single command by adding the \"force\" flag to the docker rm command. For example: docker rm -f <the-container-id> Removing a container using the Docker Dashboard If you open the Docker dashboard, you can remove a container with two clicks! It's certainly much easier than having to look up the container ID and remove it. With the dashboard opened, hover over the app container and you'll see a collection of action buttons appear on the right. Click on the trash can icon to delete the container. Confirm the removal and you're done! Starting our updated app container 1.Now, start your updated app. docker run -dp 3000:3000 node-todo 2.Refresh your browser on http://localhost:3000 and you should see your updated help text! Recap While we were able to build an update, there were two things you might have noticed: All of the existing items in our todo list are gone! That's not a very good app! We'll talk about that shortly. There were a lot of steps involved for such a small change. In an upcoming section, we'll talk about how to see code updates without needing to rebuild and start a new container very time we make a change. Before talking about persistence, we'll quickly see how to share these images with others.","title":"Part3. Update the application"},{"location":"tutorial/updating-our-app/#update-the-application","text":"As a small feature request, we've been asked by the product team to change the \"empty text\" when we don't have any todo list items. They would like to transition it to the following: You have no todo items yet! Add one above! Pretty simple, right? Let's make the change.","title":"Update the application"},{"location":"tutorial/updating-our-app/#update-the-source-code","text":"1.In the src/static/js/app.js file, update line 56 to use the new empty text. - <p className=\"text-center\">No items yet! Add one above!</p> + <p className=\"text-center\">You have no todo items yet! Add one above!</p> 2.Let's build our updated version of the image, using the same command we used before. docker build -t node-todo . 3.Let's start a new container using the updated code. docker run -dp 3000:3000 node-todo Uh oh! You probably saw an error like this (the IDs will be different): docker: Error response from daemon: driver failed programming external connectivity on endpoint laughing_burnell (bb242b2ca4d67eba76e79474fb36bb5125708ebdabd7f45c8eaf16caaabde9dd): Bind for 0.0.0.0:3000 failed: port is already allocated. So, what happened? We aren't able to start the new container because our old container is still running. The reason this is a problem is because that container is using the host's port 3000 and only one process on the machine (containers included) can listen to a specific port. To fix this, we need to remove the old container.","title":"Update the source code"},{"location":"tutorial/updating-our-app/#replacing-our-old-container","text":"To remove a container, it first needs to be stopped. Once it has stopped, it can be removed. We have two ways that we can remove the old container. Feel free to choose the path that you're most comfortable with.","title":"Replacing our Old Container"},{"location":"tutorial/updating-our-app/#removing-a-container-using-the-cli","text":"1.Get the ID of the container by using the docker ps command. docker ps 2.Use the docker stop command to stop the container. # Swap out <the-container-id> with the ID from docker ps docker stop <the-container-id> 3.Once the container has stopped, you can remove it by using the docker rm command. docker rm <the-container-id> You can stop and remove a container in a single command by adding the \"force\" flag to the docker rm command. For example: docker rm -f <the-container-id>","title":"Removing a container using the CLI"},{"location":"tutorial/updating-our-app/#removing-a-container-using-the-docker-dashboard","text":"If you open the Docker dashboard, you can remove a container with two clicks! It's certainly much easier than having to look up the container ID and remove it. With the dashboard opened, hover over the app container and you'll see a collection of action buttons appear on the right. Click on the trash can icon to delete the container. Confirm the removal and you're done!","title":"Removing a container using the Docker Dashboard"},{"location":"tutorial/updating-our-app/#starting-our-updated-app-container","text":"1.Now, start your updated app. docker run -dp 3000:3000 node-todo 2.Refresh your browser on http://localhost:3000 and you should see your updated help text!","title":"Starting our updated app container"},{"location":"tutorial/updating-our-app/#recap","text":"While we were able to build an update, there were two things you might have noticed: All of the existing items in our todo list are gone! That's not a very good app! We'll talk about that shortly. There were a lot of steps involved for such a small change. In an upcoming section, we'll talk about how to see code updates without needing to rebuild and start a new container very time we make a change. Before talking about persistence, we'll quickly see how to share these images with others.","title":"Recap"},{"location":"tutorial/using-bind-mounts/","text":"In the previous chapter, we talked about and used a named volume to persist the data in our database. Named volumes are great if we simply want to store data, as we don't have to worry about where the data is stored. With bind mounts , we control the exact mountpoint on the host. We can use this to persist data, but is often used to provide additional data into containers. When working on an application, we can use a bind mount to mount our source code into the container to let it see code changes, respond, and let us see the changes right away. For Node-based applications, nodemon is a great tool to watch for file changes and then restart the application. There are equivalent tools in most other languages and frameworks. Quick Volume Type Comparisons Bind mounts and named volumes are the two main types of volumes that come with the Docker engine. However, additional volume drivers are available to support other use cases ( SFTP , Ceph , NetApp , S3 , and more). Named Volumes Bind Mounts Host Location Docker chooses You control Mount Example (using -v ) my-volume:/usr/local/data /path/to/data:/usr/local/data Populates new volume with container contents Yes No Supports Volume Drivers Yes No Starting a Dev-Mode Container To run our container to support a development workflow, we will do the following: Mount our source code into the container Install all dependencies, including the \"dev\" dependencies Start nodemon to watch for filesystem changes So, let's do it! Make sure you don't have any previous getting-started containers running. Also make sure you are in app source code directory, i.e. /path/to/getting-started/app . If you aren't, you can cd into it, .e.g: bash cd /path/to/getting-started/app Now that you are in the getting-started/app directory, run the following command. We'll explain what's going on afterwards: bash docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you are using PowerShell then use this command. powershell docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" If you are using an Apple Silicon Mac or another ARM64 device then use this command. bash docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ node:12-alpine \\ sh -c \"apk add --no-cache python2 g++ make && yarn install && yarn run dev\" -dp 3000:3000 - same as before. Run in detached (background) mode and create a port mapping -w /app - sets the container's present working directory where the command will run from -v \"$(pwd):/app\" - bind mount (link) the host's present getting-started/app directory to the container's /app directory. Note: Docker requires absolute paths for binding mounts, so in this example we use pwd for printing the absolute path of the working directory, i.e. the app directory, instead of typing it manually node:12-alpine - the image to use. Note that this is the base image for our app from the Dockerfile sh -c \"yarn install && yarn run dev\" - the command. We're starting a shell using sh (alpine doesn't have bash ) and running yarn install to install all dependencies and then running yarn run dev . If we look in the package.json , we'll see that the dev script is starting nodemon . You can watch the logs using docker logs -f <container-id> . You'll know you're ready to go when you see this... bash docker logs -f <container-id> $ nodemon src/index.js [nodemon] 1.19.2 [nodemon] to restart at any time, enter `rs` [nodemon] watching dir(s): *.* [nodemon] starting `node src/index.js` Using sqlite database at /etc/todos/todo.db Listening on port 3000 When you're done watching the logs, exit out by hitting Ctrl + C . Now, let's make a change to the app. In the src/static/js/app.js file, let's change the \"Add Item\" button to simply say \"Add\". This change will be on line 109 - remember to save the file. diff - {submitting ? 'Adding...' : 'Add Item'} + {submitting ? 'Adding...' : 'Add'} Simply refresh the page (or open it) and you should see the change reflected in the browser almost immediately. It might take a few seconds for the Node server to restart, so if you get an error, just try refreshing after a few seconds. {: style=\"width:75%;\"} {: .text-center } Feel free to make any other changes you'd like to make. When you're done, stop the container and build your new image using docker build -t getting-started . . Using bind mounts is very common for local development setups. The advantage is that the dev machine doesn't need to have all of the build tools and environments installed. With a single docker run command, the dev environment is pulled and ready to go. We'll talk about Docker Compose in a future step, as this will help simplify our commands (we're already getting a lot of flags). Recap At this point, we can persist our database and respond rapidly to the needs and demands of our investors and founders. Hooray! But, guess what? We received great news! Your project has been selected for future development! In order to prepare for production, we need to migrate our database from working in SQLite to something that can scale a little better. For simplicity, we'll keep with a relational database and switch our application to use MySQL. But, how should we run MySQL? How do we allow the containers to talk to each other? We'll talk about that next!","title":"Part6. Use bind mounts"},{"location":"tutorial/using-bind-mounts/#quick-volume-type-comparisons","text":"Bind mounts and named volumes are the two main types of volumes that come with the Docker engine. However, additional volume drivers are available to support other use cases ( SFTP , Ceph , NetApp , S3 , and more). Named Volumes Bind Mounts Host Location Docker chooses You control Mount Example (using -v ) my-volume:/usr/local/data /path/to/data:/usr/local/data Populates new volume with container contents Yes No Supports Volume Drivers Yes No","title":"Quick Volume Type Comparisons"},{"location":"tutorial/using-bind-mounts/#starting-a-dev-mode-container","text":"To run our container to support a development workflow, we will do the following: Mount our source code into the container Install all dependencies, including the \"dev\" dependencies Start nodemon to watch for filesystem changes So, let's do it! Make sure you don't have any previous getting-started containers running. Also make sure you are in app source code directory, i.e. /path/to/getting-started/app . If you aren't, you can cd into it, .e.g: bash cd /path/to/getting-started/app Now that you are in the getting-started/app directory, run the following command. We'll explain what's going on afterwards: bash docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you are using PowerShell then use this command. powershell docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" If you are using an Apple Silicon Mac or another ARM64 device then use this command. bash docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ node:12-alpine \\ sh -c \"apk add --no-cache python2 g++ make && yarn install && yarn run dev\" -dp 3000:3000 - same as before. Run in detached (background) mode and create a port mapping -w /app - sets the container's present working directory where the command will run from -v \"$(pwd):/app\" - bind mount (link) the host's present getting-started/app directory to the container's /app directory. Note: Docker requires absolute paths for binding mounts, so in this example we use pwd for printing the absolute path of the working directory, i.e. the app directory, instead of typing it manually node:12-alpine - the image to use. Note that this is the base image for our app from the Dockerfile sh -c \"yarn install && yarn run dev\" - the command. We're starting a shell using sh (alpine doesn't have bash ) and running yarn install to install all dependencies and then running yarn run dev . If we look in the package.json , we'll see that the dev script is starting nodemon . You can watch the logs using docker logs -f <container-id> . You'll know you're ready to go when you see this... bash docker logs -f <container-id> $ nodemon src/index.js [nodemon] 1.19.2 [nodemon] to restart at any time, enter `rs` [nodemon] watching dir(s): *.* [nodemon] starting `node src/index.js` Using sqlite database at /etc/todos/todo.db Listening on port 3000 When you're done watching the logs, exit out by hitting Ctrl + C . Now, let's make a change to the app. In the src/static/js/app.js file, let's change the \"Add Item\" button to simply say \"Add\". This change will be on line 109 - remember to save the file. diff - {submitting ? 'Adding...' : 'Add Item'} + {submitting ? 'Adding...' : 'Add'} Simply refresh the page (or open it) and you should see the change reflected in the browser almost immediately. It might take a few seconds for the Node server to restart, so if you get an error, just try refreshing after a few seconds. {: style=\"width:75%;\"} {: .text-center } Feel free to make any other changes you'd like to make. When you're done, stop the container and build your new image using docker build -t getting-started . . Using bind mounts is very common for local development setups. The advantage is that the dev machine doesn't need to have all of the build tools and environments installed. With a single docker run command, the dev environment is pulled and ready to go. We'll talk about Docker Compose in a future step, as this will help simplify our commands (we're already getting a lot of flags).","title":"Starting a Dev-Mode Container"},{"location":"tutorial/using-bind-mounts/#recap","text":"At this point, we can persist our database and respond rapidly to the needs and demands of our investors and founders. Hooray! But, guess what? We received great news! Your project has been selected for future development! In order to prepare for production, we need to migrate our database from working in SQLite to something that can scale a little better. For simplicity, we'll keep with a relational database and switch our application to use MySQL. But, how should we run MySQL? How do we allow the containers to talk to each other? We'll talk about that next!","title":"Recap"},{"location":"tutorial/using-docker-compose/","text":"Docker Compose is a tool that was developed to help define and share multi-container applications. With Compose, we can create a YAML file to define the services and with a single command, can spin everything up or tear it all down. The big advantage of using Compose is you can define your application stack in a file, keep it at the root of your project repo (it's now version controlled), and easily enable someone else to contribute to your project. Someone would only need to clone your repo and start the compose app. In fact, you might see quite a few projects on GitHub/GitLab doing exactly this now. So, how do we get started? Installing Docker Compose If you installed Docker Desktop/Toolbox for either Windows or Mac, you already have Docker Compose! Play-with-Docker instances already have Docker Compose installed as well. If you are on a Linux machine, you will need to install Docker Compose using the instructions here . After installation, you should be able to run the following and see version information. docker-compose version Creating our Compose File At the root of the app project, create a file named docker-compose.yml . In the compose file, we'll start off by defining the schema version. In most cases, it's best to use the latest supported version. You can look at the Compose file reference for the current schema versions and the compatibility matrix. yaml version: \"3.8\" Next, we'll define the list of services (or containers) we want to run as part of our application. ```yaml hl_lines=\"3\" version: \"3.8\" services: ``` And now, we'll start migrating a service at a time into the compose file. Defining the App Service To remember, this was the command we were using to define our app container. docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you are using PowerShell then use this command. docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" First, let's define the service entry and the image for the container. We can pick any name for the service. The name will automatically become a network alias, which will be useful when defining our MySQL service. ```yaml hl_lines=\"4 5\" version: \"3.8\" services: app: image: node:12-alpine ``` Typically, you will see the command close to the image definition, although there is no requirement on ordering. So, let's go ahead and move that into our file. ```yaml hl_lines=\"6\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ``` Let's migrate the -p 3000:3000 part of the command by defining the ports for the service. We will use the short syntax here, but there is also a more verbose long syntax available as well. ```yaml hl_lines=\"7 8\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 ``` Next, we'll migrate both the working directory ( -w /app ) and the volume mapping ( -v \"$(pwd):/app\" ) by using the working_dir and volumes definitions. Volumes also has a short and long syntax. One advantage of Docker Compose volume definitions is we can use relative paths from the current directory. ```yaml hl_lines=\"9 10 11\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app ``` Finally, we need to migrate the environment variable definitions using the environment key. ```yaml hl_lines=\"12 13 14 15 16\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos ``` Defining the MySQL Service Now, it's time to define the MySQL service. The command that we used for that container was the following: docker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command. docker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 We will first define the new service and name it mysql so it automatically gets the network alias. We'll go ahead and specify the image to use as well. ```yaml hl_lines=\"6 7\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 ``` Next, we'll define the volume mapping. When we ran the container with docker run , the named volume was created automatically. However, that doesn't happen when running with Compose. We need to define the volume in the top-level volumes: section and then specify the mountpoint in the service config. By simply providing only the volume name, the default options are used. There are many more options available though. ```yaml hl_lines=\"8 9 10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql volumes: todo-mysql-data: ``` Finally, we only need to specify the environment variables. ```yaml hl_lines=\"10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: ``` At this point, our complete docker-compose.yml should look like this: version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: Running our Application Stack Now that we have our docker-compose.yml file, we can start it up! Make sure no other copies of the app/db are running first ( docker ps and docker rm -f <ids> ). Start up the application stack using the docker-compose up command. We'll add the -d flag to run everything in the background. bash docker-compose up -d When we run this, we should see output like this: plaintext Creating network \"app_default\" with the default driver Creating volume \"app_todo-mysql-data\" with default driver Creating app_app_1 ... done Creating app_mysql_1 ... done You'll notice that the volume was created as well as a network! By default, Docker Compose automatically creates a network specifically for the application stack (which is why we didn't define one in the compose file). Let's look at the logs using the docker-compose logs -f command. You'll see the logs from each of the services interleaved into a single stream. This is incredibly useful when you want to watch for timing-related issues. The -f flag \"follows\" the log, so will give you live output as it's generated. If you don't already, you'll see output that looks like this... plaintext mysql_1 | 2019-10-03T03:07:16.083639Z 0 [Note] mysqld: ready for connections. mysql_1 | Version: '5.7.27' socket: '/var/run/mysqld/mysqld.sock' port: 3306 MySQL Community Server (GPL) app_1 | Connected to mysql db at host mysql app_1 | Listening on port 3000 The service name is displayed at the beginning of the line (often colored) to help distinguish messages. If you want to view the logs for a specific service, you can add the service name to the end of the logs command (for example, docker-compose logs -f app ). !!! info \"Pro tip - Waiting for the DB before starting the app\" When the app is starting up, it actually sits and waits for MySQL to be up and ready before trying to connect to it. Docker doesn't have any built-in support to wait for another container to be fully up, running, and ready before starting another container. For Node-based projects, you can use the wait-port dependency. Similar projects exist for other languages/frameworks. At this point, you should be able to open your app and see it running. And hey! We're down to a single command! Seeing our App Stack in Docker Dashboard If we look at the Docker Dashboard, we'll see that there is a group named app . This is the \"project name\" from Docker Compose and used to group the containers together. By default, the project name is simply the name of the directory that the docker-compose.yml was located in. If you twirl down the app, you will see the two containers we defined in the compose file. The names are also a little more descriptive, as they follow the pattern of <project-name>_<service-name>_<replica-number> . So, it's very easy to quickly see what container is our app and which container is the mysql database. Tearing it All Down When you're ready to tear it all down, simply run docker-compose down or hit the trash can on the Docker Dashboard for the entire app. The containers will stop and the network will be removed. !!! warning \"Removing Volumes\" By default, named volumes in your compose file are NOT removed when running docker-compose down . If you want to remove the volumes, you will need to add the --volumes flag. The Docker Dashboard does _not_ remove volumes when you delete the app stack. Once torn down, you can switch to another project, run docker-compose up and be ready to contribute to that project! It really doesn't get much simpler than that! Recap In this section, we learned about Docker Compose and how it helps us dramatically simplify the defining and sharing of multi-service applications. We created a Compose file by translating the commands we were using into the appropriate compose format. At this point, we're starting to wrap up the tutorial. However, there are a few best practices about image building we want to cover, as there is a big issue with the Dockerfile we've been using. So, let's take a look!","title":"Part8. Use Docker Compose"},{"location":"tutorial/using-docker-compose/#installing-docker-compose","text":"If you installed Docker Desktop/Toolbox for either Windows or Mac, you already have Docker Compose! Play-with-Docker instances already have Docker Compose installed as well. If you are on a Linux machine, you will need to install Docker Compose using the instructions here . After installation, you should be able to run the following and see version information. docker-compose version","title":"Installing Docker Compose"},{"location":"tutorial/using-docker-compose/#creating-our-compose-file","text":"At the root of the app project, create a file named docker-compose.yml . In the compose file, we'll start off by defining the schema version. In most cases, it's best to use the latest supported version. You can look at the Compose file reference for the current schema versions and the compatibility matrix. yaml version: \"3.8\" Next, we'll define the list of services (or containers) we want to run as part of our application. ```yaml hl_lines=\"3\" version: \"3.8\" services: ``` And now, we'll start migrating a service at a time into the compose file.","title":"Creating our Compose File"},{"location":"tutorial/using-docker-compose/#defining-the-app-service","text":"To remember, this was the command we were using to define our app container. docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you are using PowerShell then use this command. docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" First, let's define the service entry and the image for the container. We can pick any name for the service. The name will automatically become a network alias, which will be useful when defining our MySQL service. ```yaml hl_lines=\"4 5\" version: \"3.8\" services: app: image: node:12-alpine ``` Typically, you will see the command close to the image definition, although there is no requirement on ordering. So, let's go ahead and move that into our file. ```yaml hl_lines=\"6\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ``` Let's migrate the -p 3000:3000 part of the command by defining the ports for the service. We will use the short syntax here, but there is also a more verbose long syntax available as well. ```yaml hl_lines=\"7 8\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 ``` Next, we'll migrate both the working directory ( -w /app ) and the volume mapping ( -v \"$(pwd):/app\" ) by using the working_dir and volumes definitions. Volumes also has a short and long syntax. One advantage of Docker Compose volume definitions is we can use relative paths from the current directory. ```yaml hl_lines=\"9 10 11\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app ``` Finally, we need to migrate the environment variable definitions using the environment key. ```yaml hl_lines=\"12 13 14 15 16\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos ```","title":"Defining the App Service"},{"location":"tutorial/using-docker-compose/#defining-the-mysql-service","text":"Now, it's time to define the MySQL service. The command that we used for that container was the following: docker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command. docker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 We will first define the new service and name it mysql so it automatically gets the network alias. We'll go ahead and specify the image to use as well. ```yaml hl_lines=\"6 7\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 ``` Next, we'll define the volume mapping. When we ran the container with docker run , the named volume was created automatically. However, that doesn't happen when running with Compose. We need to define the volume in the top-level volumes: section and then specify the mountpoint in the service config. By simply providing only the volume name, the default options are used. There are many more options available though. ```yaml hl_lines=\"8 9 10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql volumes: todo-mysql-data: ``` Finally, we only need to specify the environment variables. ```yaml hl_lines=\"10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: ``` At this point, our complete docker-compose.yml should look like this: version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data:","title":"Defining the MySQL Service"},{"location":"tutorial/using-docker-compose/#running-our-application-stack","text":"Now that we have our docker-compose.yml file, we can start it up! Make sure no other copies of the app/db are running first ( docker ps and docker rm -f <ids> ). Start up the application stack using the docker-compose up command. We'll add the -d flag to run everything in the background. bash docker-compose up -d When we run this, we should see output like this: plaintext Creating network \"app_default\" with the default driver Creating volume \"app_todo-mysql-data\" with default driver Creating app_app_1 ... done Creating app_mysql_1 ... done You'll notice that the volume was created as well as a network! By default, Docker Compose automatically creates a network specifically for the application stack (which is why we didn't define one in the compose file). Let's look at the logs using the docker-compose logs -f command. You'll see the logs from each of the services interleaved into a single stream. This is incredibly useful when you want to watch for timing-related issues. The -f flag \"follows\" the log, so will give you live output as it's generated. If you don't already, you'll see output that looks like this... plaintext mysql_1 | 2019-10-03T03:07:16.083639Z 0 [Note] mysqld: ready for connections. mysql_1 | Version: '5.7.27' socket: '/var/run/mysqld/mysqld.sock' port: 3306 MySQL Community Server (GPL) app_1 | Connected to mysql db at host mysql app_1 | Listening on port 3000 The service name is displayed at the beginning of the line (often colored) to help distinguish messages. If you want to view the logs for a specific service, you can add the service name to the end of the logs command (for example, docker-compose logs -f app ). !!! info \"Pro tip - Waiting for the DB before starting the app\" When the app is starting up, it actually sits and waits for MySQL to be up and ready before trying to connect to it. Docker doesn't have any built-in support to wait for another container to be fully up, running, and ready before starting another container. For Node-based projects, you can use the wait-port dependency. Similar projects exist for other languages/frameworks. At this point, you should be able to open your app and see it running. And hey! We're down to a single command!","title":"Running our Application Stack"},{"location":"tutorial/using-docker-compose/#seeing-our-app-stack-in-docker-dashboard","text":"If we look at the Docker Dashboard, we'll see that there is a group named app . This is the \"project name\" from Docker Compose and used to group the containers together. By default, the project name is simply the name of the directory that the docker-compose.yml was located in. If you twirl down the app, you will see the two containers we defined in the compose file. The names are also a little more descriptive, as they follow the pattern of <project-name>_<service-name>_<replica-number> . So, it's very easy to quickly see what container is our app and which container is the mysql database.","title":"Seeing our App Stack in Docker Dashboard"},{"location":"tutorial/using-docker-compose/#tearing-it-all-down","text":"When you're ready to tear it all down, simply run docker-compose down or hit the trash can on the Docker Dashboard for the entire app. The containers will stop and the network will be removed. !!! warning \"Removing Volumes\" By default, named volumes in your compose file are NOT removed when running docker-compose down . If you want to remove the volumes, you will need to add the --volumes flag. The Docker Dashboard does _not_ remove volumes when you delete the app stack. Once torn down, you can switch to another project, run docker-compose up and be ready to contribute to that project! It really doesn't get much simpler than that!","title":"Tearing it All Down"},{"location":"tutorial/using-docker-compose/#recap","text":"In this section, we learned about Docker Compose and how it helps us dramatically simplify the defining and sharing of multi-service applications. We created a Compose file by translating the commands we were using into the appropriate compose format. At this point, we're starting to wrap up the tutorial. However, there are a few best practices about image building we want to cover, as there is a big issue with the Dockerfile we've been using. So, let's take a look!","title":"Recap"},{"location":"tutorial/what-next/","text":"Although we're done with our workshop, there's still a LOT more to learn about containers! We're not going to go deep-dive here, but here are a few other areas to look at next! Container Orchestration Running containers in production is tough. You don't want to log into a machine and simply run a docker run or docker-compose up . Why not? Well, what happens if the containers die? How do you scale across several machines? Container orchestration solves this problem. Tools like Kubernetes, Swarm, Nomad, and ECS all help solve this problem, all in slightly different ways. The general idea is that you have \"managers\" who receive expected state . This state might be \"I want to run two instances of my web app and expose port 80.\" The managers then look at all of the machines in the cluster and delegate work to \"worker\" nodes. The managers watch for changes (such as a container quitting) and then work to make actual state reflect the expected state. Cloud Native Computing Foundation Projects The CNCF is a vendor-neutral home for various open-source projects, including Kubernetes, Prometheus, Envoy, Linkerd, NATS, and more! You can view the graduated and incubated projects here and the entire CNCF Landscape here . There are a LOT of projects to help solve problems around monitoring, logging, security, image registries, messaging, and more! So, if you're new to the container landscape and cloud-native application development, welcome! Please connect to the community, ask questions, and keep learning! We're excited to have you!","title":"Part10. What next?"},{"location":"tutorial/what-next/#container-orchestration","text":"Running containers in production is tough. You don't want to log into a machine and simply run a docker run or docker-compose up . Why not? Well, what happens if the containers die? How do you scale across several machines? Container orchestration solves this problem. Tools like Kubernetes, Swarm, Nomad, and ECS all help solve this problem, all in slightly different ways. The general idea is that you have \"managers\" who receive expected state . This state might be \"I want to run two instances of my web app and expose port 80.\" The managers then look at all of the machines in the cluster and delegate work to \"worker\" nodes. The managers watch for changes (such as a container quitting) and then work to make actual state reflect the expected state.","title":"Container Orchestration"},{"location":"tutorial/what-next/#cloud-native-computing-foundation-projects","text":"The CNCF is a vendor-neutral home for various open-source projects, including Kubernetes, Prometheus, Envoy, Linkerd, NATS, and more! You can view the graduated and incubated projects here and the entire CNCF Landscape here . There are a LOT of projects to help solve problems around monitoring, logging, security, image registries, messaging, and more! So, if you're new to the container landscape and cloud-native application development, welcome! Please connect to the community, ask questions, and keep learning! We're excited to have you!","title":"Cloud Native Computing Foundation Projects"}]}