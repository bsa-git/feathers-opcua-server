{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is OPC UA? As the next-generation of OPC technology, OPC UA (Unified Architecture) is a major leap forward for secure, reliable and platform independent interoperability. OPC UA is designed for the transport of data and information from first-tier factory and process control devices through to the enterprise information system. The OPC UA specification , first released in 2008, integrates all the functionality from the existing OPC Classic specifications into a single service-oriented architecture. It adds essential new features, such as platform independence, diagnostics, discovery, rendering of complex information models, security, and reliability. Additionally, OPC UA was released as an IEC Standard, IEC 62541 in October 2011. OPC UA provides a single set of services for the OPC data models, such as Data Access , Alarms & Conditions , and Historical Access and can be implemented on non-Microsoft systems, including embedded devices. OPC UA provides services for simple / complex information models of vendors and consortia to be \u201cplugged in\u201d to the address space of OPC UA. This information modeling feature allows generic software applications to browse, read, write and subscribe to a whole new world of information. Unified Architecture The OPC Unified Architecture (UA), released in 2008, is a platform independent service-oriented architecture that integrates all the functionality of the individual OPC Classic specifications into one extensible framework. This multi-layered approach accomplishes the original design specification goals of: * Functional equivalence: all COM OPC Classic specifications are mapped to UA * Platform independence: from an embedded micro-controller to cloud-based infrastructure * Secure: encryption, authentication, and auditing * Extensible: ability to add new features without affecting existing applications * Comprehensive information modeling: for defining complex information Functional Equivalence Building on the success of OPC Classic, OPC UA was designed to enhance and surpass the capabilities of the OPC Classic specifications. OPC UA is functionally equivalent to OPC Classic, yet capable of much more: Discovery: find the availability of OPC Servers on local PCs and/or networks Address space: all data is represented hierarchically (e.g. files and folders) allowing for simple and complex structures to be discovered and utilized by OPC Clients On-demand: read and write data/information based on access-permissions Subscriptions: monitor data/information and report-by-exception when values change based on a client\u2019s criteria Events: notify important information based on client\u2019s criteria Methods: clients can execute programs, etc. based on methods defined on the server Integration between OPC UA products and OPC Classic products is easily accomplished with COM/Proxy wrappers that are available in the download section. Platform Independence Given the wide array of available hardware platforms and operating systems, platform independence is essential. OPC UA functions on any of the following and more: Hardware platforms: traditional PC hardware, cloud-based servers, PLCs, micro-controllers (ARM etc.) Operating Systems: Microsoft Windows, Apple OSX, Android, or any distribution of Linux, etc. OPC UA provides the necessary infrastructure for interoperability across the enterprise, from machine-to-machine, machine-to-enterprise and everything in-between. Security One of the most important considerations in choosing a technology is security. OPC UA is firewall-friendly while addressing security concerns by providing a suite of controls: Transport: numerous protocols are defined providing options such as the ultra-fast OPC-binary transport or the more universally compatible JSON over Websockets, for example Session Encryption: messages are transmitted securely at various encryption levels Message Signing: with message signing the recipient can verify the origin and integrity of received messages Sequenced Packets: exposure to message replay attacks is eliminated with sequencing Authentication: each UA client and server is identified through X509 certificates providing control over which applications and systems are permitted to connect with each other User Control: applications can require users to authenticate (login credentials, certificate, web token etc.) and can further restrict and enhance their capabilities with access rights and address-space \u201cviews\u201d Auditing: activities by user and/or system are logged providing an access audit trail Extensible The multi-layered architecture of OPC UA provides a \u201cfuture proof\u201d framework. Innovative technologies and methodologies such as new transport protocols, security algorithms, encoding standards, or application-services can be incorporated into OPC UA while maintaining backwards compatibility for existing products. UA products built today will work with the products of tomorrow. Information Modeling and Access The OPC UA information modeling framework turns data into information. With complete object-oriented capabilities, even the most complex multi-level structures can be modeled and extended. This framework is THE fundamental element of OPC Unified Architecture. It defines the rules and base building blocks necessary to expose an information model with OPC UA. While OPC UA already defines several core models that can be applied in many industries, other organizations build their models upon them, exposing their more specific information with OPC UA. OPC UA also defines the necessary access mechanisms to information models. Look-up mechanism (browsing) to locate instances and their semantic Read and write operations for current data and historical data Method execution Notification for data and events For Client-Server communication the full range of information model access is available via services and in doing so follows the design paradigm of service-oriented architecture (SOA), with which a service provider receives requests, processes them and sends the results back with the response. Publish-Subscribe (PubSub), provides an alternative mechanism for data and event notification. While in Client-Server communication each notification is for a single client with guaranteed delivery, PubSub has been optimized for many-to-many configurations. With PubSub, OPC UA applications do not directly exchange requests and responses. Instead, Publishers send messages to a Message Oriented Middleware, without knowledge of what, if any, Subscribers there may be. Similarly, Subscribers express interest in specific types of data, and process messages that contain this data, without a need to know where it originated from. Learn how other standards organizations are leveraging OPC UA through collaborations with the OPC Foundation.","title":"What is OPC UA?"},{"location":"#what-is-opc-ua","text":"As the next-generation of OPC technology, OPC UA (Unified Architecture) is a major leap forward for secure, reliable and platform independent interoperability. OPC UA is designed for the transport of data and information from first-tier factory and process control devices through to the enterprise information system. The OPC UA specification , first released in 2008, integrates all the functionality from the existing OPC Classic specifications into a single service-oriented architecture. It adds essential new features, such as platform independence, diagnostics, discovery, rendering of complex information models, security, and reliability. Additionally, OPC UA was released as an IEC Standard, IEC 62541 in October 2011. OPC UA provides a single set of services for the OPC data models, such as Data Access , Alarms & Conditions , and Historical Access and can be implemented on non-Microsoft systems, including embedded devices. OPC UA provides services for simple / complex information models of vendors and consortia to be \u201cplugged in\u201d to the address space of OPC UA. This information modeling feature allows generic software applications to browse, read, write and subscribe to a whole new world of information.","title":"What is OPC UA?"},{"location":"#unified-architecture","text":"The OPC Unified Architecture (UA), released in 2008, is a platform independent service-oriented architecture that integrates all the functionality of the individual OPC Classic specifications into one extensible framework. This multi-layered approach accomplishes the original design specification goals of: * Functional equivalence: all COM OPC Classic specifications are mapped to UA * Platform independence: from an embedded micro-controller to cloud-based infrastructure * Secure: encryption, authentication, and auditing * Extensible: ability to add new features without affecting existing applications * Comprehensive information modeling: for defining complex information","title":"Unified Architecture"},{"location":"#functional-equivalence","text":"Building on the success of OPC Classic, OPC UA was designed to enhance and surpass the capabilities of the OPC Classic specifications. OPC UA is functionally equivalent to OPC Classic, yet capable of much more: Discovery: find the availability of OPC Servers on local PCs and/or networks Address space: all data is represented hierarchically (e.g. files and folders) allowing for simple and complex structures to be discovered and utilized by OPC Clients On-demand: read and write data/information based on access-permissions Subscriptions: monitor data/information and report-by-exception when values change based on a client\u2019s criteria Events: notify important information based on client\u2019s criteria Methods: clients can execute programs, etc. based on methods defined on the server Integration between OPC UA products and OPC Classic products is easily accomplished with COM/Proxy wrappers that are available in the download section.","title":"Functional Equivalence"},{"location":"#platform-independence","text":"Given the wide array of available hardware platforms and operating systems, platform independence is essential. OPC UA functions on any of the following and more: Hardware platforms: traditional PC hardware, cloud-based servers, PLCs, micro-controllers (ARM etc.) Operating Systems: Microsoft Windows, Apple OSX, Android, or any distribution of Linux, etc. OPC UA provides the necessary infrastructure for interoperability across the enterprise, from machine-to-machine, machine-to-enterprise and everything in-between.","title":"Platform Independence"},{"location":"#security","text":"One of the most important considerations in choosing a technology is security. OPC UA is firewall-friendly while addressing security concerns by providing a suite of controls: Transport: numerous protocols are defined providing options such as the ultra-fast OPC-binary transport or the more universally compatible JSON over Websockets, for example Session Encryption: messages are transmitted securely at various encryption levels Message Signing: with message signing the recipient can verify the origin and integrity of received messages Sequenced Packets: exposure to message replay attacks is eliminated with sequencing Authentication: each UA client and server is identified through X509 certificates providing control over which applications and systems are permitted to connect with each other User Control: applications can require users to authenticate (login credentials, certificate, web token etc.) and can further restrict and enhance their capabilities with access rights and address-space \u201cviews\u201d Auditing: activities by user and/or system are logged providing an access audit trail","title":"Security"},{"location":"#extensible","text":"The multi-layered architecture of OPC UA provides a \u201cfuture proof\u201d framework. Innovative technologies and methodologies such as new transport protocols, security algorithms, encoding standards, or application-services can be incorporated into OPC UA while maintaining backwards compatibility for existing products. UA products built today will work with the products of tomorrow.","title":"Extensible"},{"location":"#information-modeling-and-access","text":"The OPC UA information modeling framework turns data into information. With complete object-oriented capabilities, even the most complex multi-level structures can be modeled and extended. This framework is THE fundamental element of OPC Unified Architecture. It defines the rules and base building blocks necessary to expose an information model with OPC UA. While OPC UA already defines several core models that can be applied in many industries, other organizations build their models upon them, exposing their more specific information with OPC UA. OPC UA also defines the necessary access mechanisms to information models. Look-up mechanism (browsing) to locate instances and their semantic Read and write operations for current data and historical data Method execution Notification for data and events For Client-Server communication the full range of information model access is available via services and in doing so follows the design paradigm of service-oriented architecture (SOA), with which a service provider receives requests, processes them and sends the results back with the response. Publish-Subscribe (PubSub), provides an alternative mechanism for data and event notification. While in Client-Server communication each notification is for a single client with guaranteed delivery, PubSub has been optimized for many-to-many configurations. With PubSub, OPC UA applications do not directly exchange requests and responses. Instead, Publishers send messages to a Message Oriented Middleware, without knowledge of what, if any, Subscribers there may be. Similarly, Subscribers express interest in specific types of data, and process messages that contain this data, without a need to know where it originated from. Learn how other standards organizations are leveraging OPC UA through collaborations with the OPC Foundation.","title":"Information Modeling and Access"},{"location":"LICENSE/","text":"MIT License Copyright (c) [2020] [SERGII BESKOROVAINYI] Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"LICENSE/#mit-license","text":"Copyright (c) [2020] [SERGII BESKOROVAINYI] Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"tutorial/image-building-best-practices/","text":"Security Scanning When you have built an image, it is good practice to scan it for security vulnerabilities using the docker scan command. Docker has partnered with Snyk to provide the vulnerability scanning service. For example, to scan the getting-started image you created earlier in the tutorial, you can just type docker scan getting-started The scan uses a constantly updated database of vulnerabilities, so the output you see will vary as new vulnerabilities are discovered, but it might look something like this: \u2717 Low severity vulnerability found in freetype/freetype Description: CVE-2020-15999 Info: https://snyk.io/vuln/SNYK-ALPINE310-FREETYPE-1019641 Introduced through: freetype/freetype@2.10.0-r0, gd/libgd@2.2.5-r2 From: freetype/freetype@2.10.0-r0 From: gd/libgd@2.2.5-r2 > freetype/freetype@2.10.0-r0 Fixed in: 2.10.0-r1 \u2717 Medium severity vulnerability found in libxml2/libxml2 Description: Out-of-bounds Read Info: https://snyk.io/vuln/SNYK-ALPINE310-LIBXML2-674791 Introduced through: libxml2/libxml2@2.9.9-r3, libxslt/libxslt@1.1.33-r3, nginx-module-xslt/nginx-module-xslt@1.17.9-r1 From: libxml2/libxml2@2.9.9-r3 From: libxslt/libxslt@1.1.33-r3 > libxml2/libxml2@2.9.9-r3 From: nginx-module-xslt/nginx-module-xslt@1.17.9-r1 > libxml2/libxml2@2.9.9-r3 Fixed in: 2.9.9-r4 The output lists the type of vulnerability, a URL to learn more, and importantly which version of the relevant library fixes the vulnerability. There are several other options, which you can read about in the docker scan documentation . As well as scanning your newly built image on the command line, you can also configure Docker Hub to scan all newly pushed images automatically, and you can then see the results in both Docker Hub and Docker Desktop. {: style=width:75% } {: .text-center } Image Layering Did you know that you can look at what makes up an image? Using the docker image history command, you can see the command that was used to create each layer within an image. Use the docker image history command to see the layers in the getting-started image you created earlier in the tutorial. bash docker image history getting-started You should get output that looks something like this (dates/IDs may be different). plaintext IMAGE CREATED CREATED BY SIZE COMMENT a78a40cbf866 18 seconds ago /bin/sh -c #(nop) CMD [\"node\" \"src/index.j\u2026 0B f1d1808565d6 19 seconds ago /bin/sh -c yarn install --production 85.4MB a2c054d14948 36 seconds ago /bin/sh -c #(nop) COPY dir:5dc710ad87c789593\u2026 198kB 9577ae713121 37 seconds ago /bin/sh -c #(nop) WORKDIR /app 0B b95baba1cfdb 13 days ago /bin/sh -c #(nop) CMD [\"node\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ENTRYPOINT [\"docker-entry\u2026 0B <missing> 13 days ago /bin/sh -c #(nop) COPY file:238737301d473041\u2026 116B <missing> 13 days ago /bin/sh -c apk add --no-cache --virtual .bui\u2026 5.35MB <missing> 13 days ago /bin/sh -c #(nop) ENV YARN_VERSION=1.21.1 0B <missing> 13 days ago /bin/sh -c addgroup -g 1000 node && addu\u2026 74.3MB <missing> 13 days ago /bin/sh -c #(nop) ENV NODE_VERSION=12.14.1 0B <missing> 13 days ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ADD file:e69d441d729412d24\u2026 5.59MB Each of the lines represents a layer in the image. The display here shows the base at the bottom with the newest layer at the top. Using this, you can also quickly see the size of each layer, helping diagnose large images. You'll notice that several of the lines are truncated. If you add the --no-trunc flag, you'll get the full output (yes... funny how you use a truncated flag to get untruncated output, huh?) bash docker image history --no-trunc getting-started Layer Caching Now that you've seen the layering in action, there's an important lesson to learn to help decrease build times for your container images. Once a layer changes, all downstream layers have to be recreated as well Let's look at the Dockerfile we were using one more time... FROM node:12-alpine WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] Going back to the image history output, we see that each command in the Dockerfile becomes a new layer in the image. You might remember that when we made a change to the image, the yarn dependencies had to be reinstalled. Is there a way to fix this? It doesn't make much sense to ship around the same dependencies every time we build, right? To fix this, we need to restructure our Dockerfile to help support the caching of the dependencies. For Node-based applications, those dependencies are defined in the package.json file. So, what if we copied only that file in first, install the dependencies, and then copy in everything else? Then, we only recreate the yarn dependencies if there was a change to the package.json . Make sense? Update the Dockerfile to copy in the package.json first, install dependencies, and then copy everything else in. dockerfile hl_lines=\"3 4 5\" FROM node:12-alpine WORKDIR /app COPY package.json yarn.lock ./ RUN yarn install --production COPY . . CMD [\"node\", \"src/index.js\"] Create a file named .dockerignore in the same folder as the Dockerfile with the following contents. ignore node_modules .dockerignore files are an easy way to selectively copy only image relevant files. You can read more about this here . In this case, the node_modules folder should be omitted in the second COPY step because otherwise, it would possibly overwrite files which were created by the command in the RUN step. For further details on why this is recommended for Node.js applications and other best practices, have a look at their guide on Dockerizing a Node.js web app . Build a new image using docker build . bash docker build -t getting-started . You should see output like this... plaintext Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Running in d53a06c9e4c2 yarn install v1.17.3 [1/4] Resolving packages... [2/4] Fetching packages... info fsevents@1.2.9: The platform \"linux\" is incompatible with this module. info \"fsevents@1.2.9\" is an optional dependency and failed compatibility check. Excluding it from installation. [3/4] Linking dependencies... [4/4] Building fresh packages... Done in 10.89s. Removing intermediate container d53a06c9e4c2 ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> a239a11f68d8 Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 49999f68df8f Removing intermediate container 49999f68df8f ---> e709c03bc597 Successfully built e709c03bc597 Successfully tagged getting-started:latest You'll see that all layers were rebuilt. Perfectly fine since we changed the Dockerfile quite a bit. Now, make a change to the src/static/index.html file (like change the <title> to say \"The Awesome Todo App\"). Build the Docker image now using docker build -t getting-started . again. This time, your output should look a little different. plaintext hl_lines=\"5 8 11\" Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> Using cache ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Using cache ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> cccde25a3d9a Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 2be75662c150 Removing intermediate container 2be75662c150 ---> 458e5c6f080c Successfully built 458e5c6f080c Successfully tagged getting-started:latest First off, you should notice that the build was MUCH faster! And, you'll see that steps 1-4 all have Using cache . So, hooray! We're using the build cache. Pushing and pulling this image and updates to it will be much faster as well. Hooray! Multi-Stage Builds While we're not going to dive into it too much in this tutorial, multi-stage builds are an incredibly powerful tool to help use multiple stages to create an image. There are several advantages for them: Separate build-time dependencies from runtime dependencies Reduce overall image size by shipping only what your app needs to run Maven/Tomcat Example When building Java-based applications, a JDK is needed to compile the source code to Java bytecode. However, that JDK isn't needed in production. Also, you might be using tools like Maven or Gradle to help build the app. Those also aren't needed in our final image. Multi-stage builds help. FROM maven AS build WORKDIR /app COPY . . RUN mvn package FROM tomcat COPY --from=build /app/target/file.war /usr/local/tomcat/webapps In this example, we use one stage (called build ) to perform the actual Java build using Maven. In the second stage (starting at FROM tomcat ), we copy in files from the build stage. The final image is only the last stage being created (which can be overridden using the --target flag). React Example When building React applications, we need a Node environment to compile the JS code (typically JSX), SASS stylesheets, and more into static HTML, JS, and CSS. If we aren't doing server-side rendering, we don't even need a Node environment for our production build. Why not ship the static resources in a static nginx container? FROM node:12 AS build WORKDIR /app COPY package* yarn.lock ./ RUN yarn install COPY public ./public COPY src ./src RUN yarn run build FROM nginx:alpine COPY --from=build /app/build /usr/share/nginx/html Here, we are using a node:12 image to perform the build (maximizing layer caching) and then copying the output into an nginx container. Cool, huh? Recap By understanding a little bit about how images are structured, we can build images faster and ship fewer changes. Scanning images gives us confidence that the containers we are running and distributing are secure. Multi-stage builds also help us reduce overall image size and increase final container security by separating build-time dependencies from runtime dependencies.","title":"Part9. Image-building best practices"},{"location":"tutorial/image-building-best-practices/#security-scanning","text":"When you have built an image, it is good practice to scan it for security vulnerabilities using the docker scan command. Docker has partnered with Snyk to provide the vulnerability scanning service. For example, to scan the getting-started image you created earlier in the tutorial, you can just type docker scan getting-started The scan uses a constantly updated database of vulnerabilities, so the output you see will vary as new vulnerabilities are discovered, but it might look something like this: \u2717 Low severity vulnerability found in freetype/freetype Description: CVE-2020-15999 Info: https://snyk.io/vuln/SNYK-ALPINE310-FREETYPE-1019641 Introduced through: freetype/freetype@2.10.0-r0, gd/libgd@2.2.5-r2 From: freetype/freetype@2.10.0-r0 From: gd/libgd@2.2.5-r2 > freetype/freetype@2.10.0-r0 Fixed in: 2.10.0-r1 \u2717 Medium severity vulnerability found in libxml2/libxml2 Description: Out-of-bounds Read Info: https://snyk.io/vuln/SNYK-ALPINE310-LIBXML2-674791 Introduced through: libxml2/libxml2@2.9.9-r3, libxslt/libxslt@1.1.33-r3, nginx-module-xslt/nginx-module-xslt@1.17.9-r1 From: libxml2/libxml2@2.9.9-r3 From: libxslt/libxslt@1.1.33-r3 > libxml2/libxml2@2.9.9-r3 From: nginx-module-xslt/nginx-module-xslt@1.17.9-r1 > libxml2/libxml2@2.9.9-r3 Fixed in: 2.9.9-r4 The output lists the type of vulnerability, a URL to learn more, and importantly which version of the relevant library fixes the vulnerability. There are several other options, which you can read about in the docker scan documentation . As well as scanning your newly built image on the command line, you can also configure Docker Hub to scan all newly pushed images automatically, and you can then see the results in both Docker Hub and Docker Desktop. {: style=width:75% } {: .text-center }","title":"Security Scanning"},{"location":"tutorial/image-building-best-practices/#image-layering","text":"Did you know that you can look at what makes up an image? Using the docker image history command, you can see the command that was used to create each layer within an image. Use the docker image history command to see the layers in the getting-started image you created earlier in the tutorial. bash docker image history getting-started You should get output that looks something like this (dates/IDs may be different). plaintext IMAGE CREATED CREATED BY SIZE COMMENT a78a40cbf866 18 seconds ago /bin/sh -c #(nop) CMD [\"node\" \"src/index.j\u2026 0B f1d1808565d6 19 seconds ago /bin/sh -c yarn install --production 85.4MB a2c054d14948 36 seconds ago /bin/sh -c #(nop) COPY dir:5dc710ad87c789593\u2026 198kB 9577ae713121 37 seconds ago /bin/sh -c #(nop) WORKDIR /app 0B b95baba1cfdb 13 days ago /bin/sh -c #(nop) CMD [\"node\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ENTRYPOINT [\"docker-entry\u2026 0B <missing> 13 days ago /bin/sh -c #(nop) COPY file:238737301d473041\u2026 116B <missing> 13 days ago /bin/sh -c apk add --no-cache --virtual .bui\u2026 5.35MB <missing> 13 days ago /bin/sh -c #(nop) ENV YARN_VERSION=1.21.1 0B <missing> 13 days ago /bin/sh -c addgroup -g 1000 node && addu\u2026 74.3MB <missing> 13 days ago /bin/sh -c #(nop) ENV NODE_VERSION=12.14.1 0B <missing> 13 days ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ADD file:e69d441d729412d24\u2026 5.59MB Each of the lines represents a layer in the image. The display here shows the base at the bottom with the newest layer at the top. Using this, you can also quickly see the size of each layer, helping diagnose large images. You'll notice that several of the lines are truncated. If you add the --no-trunc flag, you'll get the full output (yes... funny how you use a truncated flag to get untruncated output, huh?) bash docker image history --no-trunc getting-started","title":"Image Layering"},{"location":"tutorial/image-building-best-practices/#layer-caching","text":"Now that you've seen the layering in action, there's an important lesson to learn to help decrease build times for your container images. Once a layer changes, all downstream layers have to be recreated as well Let's look at the Dockerfile we were using one more time... FROM node:12-alpine WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] Going back to the image history output, we see that each command in the Dockerfile becomes a new layer in the image. You might remember that when we made a change to the image, the yarn dependencies had to be reinstalled. Is there a way to fix this? It doesn't make much sense to ship around the same dependencies every time we build, right? To fix this, we need to restructure our Dockerfile to help support the caching of the dependencies. For Node-based applications, those dependencies are defined in the package.json file. So, what if we copied only that file in first, install the dependencies, and then copy in everything else? Then, we only recreate the yarn dependencies if there was a change to the package.json . Make sense? Update the Dockerfile to copy in the package.json first, install dependencies, and then copy everything else in. dockerfile hl_lines=\"3 4 5\" FROM node:12-alpine WORKDIR /app COPY package.json yarn.lock ./ RUN yarn install --production COPY . . CMD [\"node\", \"src/index.js\"] Create a file named .dockerignore in the same folder as the Dockerfile with the following contents. ignore node_modules .dockerignore files are an easy way to selectively copy only image relevant files. You can read more about this here . In this case, the node_modules folder should be omitted in the second COPY step because otherwise, it would possibly overwrite files which were created by the command in the RUN step. For further details on why this is recommended for Node.js applications and other best practices, have a look at their guide on Dockerizing a Node.js web app . Build a new image using docker build . bash docker build -t getting-started . You should see output like this... plaintext Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Running in d53a06c9e4c2 yarn install v1.17.3 [1/4] Resolving packages... [2/4] Fetching packages... info fsevents@1.2.9: The platform \"linux\" is incompatible with this module. info \"fsevents@1.2.9\" is an optional dependency and failed compatibility check. Excluding it from installation. [3/4] Linking dependencies... [4/4] Building fresh packages... Done in 10.89s. Removing intermediate container d53a06c9e4c2 ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> a239a11f68d8 Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 49999f68df8f Removing intermediate container 49999f68df8f ---> e709c03bc597 Successfully built e709c03bc597 Successfully tagged getting-started:latest You'll see that all layers were rebuilt. Perfectly fine since we changed the Dockerfile quite a bit. Now, make a change to the src/static/index.html file (like change the <title> to say \"The Awesome Todo App\"). Build the Docker image now using docker build -t getting-started . again. This time, your output should look a little different. plaintext hl_lines=\"5 8 11\" Sending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---> b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---> Using cache ---> 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---> Using cache ---> bd5306f49fc8 Step 4/6 : RUN yarn install --production ---> Using cache ---> 4e68fbc2d704 Step 5/6 : COPY . . ---> cccde25a3d9a Step 6/6 : CMD [\"node\", \"src/index.js\"] ---> Running in 2be75662c150 Removing intermediate container 2be75662c150 ---> 458e5c6f080c Successfully built 458e5c6f080c Successfully tagged getting-started:latest First off, you should notice that the build was MUCH faster! And, you'll see that steps 1-4 all have Using cache . So, hooray! We're using the build cache. Pushing and pulling this image and updates to it will be much faster as well. Hooray!","title":"Layer Caching"},{"location":"tutorial/image-building-best-practices/#multi-stage-builds","text":"While we're not going to dive into it too much in this tutorial, multi-stage builds are an incredibly powerful tool to help use multiple stages to create an image. There are several advantages for them: Separate build-time dependencies from runtime dependencies Reduce overall image size by shipping only what your app needs to run","title":"Multi-Stage Builds"},{"location":"tutorial/image-building-best-practices/#maventomcat-example","text":"When building Java-based applications, a JDK is needed to compile the source code to Java bytecode. However, that JDK isn't needed in production. Also, you might be using tools like Maven or Gradle to help build the app. Those also aren't needed in our final image. Multi-stage builds help. FROM maven AS build WORKDIR /app COPY . . RUN mvn package FROM tomcat COPY --from=build /app/target/file.war /usr/local/tomcat/webapps In this example, we use one stage (called build ) to perform the actual Java build using Maven. In the second stage (starting at FROM tomcat ), we copy in files from the build stage. The final image is only the last stage being created (which can be overridden using the --target flag).","title":"Maven/Tomcat Example"},{"location":"tutorial/image-building-best-practices/#react-example","text":"When building React applications, we need a Node environment to compile the JS code (typically JSX), SASS stylesheets, and more into static HTML, JS, and CSS. If we aren't doing server-side rendering, we don't even need a Node environment for our production build. Why not ship the static resources in a static nginx container? FROM node:12 AS build WORKDIR /app COPY package* yarn.lock ./ RUN yarn install COPY public ./public COPY src ./src RUN yarn run build FROM nginx:alpine COPY --from=build /app/build /usr/share/nginx/html Here, we are using a node:12 image to perform the build (maximizing layer caching) and then copying the output into an nginx container. Cool, huh?","title":"React Example"},{"location":"tutorial/image-building-best-practices/#recap","text":"By understanding a little bit about how images are structured, we can build images faster and ship fewer changes. Scanning images gives us confidence that the containers we are running and distributing are secure. Multi-stage builds also help us reduce overall image size and increase final container security by separating build-time dependencies from runtime dependencies.","title":"Recap"},{"location":"tutorial/multi-container-apps/","text":"Up to this point, we have been working with single container apps. But, we now want to add MySQL to the application stack. The following question often arises - \"Where will MySQL run? Install it in the same container or run it separately?\" In general, each container should do one thing and do it well. A few reasons: There's a good chance you'd have to scale APIs and front-ends differently than databases. Separate containers let you version and update versions in isolation. While you may use a container for the database locally, you may want to use a managed service for the database in production. You don't want to ship your database engine with your app then. Running multiple processes will require a process manager (the container only starts one process), which adds complexity to container startup/shutdown. And there are more reasons. So, we will update our application to work like this: {: .text-center } Container Networking Remember that containers, by default, run in isolation and don't know anything about other processes or containers on the same machine. So, how do we allow one container to talk to another? The answer is networking . Now, you don't have to be a network engineer (hooray!). Simply remember this rule... If two containers are on the same network, they can talk to each other. If they aren't, they can't. Starting MySQL There are two ways to put a container on a network: 1) Assign it at start or 2) connect an existing container. For now, we will create the network first and attach the MySQL container at startup. Create the network. bash docker network create todo-app Start a MySQL container and attach it to the network. We're also going to define a few environment variables that the database will use to initialize the database (see the \"Environment Variables\" section in the MySQL Docker Hub listing ). bash docker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command. powershell docker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 You'll also see we specified the --network-alias flag. We'll come back to that in just a moment. !!! info \"Pro-tip\" You'll notice we're using a volume named todo-mysql-data here and mounting it at /var/lib/mysql , which is where MySQL stores its data. However, we never ran a docker volume create command. Docker recognizes we want to use a named volume and creates one automatically for us. !!! info \"Troubleshooting\" If you see a docker: no matching manifest error, it's because you're trying to run the container in a different architecture than amd64, which is the only supported architecture for the mysql image at the moment. To solve this add the flag --platform linux/amd64 in the previous command. So your new command should look like this: bash docker run -d \\ --network todo-app --network-alias mysql --platform linux/amd64 \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 To confirm we have the database up and running, connect to the database and verify it connects. bash docker exec -it <mysql-container-id> mysql -p When the password prompt comes up, type in secret . In the MySQL shell, list the databases and verify you see the todos database. cli mysql> SHOW DATABASES; You should see output that looks like this: plaintext +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | todos | +--------------------+ 5 rows in set (0.00 sec) Hooray! We have our todos database and it's ready for us to use! To exit the sql terminal type exit in the terminal. Connecting to MySQL Now that we know MySQL is up and running, let's use it! But, the question is... how? If we run another container on the same network, how do we find the container (remember each container has its own IP address)? To figure it out, we're going to make use of the nicolaka/netshoot container, which ships with a lot of tools that are useful for troubleshooting or debugging networking issues. Start a new container using the nicolaka/netshoot image. Make sure to connect it to the same network. bash docker run -it --network todo-app nicolaka/netshoot Inside the container, we're going to use the dig command, which is a useful DNS tool. We're going to look up the IP address for the hostname mysql . bash dig mysql And you'll get an output like this... ```text ; <<>> DiG 9.14.1 <<>> mysql ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 32162 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;mysql. IN A ;; ANSWER SECTION: mysql. 600 IN A 172.23.0.2 ;; Query time: 0 msec ;; SERVER: 127.0.0.11#53(127.0.0.11) ;; WHEN: Tue Oct 01 23:47:24 UTC 2019 ;; MSG SIZE rcvd: 44 ``` In the \"ANSWER SECTION\", you will see an A record for mysql that resolves to 172.23.0.2 (your IP address will most likely have a different value). While mysql isn't normally a valid hostname, Docker was able to resolve it to the IP address of the container that had that network alias (remember the --network-alias flag we used earlier?). What this means is... our app only simply needs to connect to a host named mysql and it'll talk to the database! It doesn't get much simpler than that! Running our App with MySQL The todo app supports the setting of a few environment variables to specify MySQL connection settings. They are: MYSQL_HOST - the hostname for the running MySQL server MYSQL_USER - the username to use for the connection MYSQL_PASSWORD - the password to use for the connection MYSQL_DB - the database to use once connected !!! warning Setting Connection Settings via Env Vars While using env vars to set connection settings is generally ok for development, it is HIGHLY DISCOURAGED when running applications in production. Diogo Monica, the former lead of security at Docker, wrote a fantastic blog post explaining why. A more secure mechanism is to use the secret support provided by your container orchestration framework. In most cases, these secrets are mounted as files in the running container. You'll see many apps (including the MySQL image and the todo app) also support env vars with a `_FILE` suffix to point to a file containing the variable. As an example, setting the `MYSQL_PASSWORD_FILE` var will cause the app to use the contents of the referenced file as the connection password. Docker doesn't do anything to support these env vars. Your app will need to know to look for the variable and get the file contents. With all of that explained, let's start our dev-ready container! We'll specify each of the environment variables above, as well as connect the container to our app network. bash hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you updated your docker file in the Bind Mount section of the tutorial use the updated command: bash hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"apk --no-cache --virtual build-dependencies add python2 make g++ && yarn install && yarn run dev\" If you are using PowerShell then use this command. powershell hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" If we look at the logs for the container ( docker logs <container-id> ), we should see a message indicating it's using the mysql database. ```plaintext hl_lines=\"7\" Previous log messages omitted $ nodemon src/index.js [nodemon] 1.19.2 [nodemon] to restart at any time, enter rs [nodemon] watching dir(s): . [nodemon] starting node src/index.js Connected to mysql db at host mysql Listening on port 3000 ``` Open the app in your browser and add a few items to your todo list. Connect to the mysql database and prove that the items are being written to the database. Remember, the password is secret . bash docker exec -it <mysql-container-id> mysql -p todos And in the mysql shell, run the following: plaintext mysql> select * from todo_items; +--------------------------------------+--------------------+-----------+ | id | name | completed | +--------------------------------------+--------------------+-----------+ | c906ff08-60e6-44e6-8f49-ed56a0853e85 | Do amazing things! | 0 | | 2912a79e-8486-4bc3-a4c5-460793a575ab | Be awesome! | 0 | +--------------------------------------+--------------------+-----------+ Obviously, your table will look different because it has your items. But, you should see them stored there! If you take a quick look at the Docker Dashboard, you'll see that we have two app containers running. But, there's no real indication that they are grouped together in a single app. We'll see how to make that better shortly! Recap At this point, we have an application that now stores its data in an external database running in a separate container. We learned a little bit about container networking and saw how service discovery can be performed using DNS. But, there's a good chance you are starting to feel a little overwhelmed with everything you need to do to start up this application. We have to create a network, start containers, specify all of the environment variables, expose ports, and more! That's a lot to remember and it's certainly making things harder to pass along to someone else. In the next section, we'll talk about Docker Compose. With Docker Compose, we can share our application stacks in a much easier way and let others spin them up with a single (and simple) command!","title":"Part7. Multi-container apps"},{"location":"tutorial/multi-container-apps/#container-networking","text":"Remember that containers, by default, run in isolation and don't know anything about other processes or containers on the same machine. So, how do we allow one container to talk to another? The answer is networking . Now, you don't have to be a network engineer (hooray!). Simply remember this rule... If two containers are on the same network, they can talk to each other. If they aren't, they can't.","title":"Container Networking"},{"location":"tutorial/multi-container-apps/#starting-mysql","text":"There are two ways to put a container on a network: 1) Assign it at start or 2) connect an existing container. For now, we will create the network first and attach the MySQL container at startup. Create the network. bash docker network create todo-app Start a MySQL container and attach it to the network. We're also going to define a few environment variables that the database will use to initialize the database (see the \"Environment Variables\" section in the MySQL Docker Hub listing ). bash docker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command. powershell docker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 You'll also see we specified the --network-alias flag. We'll come back to that in just a moment. !!! info \"Pro-tip\" You'll notice we're using a volume named todo-mysql-data here and mounting it at /var/lib/mysql , which is where MySQL stores its data. However, we never ran a docker volume create command. Docker recognizes we want to use a named volume and creates one automatically for us. !!! info \"Troubleshooting\" If you see a docker: no matching manifest error, it's because you're trying to run the container in a different architecture than amd64, which is the only supported architecture for the mysql image at the moment. To solve this add the flag --platform linux/amd64 in the previous command. So your new command should look like this: bash docker run -d \\ --network todo-app --network-alias mysql --platform linux/amd64 \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 To confirm we have the database up and running, connect to the database and verify it connects. bash docker exec -it <mysql-container-id> mysql -p When the password prompt comes up, type in secret . In the MySQL shell, list the databases and verify you see the todos database. cli mysql> SHOW DATABASES; You should see output that looks like this: plaintext +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | todos | +--------------------+ 5 rows in set (0.00 sec) Hooray! We have our todos database and it's ready for us to use! To exit the sql terminal type exit in the terminal.","title":"Starting MySQL"},{"location":"tutorial/multi-container-apps/#connecting-to-mysql","text":"Now that we know MySQL is up and running, let's use it! But, the question is... how? If we run another container on the same network, how do we find the container (remember each container has its own IP address)? To figure it out, we're going to make use of the nicolaka/netshoot container, which ships with a lot of tools that are useful for troubleshooting or debugging networking issues. Start a new container using the nicolaka/netshoot image. Make sure to connect it to the same network. bash docker run -it --network todo-app nicolaka/netshoot Inside the container, we're going to use the dig command, which is a useful DNS tool. We're going to look up the IP address for the hostname mysql . bash dig mysql And you'll get an output like this... ```text ; <<>> DiG 9.14.1 <<>> mysql ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 32162 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;mysql. IN A ;; ANSWER SECTION: mysql. 600 IN A 172.23.0.2 ;; Query time: 0 msec ;; SERVER: 127.0.0.11#53(127.0.0.11) ;; WHEN: Tue Oct 01 23:47:24 UTC 2019 ;; MSG SIZE rcvd: 44 ``` In the \"ANSWER SECTION\", you will see an A record for mysql that resolves to 172.23.0.2 (your IP address will most likely have a different value). While mysql isn't normally a valid hostname, Docker was able to resolve it to the IP address of the container that had that network alias (remember the --network-alias flag we used earlier?). What this means is... our app only simply needs to connect to a host named mysql and it'll talk to the database! It doesn't get much simpler than that!","title":"Connecting to MySQL"},{"location":"tutorial/multi-container-apps/#running-our-app-with-mysql","text":"The todo app supports the setting of a few environment variables to specify MySQL connection settings. They are: MYSQL_HOST - the hostname for the running MySQL server MYSQL_USER - the username to use for the connection MYSQL_PASSWORD - the password to use for the connection MYSQL_DB - the database to use once connected !!! warning Setting Connection Settings via Env Vars While using env vars to set connection settings is generally ok for development, it is HIGHLY DISCOURAGED when running applications in production. Diogo Monica, the former lead of security at Docker, wrote a fantastic blog post explaining why. A more secure mechanism is to use the secret support provided by your container orchestration framework. In most cases, these secrets are mounted as files in the running container. You'll see many apps (including the MySQL image and the todo app) also support env vars with a `_FILE` suffix to point to a file containing the variable. As an example, setting the `MYSQL_PASSWORD_FILE` var will cause the app to use the contents of the referenced file as the connection password. Docker doesn't do anything to support these env vars. Your app will need to know to look for the variable and get the file contents. With all of that explained, let's start our dev-ready container! We'll specify each of the environment variables above, as well as connect the container to our app network. bash hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you updated your docker file in the Bind Mount section of the tutorial use the updated command: bash hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"apk --no-cache --virtual build-dependencies add python2 make g++ && yarn install && yarn run dev\" If you are using PowerShell then use this command. powershell hl_lines=\"3 4 5 6 7\" docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" If we look at the logs for the container ( docker logs <container-id> ), we should see a message indicating it's using the mysql database. ```plaintext hl_lines=\"7\"","title":"Running our App with MySQL"},{"location":"tutorial/multi-container-apps/#previous-log-messages-omitted","text":"$ nodemon src/index.js [nodemon] 1.19.2 [nodemon] to restart at any time, enter rs [nodemon] watching dir(s): . [nodemon] starting node src/index.js Connected to mysql db at host mysql Listening on port 3000 ``` Open the app in your browser and add a few items to your todo list. Connect to the mysql database and prove that the items are being written to the database. Remember, the password is secret . bash docker exec -it <mysql-container-id> mysql -p todos And in the mysql shell, run the following: plaintext mysql> select * from todo_items; +--------------------------------------+--------------------+-----------+ | id | name | completed | +--------------------------------------+--------------------+-----------+ | c906ff08-60e6-44e6-8f49-ed56a0853e85 | Do amazing things! | 0 | | 2912a79e-8486-4bc3-a4c5-460793a575ab | Be awesome! | 0 | +--------------------------------------+--------------------+-----------+ Obviously, your table will look different because it has your items. But, you should see them stored there! If you take a quick look at the Docker Dashboard, you'll see that we have two app containers running. But, there's no real indication that they are grouped together in a single app. We'll see how to make that better shortly!","title":"Previous log messages omitted"},{"location":"tutorial/multi-container-apps/#recap","text":"At this point, we have an application that now stores its data in an external database running in a separate container. We learned a little bit about container networking and saw how service discovery can be performed using DNS. But, there's a good chance you are starting to feel a little overwhelmed with everything you need to do to start up this application. We have to create a network, start containers, specify all of the environment variables, expose ports, and more! That's a lot to remember and it's certainly making things harder to pass along to someone else. In the next section, we'll talk about Docker Compose. With Docker Compose, we can share our application stacks in a much easier way and let others spin them up with a single (and simple) command!","title":"Recap"},{"location":"tutorial/part1_getting-started/","text":"Introduction. The application FEATHERS-OPCUA-SERVER is designed to implement the use of the library NodeOPCUA . NodeOPCUA is a OPC UA stack fully written in TypeScript for NodeJS . Basic properties: NodeOPCUA takes advantage of the asynchronous nature of node.js, creating highly responsive applications. NodeOPCUA has been developed using TDD and benefits from more than 2500 unit tests and 90% code coverage. NodeOPCUA can be use in Javascript as well as in Typescript. NodeOPCUA is free for commercial use. Check out the license . NodeOPCUA is available on GitHub . Check out the source code . NodeOPCUA runs on all the platforms that nodeJS supports. NodeOPCUA will benefit from a comprehensive SDK API documentation , numerous end-to-end functional tests, and a set of practical examples to help you learn how to use it. Why do you need a framework. There was an idea to embed the implementation of the protocol NodeOPCUA in the framework in order to use the features of this framework, such as: Working with databases. Access to data through protocols REST API and Websockets . Users authentication mechanisms. Create a client application and display OPC UA data in the client application. Why the framework FeathersJS was chosen. FeathersJS is a set of tools and an architecture template that makes it easy to build scalable real-time REST APIs. Comparison with other frameworks can be found here . The main idea of the FeathersJS framework is to move away from the MVC concept and use the concept of working with services and hooks . This made it possible to create a structure that can grow with you as your product grows. It's flexible enough to quickly adapt to changing business needs, powerful enough to build modern applications, and simple enough to build and run quickly. Here is an overview of the API documentation fit together:","title":"Part1. Getting started"},{"location":"tutorial/part1_getting-started/#introduction","text":"The application FEATHERS-OPCUA-SERVER is designed to implement the use of the library NodeOPCUA . NodeOPCUA is a OPC UA stack fully written in TypeScript for NodeJS . Basic properties: NodeOPCUA takes advantage of the asynchronous nature of node.js, creating highly responsive applications. NodeOPCUA has been developed using TDD and benefits from more than 2500 unit tests and 90% code coverage. NodeOPCUA can be use in Javascript as well as in Typescript. NodeOPCUA is free for commercial use. Check out the license . NodeOPCUA is available on GitHub . Check out the source code . NodeOPCUA runs on all the platforms that nodeJS supports. NodeOPCUA will benefit from a comprehensive SDK API documentation , numerous end-to-end functional tests, and a set of practical examples to help you learn how to use it.","title":"Introduction."},{"location":"tutorial/part1_getting-started/#why-do-you-need-a-framework","text":"There was an idea to embed the implementation of the protocol NodeOPCUA in the framework in order to use the features of this framework, such as: Working with databases. Access to data through protocols REST API and Websockets . Users authentication mechanisms. Create a client application and display OPC UA data in the client application.","title":"Why do you need a framework."},{"location":"tutorial/part1_getting-started/#why-the-framework-feathersjs-was-chosen","text":"FeathersJS is a set of tools and an architecture template that makes it easy to build scalable real-time REST APIs. Comparison with other frameworks can be found here . The main idea of the FeathersJS framework is to move away from the MVC concept and use the concept of working with services and hooks . This made it possible to create a structure that can grow with you as your product grows. It's flexible enough to quickly adapt to changing business needs, powerful enough to build modern applications, and simple enough to build and run quickly. Here is an overview of the API documentation fit together:","title":"Why the framework FeathersJS was chosen."},{"location":"tutorial/part2_framework-overview/","text":"Introducing FeathersJS FeathersJS a flexible, real-time JavaScript framework built on top of Express for the server and as a standalone client for the browser and React Native. FeathersJS isn\u2019t just another Rails clone. Instead of the typical MVC pattern it encourages a Service Oriented Architecture paired with Cross-cutting Concerns allowing you to build complex real-time apps and scalable REST APIs very quickly and with very little code. Sounds too good to be true? Once you try it you\u2019ll see that you can build prototypes in minutes and flexible, scalable, production-ready apps in days. Not weeks or months. Modern, solid, and 100% JavaScript Feathers is built using promises and ES6 so you can build your apps with the latest JavaScript features and write terse, elegant code. Feathers itself is only a few hundred lines of code and is a fully compatible wrapper over top of Express , Socket.io and Primus , all of which have been used in production by thousands of companies. Universal Feathers can be used in the browser, React Native and server side and provides everything you need to structure your application and communicate with a Feathers server while still letting you pick your favourite view engine. Using the Feathers client you can quickly add authentication, share validation and business logic code between your server and client, and easily make your apps real-time. Framework Friendly Feathers is completely client agnostic and easily integrates with any client side framework. It plays especially well with React, Angular and React Native. They\u2019re practically BFFs. We have guides for some of the most popular JS frameworks and are adding new ones every week. Service Oriented Services are the core of Feathers. They provide instant CRUD functionality for a resource through a series of familiar methods; find, get, create, update, patch, and remove. Almost any resource can be mapped to these actions; external APIs, database resources, file uploads, you name it. This consistent interface makes it easy to \u201chook\u201d into these CRUD actions to provide custom functionality. For example, if you have a socket transport like Socket.io enabled, Feathers will automatically emit created, updated, patched, and removed events for you. Feathers gives you the structure to build service oriented apps from day one by keeping services discrete. If you eventually need to split up your app into microservices it\u2019s an easy transition and your Feathers apps can scale painlessly. Instant Real-time REST APIs Since Feathers provides instant CRUD functionality via Services , it also exposes both a RESTful and real-time API automatically through HTTP/HTTPS and over websockets. Feathers allows you to send and receive data over sockets similar to Meteor\u2019s DDP so you can use Primus or Socket.io for your sole app communication\u2026 or not. Feathers gives you the flexibility to choose how you want to expose your REST API ; over HTTP(S) , websockets or both \u2014 and it does this with just a few lines of code. Datastore Agnostic Feathers has adapters for 15+ data sources and 4 different ORMs out of the box. More than any other real-time framework! This gives you the ability to access data in MongoDB , Postgres , MySQL , Sequel Server , S3 and more! You can have multiple datastores in a single app and swap them out painlessly due to our consistent query interface . Incredibly Pluggable We like to consider Feathers as a \u201cbatteries included but easily swappable framework\u201d. We have entirely optional plugins that provide authentication , SMS , or email messaging out of the box. You can include exactly what you need, typically in just a couple lines of code. No more, no less.","title":"Part2. FeathersJS framework overview"},{"location":"tutorial/part2_framework-overview/#introducing-feathersjs","text":"FeathersJS a flexible, real-time JavaScript framework built on top of Express for the server and as a standalone client for the browser and React Native. FeathersJS isn\u2019t just another Rails clone. Instead of the typical MVC pattern it encourages a Service Oriented Architecture paired with Cross-cutting Concerns allowing you to build complex real-time apps and scalable REST APIs very quickly and with very little code. Sounds too good to be true? Once you try it you\u2019ll see that you can build prototypes in minutes and flexible, scalable, production-ready apps in days. Not weeks or months.","title":"Introducing FeathersJS"},{"location":"tutorial/part2_framework-overview/#modern-solid-and-100-javascript","text":"Feathers is built using promises and ES6 so you can build your apps with the latest JavaScript features and write terse, elegant code. Feathers itself is only a few hundred lines of code and is a fully compatible wrapper over top of Express , Socket.io and Primus , all of which have been used in production by thousands of companies.","title":"Modern, solid, and 100% JavaScript"},{"location":"tutorial/part2_framework-overview/#universal","text":"Feathers can be used in the browser, React Native and server side and provides everything you need to structure your application and communicate with a Feathers server while still letting you pick your favourite view engine. Using the Feathers client you can quickly add authentication, share validation and business logic code between your server and client, and easily make your apps real-time.","title":"Universal"},{"location":"tutorial/part2_framework-overview/#framework-friendly","text":"Feathers is completely client agnostic and easily integrates with any client side framework. It plays especially well with React, Angular and React Native. They\u2019re practically BFFs. We have guides for some of the most popular JS frameworks and are adding new ones every week.","title":"Framework Friendly"},{"location":"tutorial/part2_framework-overview/#service-oriented","text":"Services are the core of Feathers. They provide instant CRUD functionality for a resource through a series of familiar methods; find, get, create, update, patch, and remove. Almost any resource can be mapped to these actions; external APIs, database resources, file uploads, you name it. This consistent interface makes it easy to \u201chook\u201d into these CRUD actions to provide custom functionality. For example, if you have a socket transport like Socket.io enabled, Feathers will automatically emit created, updated, patched, and removed events for you. Feathers gives you the structure to build service oriented apps from day one by keeping services discrete. If you eventually need to split up your app into microservices it\u2019s an easy transition and your Feathers apps can scale painlessly.","title":"Service Oriented"},{"location":"tutorial/part2_framework-overview/#instant-real-time-rest-apis","text":"Since Feathers provides instant CRUD functionality via Services , it also exposes both a RESTful and real-time API automatically through HTTP/HTTPS and over websockets. Feathers allows you to send and receive data over sockets similar to Meteor\u2019s DDP so you can use Primus or Socket.io for your sole app communication\u2026 or not. Feathers gives you the flexibility to choose how you want to expose your REST API ; over HTTP(S) , websockets or both \u2014 and it does this with just a few lines of code.","title":"Instant Real-time REST APIs"},{"location":"tutorial/part2_framework-overview/#datastore-agnostic","text":"Feathers has adapters for 15+ data sources and 4 different ORMs out of the box. More than any other real-time framework! This gives you the ability to access data in MongoDB , Postgres , MySQL , Sequel Server , S3 and more! You can have multiple datastores in a single app and swap them out painlessly due to our consistent query interface .","title":"Datastore Agnostic"},{"location":"tutorial/part2_framework-overview/#incredibly-pluggable","text":"We like to consider Feathers as a \u201cbatteries included but easily swappable framework\u201d. We have entirely optional plugins that provide authentication , SMS , or email messaging out of the box. You can include exactly what you need, typically in just a couple lines of code. No more, no less.","title":"Incredibly Pluggable"},{"location":"tutorial/part3_app-features/","text":"Application Features Working with services on the server side and on the client side . Using Hooks when working with services. Creation of Real-time APIs . NodeOPCUA library functions are implemented as two classes OpcuaServer and OpcuaClient . An instances of the class OpcuaServer is placed in the service opcua-servers as a list item. An instances of the class OpcuaClient is placed in the service opcua-clients as a list item. OPC UA tags are stored in the database MongoDB or neDB . Operations with tags occur through the service opcua-tags . OPC UA values are stored in the database MongoDB or neDB . Operations with values occur through the service opcua-values . User registration / logging procedure is provided. Authentication process is based on Express Password strategies. JWT Authentication uses JSON Web Token . Local Authentication is used by Email and Password . OAuth 2.0 Authentication via Google , GitHub . The authorization process is based on feathers-castle strategies. Provides Email validation when registering a user. Provides validation when Email is changed by the user. Provides a procedure for recovering a password forgotten by the user. Provides user password change procedure. The administrator can manage users: assign roles, divide by groups, change user activity, delete a user. Working with the database: MongoDB or neDB through services. Site event logging procedure provided. Created tests for the server side.","title":"Part3. Application Features"},{"location":"tutorial/part3_app-features/#application-features","text":"Working with services on the server side and on the client side . Using Hooks when working with services. Creation of Real-time APIs . NodeOPCUA library functions are implemented as two classes OpcuaServer and OpcuaClient . An instances of the class OpcuaServer is placed in the service opcua-servers as a list item. An instances of the class OpcuaClient is placed in the service opcua-clients as a list item. OPC UA tags are stored in the database MongoDB or neDB . Operations with tags occur through the service opcua-tags . OPC UA values are stored in the database MongoDB or neDB . Operations with values occur through the service opcua-values . User registration / logging procedure is provided. Authentication process is based on Express Password strategies. JWT Authentication uses JSON Web Token . Local Authentication is used by Email and Password . OAuth 2.0 Authentication via Google , GitHub . The authorization process is based on feathers-castle strategies. Provides Email validation when registering a user. Provides validation when Email is changed by the user. Provides a procedure for recovering a password forgotten by the user. Provides user password change procedure. The administrator can manage users: assign roles, divide by groups, change user activity, delete a user. Working with the database: MongoDB or neDB through services. Site event logging procedure provided. Created tests for the server side.","title":"Application Features"},{"location":"tutorial/persisting-our-data/","text":"In case you didn't notice, our todo list is being wiped clean every single time we launch the container. Why is this? Let's dive into how the container is working. The Container's Filesystem When a container runs, it uses the various layers from an image for its filesystem. Each container also gets its own \"scratch space\" to create/update/remove files. Any changes won't be seen in another container, even if they are using the same image. Seeing this in Practice To see this in action, we're going to start two containers and create a file in each. What you'll see is that the files created in one container aren't available in another. Start a ubuntu container that will create a file named /data.txt with a random number between 1 and 10000. bash docker run -d ubuntu bash -c \"shuf -i 1-10000 -n 1 -o /data.txt && tail -f /dev/null\" In case you're curious about the command, we're starting a bash shell and invoking two commands (why we have the && ). The first portion picks a single random number and writes it to /data.txt . The second command is simply watching a file to keep the container running. Validate we can see the output by exec 'ing into the container. To do so, open the Dashboard and click the first action of the container that is running the ubuntu image. {: style=width:75% } {: .text-center } You will see a terminal that is running a shell in the ubuntu container. Run the following command to see the content of the /data.txt file. Close this terminal afterwards again. bash cat /data.txt If you prefer the command line you can use the docker exec command to do the same. You need to get the container's ID (use docker ps to get it) and get the content with the following command. bash docker exec <container-id> cat /data.txt You should see a random number! Now, let's start another ubuntu container (the same image) and we'll see we don't have the same file. bash docker run -it ubuntu ls / And look! There's no data.txt file there! That's because it was written to the scratch space for only the first container. Go ahead and remove the first container using the docker rm -f <container-id> command. bash docker rm -f <container-id> Container Volumes With the previous experiment, we saw that each container starts from the image definition each time it starts. While containers can create, update, and delete files, those changes are lost when the container is removed and all changes are isolated to that container. With volumes, we can change all of this. Volumes provide the ability to connect specific filesystem paths of the container back to the host machine. If a directory in the container is mounted, changes in that directory are also seen on the host machine. If we mount that same directory across container restarts, we'd see the same files. There are two main types of volumes. We will eventually use both, but we will start with named volumes . Persisting our Todo Data By default, the todo app stores its data in a SQLite Database at /etc/todos/todo.db . If you're not familiar with SQLite, no worries! It's simply a relational database in which all of the data is stored in a single file. While this isn't the best for large-scale applications, it works for small demos. We'll talk about switching this to a different database engine later. With the database being a single file, if we can persist that file on the host and make it available to the next container, it should be able to pick up where the last one left off. By creating a volume and attaching (often called \"mounting\") it to the directory the data is stored in, we can persist the data. As our container writes to the todo.db file, it will be persisted to the host in the volume. As mentioned, we are going to use a named volume . Think of a named volume as simply a bucket of data. Docker maintains the physical location on the disk and you only need to remember the name of the volume. Every time you use the volume, Docker will make sure the correct data is provided. Create a volume by using the docker volume create command. bash docker volume create todo-db Stop the todo app container once again in the Dashboard (or with docker rm -f <container-id> ), as it is still running without using the persistent volume. Start the todo app container, but add the -v flag to specify a volume mount. We will use the named volume and mount it to /etc/todos , which will capture all files created at the path. bash docker run -dp 3000:3000 -v todo-db:/etc/todos getting-started Once the container starts up, open the app and add a few items to your todo list. {: style=\"width: 55%; \" } {: .text-center } Remove the container for the todo app. Use the Dashboard or docker ps to get the ID and then docker rm -f <container-id> to remove it. Start a new container using the same command from above. Open the app. You should see your items still in your list! Go ahead and remove the container when you're done checking out your list. Hooray! You've now learned how to persist data! !!! info \"Pro-tip\" While named volumes and bind mounts (which we'll talk about in a minute) are the two main types of volumes supported by a default Docker engine installation, there are many volume driver plugins available to support NFS, SFTP, NetApp, and more! This will be especially important once you start running containers on multiple hosts in a clustered environment with Swarm, Kubernetes, etc. Diving into our Volume A lot of people frequently ask \"Where is Docker actually storing my data when I use a named volume?\" If you want to know, you can use the docker volume inspect command. docker volume inspect todo-db [ { \"CreatedAt\": \"2019-09-26T02:18:36Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/todo-db/_data\", \"Name\": \"todo-db\", \"Options\": {}, \"Scope\": \"local\" } ] The Mountpoint is the actual location on the disk where the data is stored. Note that on most machines, you will need to have root access to access this directory from the host. But, that's where it is! !!! info \"Accessing Volume data directly on Docker Desktop\" While running in Docker Desktop, the Docker commands are actually running inside a small VM on your machine. If you wanted to look at the actual contents of the Mountpoint directory, you would need to first get inside of the VM. Recap At this point, we have a functioning application that can survive restarts! We can show it off to our investors and hope they can catch our vision! However, we saw earlier that rebuilding images for every change takes quite a bit of time. There's got to be a better way to make changes, right? With bind mounts (which we hinted at earlier), there is a better way! Let's take a look at that now!","title":"Part5. Persist the DB"},{"location":"tutorial/persisting-our-data/#the-containers-filesystem","text":"When a container runs, it uses the various layers from an image for its filesystem. Each container also gets its own \"scratch space\" to create/update/remove files. Any changes won't be seen in another container, even if they are using the same image.","title":"The Container's Filesystem"},{"location":"tutorial/persisting-our-data/#seeing-this-in-practice","text":"To see this in action, we're going to start two containers and create a file in each. What you'll see is that the files created in one container aren't available in another. Start a ubuntu container that will create a file named /data.txt with a random number between 1 and 10000. bash docker run -d ubuntu bash -c \"shuf -i 1-10000 -n 1 -o /data.txt && tail -f /dev/null\" In case you're curious about the command, we're starting a bash shell and invoking two commands (why we have the && ). The first portion picks a single random number and writes it to /data.txt . The second command is simply watching a file to keep the container running. Validate we can see the output by exec 'ing into the container. To do so, open the Dashboard and click the first action of the container that is running the ubuntu image. {: style=width:75% } {: .text-center } You will see a terminal that is running a shell in the ubuntu container. Run the following command to see the content of the /data.txt file. Close this terminal afterwards again. bash cat /data.txt If you prefer the command line you can use the docker exec command to do the same. You need to get the container's ID (use docker ps to get it) and get the content with the following command. bash docker exec <container-id> cat /data.txt You should see a random number! Now, let's start another ubuntu container (the same image) and we'll see we don't have the same file. bash docker run -it ubuntu ls / And look! There's no data.txt file there! That's because it was written to the scratch space for only the first container. Go ahead and remove the first container using the docker rm -f <container-id> command. bash docker rm -f <container-id>","title":"Seeing this in Practice"},{"location":"tutorial/persisting-our-data/#container-volumes","text":"With the previous experiment, we saw that each container starts from the image definition each time it starts. While containers can create, update, and delete files, those changes are lost when the container is removed and all changes are isolated to that container. With volumes, we can change all of this. Volumes provide the ability to connect specific filesystem paths of the container back to the host machine. If a directory in the container is mounted, changes in that directory are also seen on the host machine. If we mount that same directory across container restarts, we'd see the same files. There are two main types of volumes. We will eventually use both, but we will start with named volumes .","title":"Container Volumes"},{"location":"tutorial/persisting-our-data/#persisting-our-todo-data","text":"By default, the todo app stores its data in a SQLite Database at /etc/todos/todo.db . If you're not familiar with SQLite, no worries! It's simply a relational database in which all of the data is stored in a single file. While this isn't the best for large-scale applications, it works for small demos. We'll talk about switching this to a different database engine later. With the database being a single file, if we can persist that file on the host and make it available to the next container, it should be able to pick up where the last one left off. By creating a volume and attaching (often called \"mounting\") it to the directory the data is stored in, we can persist the data. As our container writes to the todo.db file, it will be persisted to the host in the volume. As mentioned, we are going to use a named volume . Think of a named volume as simply a bucket of data. Docker maintains the physical location on the disk and you only need to remember the name of the volume. Every time you use the volume, Docker will make sure the correct data is provided. Create a volume by using the docker volume create command. bash docker volume create todo-db Stop the todo app container once again in the Dashboard (or with docker rm -f <container-id> ), as it is still running without using the persistent volume. Start the todo app container, but add the -v flag to specify a volume mount. We will use the named volume and mount it to /etc/todos , which will capture all files created at the path. bash docker run -dp 3000:3000 -v todo-db:/etc/todos getting-started Once the container starts up, open the app and add a few items to your todo list. {: style=\"width: 55%; \" } {: .text-center } Remove the container for the todo app. Use the Dashboard or docker ps to get the ID and then docker rm -f <container-id> to remove it. Start a new container using the same command from above. Open the app. You should see your items still in your list! Go ahead and remove the container when you're done checking out your list. Hooray! You've now learned how to persist data! !!! info \"Pro-tip\" While named volumes and bind mounts (which we'll talk about in a minute) are the two main types of volumes supported by a default Docker engine installation, there are many volume driver plugins available to support NFS, SFTP, NetApp, and more! This will be especially important once you start running containers on multiple hosts in a clustered environment with Swarm, Kubernetes, etc.","title":"Persisting our Todo Data"},{"location":"tutorial/persisting-our-data/#diving-into-our-volume","text":"A lot of people frequently ask \"Where is Docker actually storing my data when I use a named volume?\" If you want to know, you can use the docker volume inspect command. docker volume inspect todo-db [ { \"CreatedAt\": \"2019-09-26T02:18:36Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/todo-db/_data\", \"Name\": \"todo-db\", \"Options\": {}, \"Scope\": \"local\" } ] The Mountpoint is the actual location on the disk where the data is stored. Note that on most machines, you will need to have root access to access this directory from the host. But, that's where it is! !!! info \"Accessing Volume data directly on Docker Desktop\" While running in Docker Desktop, the Docker commands are actually running inside a small VM on your machine. If you wanted to look at the actual contents of the Mountpoint directory, you would need to first get inside of the VM.","title":"Diving into our Volume"},{"location":"tutorial/persisting-our-data/#recap","text":"At this point, we have a functioning application that can survive restarts! We can show it off to our investors and hope they can catch our vision! However, we saw earlier that rebuilding images for every change takes quite a bit of time. There's got to be a better way to make changes, right? With bind mounts (which we hinted at earlier), there is a better way! Let's take a look at that now!","title":"Recap"},{"location":"tutorial/sharing-our-app/","text":"Share the application Now that we've built an image, let's share it! To share Docker images, you have to use a Docker registry. The default registry is Docker Hub and is where all of the images we've used have come from. Docker ID A Docker ID allows you to access Docker Hub which is the world\u2019s largest library and community for container images. Create a Docker ID for free if you don\u2019t have one. Create a Repo To push an image, we first need to create a repo on Docker Hub . Sign up or Sign in to Docker Hub . Click the Create Repository button. For the repo name, use node-todo . Make sure the Visibility is Public . Click the Create button! Private repositories Did you know that Docker offers private repositories which allows you to restrict content to specific users or teams? Check out the details on the Docker pricing page. If you look on the right-side of the page, you'll see a section named Docker commands . This gives an example command that you will need to run to push to this repo. Pushing our Image 1.In the command line, try running the push command you see on Docker Hub. Note that your command will be using your namespace -> YOUR-USER-NAME . $ docker push YOUR-USER-NAME/node-todo The push refers to repository [docker.io/YOUR-USER-NAME/node-todo] An image does not exist locally with the tag: YOUR-USER-NAME/node-todo Why did it fail? The push command was looking for an image named YOUR-USER-NAME/node-todo, but didn't find one. If you run docker image ls , you won't see one either. To fix this, we need to \"tag\" our existing image we've built to give it another name. 2.Login to the Docker Hub using the command docker login -u YOUR-USER-NAME . 3.Use the docker tag command to give the node-todo image a new name. Be sure to swap out YOUR-USER-NAME with your Docker ID. docker tag node-todo YOUR-USER-NAME/node-todo 4.Now try your push command again. If you're copying the value from Docker Hub, you can drop the tagname portion, as we didn't add a tag to the image name. If you don't specify a tag, Docker will use a tag called latest . docker push YOUR-USER-NAME/node-todo Running our Image on a New Instance Now that our image has been built and pushed into a registry, let's try running our app on a brand new instance that has never seen this container image! To do this, we will use Play with Docker. Open your browser to Play with Docker . Log in with your Docker Hub account. Once you're logged in, click on the \"+ ADD NEW INSTANCE\" link in the left side bar. (If you don't see it, make your browser a little wider.) After a few seconds, a terminal window will be opened in your browser. 1.In the terminal, start your freshly pushed app. docker run -dp 3000:3000 YOUR-USER-NAME/node-todo You should see the image get pulled down and eventually start up! 2.Click on the 3000 badge when it comes up and you should see the app with your modifications! Hooray! If the 3000 badge doesn't show up, you can click on the \"Open Port\" button and type in 3000 . Recap In this section, we learned how to share our images by pushing them to a registry. We then went to a brand new instance and were able to run the freshly pushed image. This is quite common in CI pipelines, where the pipeline will create the image and push it to a registry and then the production environment can use the latest version of the image. Now that we have that figured out, let's circle back around to what we noticed at the end of the last section. As a reminder, we noticed that when we restarted the app, we lost all of our todo list items. That's obviously not a great user experience, so let's learn how we can persist the data across restarts!","title":"Part4. Share the application"},{"location":"tutorial/sharing-our-app/#share-the-application","text":"Now that we've built an image, let's share it! To share Docker images, you have to use a Docker registry. The default registry is Docker Hub and is where all of the images we've used have come from. Docker ID A Docker ID allows you to access Docker Hub which is the world\u2019s largest library and community for container images. Create a Docker ID for free if you don\u2019t have one.","title":"Share the application"},{"location":"tutorial/sharing-our-app/#create-a-repo","text":"To push an image, we first need to create a repo on Docker Hub . Sign up or Sign in to Docker Hub . Click the Create Repository button. For the repo name, use node-todo . Make sure the Visibility is Public . Click the Create button! Private repositories Did you know that Docker offers private repositories which allows you to restrict content to specific users or teams? Check out the details on the Docker pricing page. If you look on the right-side of the page, you'll see a section named Docker commands . This gives an example command that you will need to run to push to this repo.","title":"Create a Repo"},{"location":"tutorial/sharing-our-app/#pushing-our-image","text":"1.In the command line, try running the push command you see on Docker Hub. Note that your command will be using your namespace -> YOUR-USER-NAME . $ docker push YOUR-USER-NAME/node-todo The push refers to repository [docker.io/YOUR-USER-NAME/node-todo] An image does not exist locally with the tag: YOUR-USER-NAME/node-todo Why did it fail? The push command was looking for an image named YOUR-USER-NAME/node-todo, but didn't find one. If you run docker image ls , you won't see one either. To fix this, we need to \"tag\" our existing image we've built to give it another name. 2.Login to the Docker Hub using the command docker login -u YOUR-USER-NAME . 3.Use the docker tag command to give the node-todo image a new name. Be sure to swap out YOUR-USER-NAME with your Docker ID. docker tag node-todo YOUR-USER-NAME/node-todo 4.Now try your push command again. If you're copying the value from Docker Hub, you can drop the tagname portion, as we didn't add a tag to the image name. If you don't specify a tag, Docker will use a tag called latest . docker push YOUR-USER-NAME/node-todo","title":"Pushing our Image"},{"location":"tutorial/sharing-our-app/#running-our-image-on-a-new-instance","text":"Now that our image has been built and pushed into a registry, let's try running our app on a brand new instance that has never seen this container image! To do this, we will use Play with Docker. Open your browser to Play with Docker . Log in with your Docker Hub account. Once you're logged in, click on the \"+ ADD NEW INSTANCE\" link in the left side bar. (If you don't see it, make your browser a little wider.) After a few seconds, a terminal window will be opened in your browser. 1.In the terminal, start your freshly pushed app. docker run -dp 3000:3000 YOUR-USER-NAME/node-todo You should see the image get pulled down and eventually start up! 2.Click on the 3000 badge when it comes up and you should see the app with your modifications! Hooray! If the 3000 badge doesn't show up, you can click on the \"Open Port\" button and type in 3000 .","title":"Running our Image on a New Instance"},{"location":"tutorial/sharing-our-app/#recap","text":"In this section, we learned how to share our images by pushing them to a registry. We then went to a brand new instance and were able to run the freshly pushed image. This is quite common in CI pipelines, where the pipeline will create the image and push it to a registry and then the production environment can use the latest version of the image. Now that we have that figured out, let's circle back around to what we noticed at the end of the last section. As a reminder, we noticed that when we restarted the app, we lost all of our todo list items. That's obviously not a great user experience, so let's learn how we can persist the data across restarts!","title":"Recap"},{"location":"tutorial/using-bind-mounts/","text":"In the previous chapter, we talked about and used a named volume to persist the data in our database. Named volumes are great if we simply want to store data, as we don't have to worry about where the data is stored. With bind mounts , we control the exact mountpoint on the host. We can use this to persist data, but is often used to provide additional data into containers. When working on an application, we can use a bind mount to mount our source code into the container to let it see code changes, respond, and let us see the changes right away. For Node-based applications, nodemon is a great tool to watch for file changes and then restart the application. There are equivalent tools in most other languages and frameworks. Quick Volume Type Comparisons Bind mounts and named volumes are the two main types of volumes that come with the Docker engine. However, additional volume drivers are available to support other use cases ( SFTP , Ceph , NetApp , S3 , and more). Named Volumes Bind Mounts Host Location Docker chooses You control Mount Example (using -v ) my-volume:/usr/local/data /path/to/data:/usr/local/data Populates new volume with container contents Yes No Supports Volume Drivers Yes No Starting a Dev-Mode Container To run our container to support a development workflow, we will do the following: Mount our source code into the container Install all dependencies, including the \"dev\" dependencies Start nodemon to watch for filesystem changes So, let's do it! Make sure you don't have any previous getting-started containers running. Also make sure you are in app source code directory, i.e. /path/to/getting-started/app . If you aren't, you can cd into it, .e.g: bash cd /path/to/getting-started/app Now that you are in the getting-started/app directory, run the following command. We'll explain what's going on afterwards: bash docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you are using PowerShell then use this command. powershell docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" If you are using an Apple Silicon Mac or another ARM64 device then use this command. bash docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ node:12-alpine \\ sh -c \"apk add --no-cache python2 g++ make && yarn install && yarn run dev\" -dp 3000:3000 - same as before. Run in detached (background) mode and create a port mapping -w /app - sets the container's present working directory where the command will run from -v \"$(pwd):/app\" - bind mount (link) the host's present getting-started/app directory to the container's /app directory. Note: Docker requires absolute paths for binding mounts, so in this example we use pwd for printing the absolute path of the working directory, i.e. the app directory, instead of typing it manually node:12-alpine - the image to use. Note that this is the base image for our app from the Dockerfile sh -c \"yarn install && yarn run dev\" - the command. We're starting a shell using sh (alpine doesn't have bash ) and running yarn install to install all dependencies and then running yarn run dev . If we look in the package.json , we'll see that the dev script is starting nodemon . You can watch the logs using docker logs -f <container-id> . You'll know you're ready to go when you see this... bash docker logs -f <container-id> $ nodemon src/index.js [nodemon] 1.19.2 [nodemon] to restart at any time, enter `rs` [nodemon] watching dir(s): *.* [nodemon] starting `node src/index.js` Using sqlite database at /etc/todos/todo.db Listening on port 3000 When you're done watching the logs, exit out by hitting Ctrl + C . Now, let's make a change to the app. In the src/static/js/app.js file, let's change the \"Add Item\" button to simply say \"Add\". This change will be on line 109 - remember to save the file. diff - {submitting ? 'Adding...' : 'Add Item'} + {submitting ? 'Adding...' : 'Add'} Simply refresh the page (or open it) and you should see the change reflected in the browser almost immediately. It might take a few seconds for the Node server to restart, so if you get an error, just try refreshing after a few seconds. {: style=\"width:75%;\"} {: .text-center } Feel free to make any other changes you'd like to make. When you're done, stop the container and build your new image using docker build -t getting-started . . Using bind mounts is very common for local development setups. The advantage is that the dev machine doesn't need to have all of the build tools and environments installed. With a single docker run command, the dev environment is pulled and ready to go. We'll talk about Docker Compose in a future step, as this will help simplify our commands (we're already getting a lot of flags). Recap At this point, we can persist our database and respond rapidly to the needs and demands of our investors and founders. Hooray! But, guess what? We received great news! Your project has been selected for future development! In order to prepare for production, we need to migrate our database from working in SQLite to something that can scale a little better. For simplicity, we'll keep with a relational database and switch our application to use MySQL. But, how should we run MySQL? How do we allow the containers to talk to each other? We'll talk about that next!","title":"Part6. Use bind mounts"},{"location":"tutorial/using-bind-mounts/#quick-volume-type-comparisons","text":"Bind mounts and named volumes are the two main types of volumes that come with the Docker engine. However, additional volume drivers are available to support other use cases ( SFTP , Ceph , NetApp , S3 , and more). Named Volumes Bind Mounts Host Location Docker chooses You control Mount Example (using -v ) my-volume:/usr/local/data /path/to/data:/usr/local/data Populates new volume with container contents Yes No Supports Volume Drivers Yes No","title":"Quick Volume Type Comparisons"},{"location":"tutorial/using-bind-mounts/#starting-a-dev-mode-container","text":"To run our container to support a development workflow, we will do the following: Mount our source code into the container Install all dependencies, including the \"dev\" dependencies Start nodemon to watch for filesystem changes So, let's do it! Make sure you don't have any previous getting-started containers running. Also make sure you are in app source code directory, i.e. /path/to/getting-started/app . If you aren't, you can cd into it, .e.g: bash cd /path/to/getting-started/app Now that you are in the getting-started/app directory, run the following command. We'll explain what's going on afterwards: bash docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you are using PowerShell then use this command. powershell docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" If you are using an Apple Silicon Mac or another ARM64 device then use this command. bash docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ node:12-alpine \\ sh -c \"apk add --no-cache python2 g++ make && yarn install && yarn run dev\" -dp 3000:3000 - same as before. Run in detached (background) mode and create a port mapping -w /app - sets the container's present working directory where the command will run from -v \"$(pwd):/app\" - bind mount (link) the host's present getting-started/app directory to the container's /app directory. Note: Docker requires absolute paths for binding mounts, so in this example we use pwd for printing the absolute path of the working directory, i.e. the app directory, instead of typing it manually node:12-alpine - the image to use. Note that this is the base image for our app from the Dockerfile sh -c \"yarn install && yarn run dev\" - the command. We're starting a shell using sh (alpine doesn't have bash ) and running yarn install to install all dependencies and then running yarn run dev . If we look in the package.json , we'll see that the dev script is starting nodemon . You can watch the logs using docker logs -f <container-id> . You'll know you're ready to go when you see this... bash docker logs -f <container-id> $ nodemon src/index.js [nodemon] 1.19.2 [nodemon] to restart at any time, enter `rs` [nodemon] watching dir(s): *.* [nodemon] starting `node src/index.js` Using sqlite database at /etc/todos/todo.db Listening on port 3000 When you're done watching the logs, exit out by hitting Ctrl + C . Now, let's make a change to the app. In the src/static/js/app.js file, let's change the \"Add Item\" button to simply say \"Add\". This change will be on line 109 - remember to save the file. diff - {submitting ? 'Adding...' : 'Add Item'} + {submitting ? 'Adding...' : 'Add'} Simply refresh the page (or open it) and you should see the change reflected in the browser almost immediately. It might take a few seconds for the Node server to restart, so if you get an error, just try refreshing after a few seconds. {: style=\"width:75%;\"} {: .text-center } Feel free to make any other changes you'd like to make. When you're done, stop the container and build your new image using docker build -t getting-started . . Using bind mounts is very common for local development setups. The advantage is that the dev machine doesn't need to have all of the build tools and environments installed. With a single docker run command, the dev environment is pulled and ready to go. We'll talk about Docker Compose in a future step, as this will help simplify our commands (we're already getting a lot of flags).","title":"Starting a Dev-Mode Container"},{"location":"tutorial/using-bind-mounts/#recap","text":"At this point, we can persist our database and respond rapidly to the needs and demands of our investors and founders. Hooray! But, guess what? We received great news! Your project has been selected for future development! In order to prepare for production, we need to migrate our database from working in SQLite to something that can scale a little better. For simplicity, we'll keep with a relational database and switch our application to use MySQL. But, how should we run MySQL? How do we allow the containers to talk to each other? We'll talk about that next!","title":"Recap"},{"location":"tutorial/using-docker-compose/","text":"Docker Compose is a tool that was developed to help define and share multi-container applications. With Compose, we can create a YAML file to define the services and with a single command, can spin everything up or tear it all down. The big advantage of using Compose is you can define your application stack in a file, keep it at the root of your project repo (it's now version controlled), and easily enable someone else to contribute to your project. Someone would only need to clone your repo and start the compose app. In fact, you might see quite a few projects on GitHub/GitLab doing exactly this now. So, how do we get started? Installing Docker Compose If you installed Docker Desktop/Toolbox for either Windows or Mac, you already have Docker Compose! Play-with-Docker instances already have Docker Compose installed as well. If you are on a Linux machine, you will need to install Docker Compose using the instructions here . After installation, you should be able to run the following and see version information. docker-compose version Creating our Compose File At the root of the app project, create a file named docker-compose.yml . In the compose file, we'll start off by defining the schema version. In most cases, it's best to use the latest supported version. You can look at the Compose file reference for the current schema versions and the compatibility matrix. yaml version: \"3.8\" Next, we'll define the list of services (or containers) we want to run as part of our application. ```yaml hl_lines=\"3\" version: \"3.8\" services: ``` And now, we'll start migrating a service at a time into the compose file. Defining the App Service To remember, this was the command we were using to define our app container. docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you are using PowerShell then use this command. docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" First, let's define the service entry and the image for the container. We can pick any name for the service. The name will automatically become a network alias, which will be useful when defining our MySQL service. ```yaml hl_lines=\"4 5\" version: \"3.8\" services: app: image: node:12-alpine ``` Typically, you will see the command close to the image definition, although there is no requirement on ordering. So, let's go ahead and move that into our file. ```yaml hl_lines=\"6\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ``` Let's migrate the -p 3000:3000 part of the command by defining the ports for the service. We will use the short syntax here, but there is also a more verbose long syntax available as well. ```yaml hl_lines=\"7 8\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 ``` Next, we'll migrate both the working directory ( -w /app ) and the volume mapping ( -v \"$(pwd):/app\" ) by using the working_dir and volumes definitions. Volumes also has a short and long syntax. One advantage of Docker Compose volume definitions is we can use relative paths from the current directory. ```yaml hl_lines=\"9 10 11\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app ``` Finally, we need to migrate the environment variable definitions using the environment key. ```yaml hl_lines=\"12 13 14 15 16\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos ``` Defining the MySQL Service Now, it's time to define the MySQL service. The command that we used for that container was the following: docker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command. docker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 We will first define the new service and name it mysql so it automatically gets the network alias. We'll go ahead and specify the image to use as well. ```yaml hl_lines=\"6 7\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 ``` Next, we'll define the volume mapping. When we ran the container with docker run , the named volume was created automatically. However, that doesn't happen when running with Compose. We need to define the volume in the top-level volumes: section and then specify the mountpoint in the service config. By simply providing only the volume name, the default options are used. There are many more options available though. ```yaml hl_lines=\"8 9 10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql volumes: todo-mysql-data: ``` Finally, we only need to specify the environment variables. ```yaml hl_lines=\"10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: ``` At this point, our complete docker-compose.yml should look like this: version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: Running our Application Stack Now that we have our docker-compose.yml file, we can start it up! Make sure no other copies of the app/db are running first ( docker ps and docker rm -f <ids> ). Start up the application stack using the docker-compose up command. We'll add the -d flag to run everything in the background. bash docker-compose up -d When we run this, we should see output like this: plaintext Creating network \"app_default\" with the default driver Creating volume \"app_todo-mysql-data\" with default driver Creating app_app_1 ... done Creating app_mysql_1 ... done You'll notice that the volume was created as well as a network! By default, Docker Compose automatically creates a network specifically for the application stack (which is why we didn't define one in the compose file). Let's look at the logs using the docker-compose logs -f command. You'll see the logs from each of the services interleaved into a single stream. This is incredibly useful when you want to watch for timing-related issues. The -f flag \"follows\" the log, so will give you live output as it's generated. If you don't already, you'll see output that looks like this... plaintext mysql_1 | 2019-10-03T03:07:16.083639Z 0 [Note] mysqld: ready for connections. mysql_1 | Version: '5.7.27' socket: '/var/run/mysqld/mysqld.sock' port: 3306 MySQL Community Server (GPL) app_1 | Connected to mysql db at host mysql app_1 | Listening on port 3000 The service name is displayed at the beginning of the line (often colored) to help distinguish messages. If you want to view the logs for a specific service, you can add the service name to the end of the logs command (for example, docker-compose logs -f app ). !!! info \"Pro tip - Waiting for the DB before starting the app\" When the app is starting up, it actually sits and waits for MySQL to be up and ready before trying to connect to it. Docker doesn't have any built-in support to wait for another container to be fully up, running, and ready before starting another container. For Node-based projects, you can use the wait-port dependency. Similar projects exist for other languages/frameworks. At this point, you should be able to open your app and see it running. And hey! We're down to a single command! Seeing our App Stack in Docker Dashboard If we look at the Docker Dashboard, we'll see that there is a group named app . This is the \"project name\" from Docker Compose and used to group the containers together. By default, the project name is simply the name of the directory that the docker-compose.yml was located in. If you twirl down the app, you will see the two containers we defined in the compose file. The names are also a little more descriptive, as they follow the pattern of <project-name>_<service-name>_<replica-number> . So, it's very easy to quickly see what container is our app and which container is the mysql database. Tearing it All Down When you're ready to tear it all down, simply run docker-compose down or hit the trash can on the Docker Dashboard for the entire app. The containers will stop and the network will be removed. !!! warning \"Removing Volumes\" By default, named volumes in your compose file are NOT removed when running docker-compose down . If you want to remove the volumes, you will need to add the --volumes flag. The Docker Dashboard does _not_ remove volumes when you delete the app stack. Once torn down, you can switch to another project, run docker-compose up and be ready to contribute to that project! It really doesn't get much simpler than that! Recap In this section, we learned about Docker Compose and how it helps us dramatically simplify the defining and sharing of multi-service applications. We created a Compose file by translating the commands we were using into the appropriate compose format. At this point, we're starting to wrap up the tutorial. However, there are a few best practices about image building we want to cover, as there is a big issue with the Dockerfile we've been using. So, let's take a look!","title":"Part8. Use Docker Compose"},{"location":"tutorial/using-docker-compose/#installing-docker-compose","text":"If you installed Docker Desktop/Toolbox for either Windows or Mac, you already have Docker Compose! Play-with-Docker instances already have Docker Compose installed as well. If you are on a Linux machine, you will need to install Docker Compose using the instructions here . After installation, you should be able to run the following and see version information. docker-compose version","title":"Installing Docker Compose"},{"location":"tutorial/using-docker-compose/#creating-our-compose-file","text":"At the root of the app project, create a file named docker-compose.yml . In the compose file, we'll start off by defining the schema version. In most cases, it's best to use the latest supported version. You can look at the Compose file reference for the current schema versions and the compatibility matrix. yaml version: \"3.8\" Next, we'll define the list of services (or containers) we want to run as part of our application. ```yaml hl_lines=\"3\" version: \"3.8\" services: ``` And now, we'll start migrating a service at a time into the compose file.","title":"Creating our Compose File"},{"location":"tutorial/using-docker-compose/#defining-the-app-service","text":"To remember, this was the command we were using to define our app container. docker run -dp 3000:3000 \\ -w /app -v \"$(pwd):/app\" \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \"yarn install && yarn run dev\" If you are using PowerShell then use this command. docker run -dp 3000:3000 ` -w /app -v \"$(pwd):/app\" ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \"yarn install && yarn run dev\" First, let's define the service entry and the image for the container. We can pick any name for the service. The name will automatically become a network alias, which will be useful when defining our MySQL service. ```yaml hl_lines=\"4 5\" version: \"3.8\" services: app: image: node:12-alpine ``` Typically, you will see the command close to the image definition, although there is no requirement on ordering. So, let's go ahead and move that into our file. ```yaml hl_lines=\"6\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ``` Let's migrate the -p 3000:3000 part of the command by defining the ports for the service. We will use the short syntax here, but there is also a more verbose long syntax available as well. ```yaml hl_lines=\"7 8\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 ``` Next, we'll migrate both the working directory ( -w /app ) and the volume mapping ( -v \"$(pwd):/app\" ) by using the working_dir and volumes definitions. Volumes also has a short and long syntax. One advantage of Docker Compose volume definitions is we can use relative paths from the current directory. ```yaml hl_lines=\"9 10 11\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app ``` Finally, we need to migrate the environment variable definitions using the environment key. ```yaml hl_lines=\"12 13 14 15 16\" version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos ```","title":"Defining the App Service"},{"location":"tutorial/using-docker-compose/#defining-the-mysql-service","text":"Now, it's time to define the MySQL service. The command that we used for that container was the following: docker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command. docker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 We will first define the new service and name it mysql so it automatically gets the network alias. We'll go ahead and specify the image to use as well. ```yaml hl_lines=\"6 7\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 ``` Next, we'll define the volume mapping. When we ran the container with docker run , the named volume was created automatically. However, that doesn't happen when running with Compose. We need to define the volume in the top-level volumes: section and then specify the mountpoint in the service config. By simply providing only the volume name, the default options are used. There are many more options available though. ```yaml hl_lines=\"8 9 10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql volumes: todo-mysql-data: ``` Finally, we only need to specify the environment variables. ```yaml hl_lines=\"10 11 12\" version: \"3.8\" services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: ``` At this point, our complete docker-compose.yml should look like this: version: \"3.8\" services: app: image: node:12-alpine command: sh -c \"yarn install && yarn run dev\" ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data:","title":"Defining the MySQL Service"},{"location":"tutorial/using-docker-compose/#running-our-application-stack","text":"Now that we have our docker-compose.yml file, we can start it up! Make sure no other copies of the app/db are running first ( docker ps and docker rm -f <ids> ). Start up the application stack using the docker-compose up command. We'll add the -d flag to run everything in the background. bash docker-compose up -d When we run this, we should see output like this: plaintext Creating network \"app_default\" with the default driver Creating volume \"app_todo-mysql-data\" with default driver Creating app_app_1 ... done Creating app_mysql_1 ... done You'll notice that the volume was created as well as a network! By default, Docker Compose automatically creates a network specifically for the application stack (which is why we didn't define one in the compose file). Let's look at the logs using the docker-compose logs -f command. You'll see the logs from each of the services interleaved into a single stream. This is incredibly useful when you want to watch for timing-related issues. The -f flag \"follows\" the log, so will give you live output as it's generated. If you don't already, you'll see output that looks like this... plaintext mysql_1 | 2019-10-03T03:07:16.083639Z 0 [Note] mysqld: ready for connections. mysql_1 | Version: '5.7.27' socket: '/var/run/mysqld/mysqld.sock' port: 3306 MySQL Community Server (GPL) app_1 | Connected to mysql db at host mysql app_1 | Listening on port 3000 The service name is displayed at the beginning of the line (often colored) to help distinguish messages. If you want to view the logs for a specific service, you can add the service name to the end of the logs command (for example, docker-compose logs -f app ). !!! info \"Pro tip - Waiting for the DB before starting the app\" When the app is starting up, it actually sits and waits for MySQL to be up and ready before trying to connect to it. Docker doesn't have any built-in support to wait for another container to be fully up, running, and ready before starting another container. For Node-based projects, you can use the wait-port dependency. Similar projects exist for other languages/frameworks. At this point, you should be able to open your app and see it running. And hey! We're down to a single command!","title":"Running our Application Stack"},{"location":"tutorial/using-docker-compose/#seeing-our-app-stack-in-docker-dashboard","text":"If we look at the Docker Dashboard, we'll see that there is a group named app . This is the \"project name\" from Docker Compose and used to group the containers together. By default, the project name is simply the name of the directory that the docker-compose.yml was located in. If you twirl down the app, you will see the two containers we defined in the compose file. The names are also a little more descriptive, as they follow the pattern of <project-name>_<service-name>_<replica-number> . So, it's very easy to quickly see what container is our app and which container is the mysql database.","title":"Seeing our App Stack in Docker Dashboard"},{"location":"tutorial/using-docker-compose/#tearing-it-all-down","text":"When you're ready to tear it all down, simply run docker-compose down or hit the trash can on the Docker Dashboard for the entire app. The containers will stop and the network will be removed. !!! warning \"Removing Volumes\" By default, named volumes in your compose file are NOT removed when running docker-compose down . If you want to remove the volumes, you will need to add the --volumes flag. The Docker Dashboard does _not_ remove volumes when you delete the app stack. Once torn down, you can switch to another project, run docker-compose up and be ready to contribute to that project! It really doesn't get much simpler than that!","title":"Tearing it All Down"},{"location":"tutorial/using-docker-compose/#recap","text":"In this section, we learned about Docker Compose and how it helps us dramatically simplify the defining and sharing of multi-service applications. We created a Compose file by translating the commands we were using into the appropriate compose format. At this point, we're starting to wrap up the tutorial. However, there are a few best practices about image building we want to cover, as there is a big issue with the Dockerfile we've been using. So, let's take a look!","title":"Recap"}]}